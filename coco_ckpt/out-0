2024-08-29,16:18:17 | INFO | Running with a single process. Device cuda:0.
2024-08-29,16:18:17 | INFO | Loaded ViT-B-32 model config.
2024-08-29,16:18:19 | INFO | Loading pretrained ViT-B-32 weights (laion2b_s34b_b79k).
2024-08-29,16:18:20 | INFO | Model:
2024-08-29,16:18:20 | INFO | CLIP(
  (visual): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
2024-08-29,16:18:20 | INFO | Params:
2024-08-29,16:18:20 | INFO |   accum_freq: 1
2024-08-29,16:18:20 | INFO |   aug_cfg: {}
2024-08-29,16:18:20 | INFO |   batch_size: 100
2024-08-29,16:18:20 | INFO |   beta1: 0.9
2024-08-29,16:18:20 | INFO |   beta2: 0.99
2024-08-29,16:18:20 | INFO |   checkpoint_path: C:/Users/INDA_HIWI/Desktop/plot_oc_flickr_train_ae/oc/coco_logs/2024_08_29-16_18_17-model_ViT-B-32-lr_1e-08-b_100-j_0-p_fp32\checkpoints
2024-08-29,16:18:20 | INFO |   coca_caption_loss_weight: 2.0
2024-08-29,16:18:20 | INFO |   coca_contrastive_loss_weight: 1.0
2024-08-29,16:18:20 | INFO |   copy_codebase: False
2024-08-29,16:18:20 | INFO |   csv_caption_key: title
2024-08-29,16:18:20 | INFO |   csv_img_key: filepath
2024-08-29,16:18:20 | INFO |   csv_separator: 	
2024-08-29,16:18:20 | INFO |   dataset_resampled: False
2024-08-29,16:18:20 | INFO |   dataset_type: auto
2024-08-29,16:18:20 | INFO |   ddp_static_graph: False
2024-08-29,16:18:20 | INFO |   debug: False
2024-08-29,16:18:20 | INFO |   delete_previous_checkpoint: False
2024-08-29,16:18:20 | INFO |   device: cuda:0
2024-08-29,16:18:20 | INFO |   dist_backend: nccl
2024-08-29,16:18:20 | INFO |   dist_url: env://
2024-08-29,16:18:20 | INFO |   distill: False
2024-08-29,16:18:20 | INFO |   distill_model: None
2024-08-29,16:18:20 | INFO |   distill_pretrained: None
2024-08-29,16:18:20 | INFO |   distributed: False
2024-08-29,16:18:20 | INFO |   epochs: 500
2024-08-29,16:18:20 | INFO |   epochs_cooldown: None
2024-08-29,16:18:20 | INFO |   eps: 1e-06
2024-08-29,16:18:20 | INFO |   force_custom_text: False
2024-08-29,16:18:20 | INFO |   force_image_size: None
2024-08-29,16:18:20 | INFO |   force_patch_dropout: None
2024-08-29,16:18:20 | INFO |   force_quick_gelu: False
2024-08-29,16:18:20 | INFO |   gather_with_grad: False
2024-08-29,16:18:20 | INFO |   grad_checkpointing: False
2024-08-29,16:18:20 | INFO |   grad_clip_norm: None
2024-08-29,16:18:20 | INFO |   horovod: False
2024-08-29,16:18:20 | INFO |   image_interpolation: None
2024-08-29,16:18:20 | INFO |   image_mean: None
2024-08-29,16:18:20 | INFO |   image_resize_mode: None
2024-08-29,16:18:20 | INFO |   image_std: None
2024-08-29,16:18:20 | INFO |   imagenet_v2: None
2024-08-29,16:18:20 | INFO |   imagenet_val: None
2024-08-29,16:18:20 | INFO |   local_loss: False
2024-08-29,16:18:20 | INFO |   local_rank: 0
2024-08-29,16:18:20 | INFO |   lock_image: False
2024-08-29,16:18:20 | INFO |   lock_image_freeze_bn_stats: False
2024-08-29,16:18:20 | INFO |   lock_image_unlocked_groups: 0
2024-08-29,16:18:20 | INFO |   lock_text: False
2024-08-29,16:18:20 | INFO |   lock_text_freeze_layer_norm: False
2024-08-29,16:18:20 | INFO |   lock_text_unlocked_layers: 0
2024-08-29,16:18:20 | INFO |   log_every_n_steps: 100
2024-08-29,16:18:20 | INFO |   log_level: 20
2024-08-29,16:18:20 | INFO |   log_local: True
2024-08-29,16:18:20 | INFO |   log_path: C:/Users/INDA_HIWI/Desktop/plot_oc_flickr_train_ae/oc/coco_logs/2024_08_29-16_18_17-model_ViT-B-32-lr_1e-08-b_100-j_0-p_fp32\out-0
2024-08-29,16:18:20 | INFO |   logs: C:/Users/INDA_HIWI/Desktop/plot_oc_flickr_train_ae/oc/coco_logs/
2024-08-29,16:18:20 | INFO |   lr: 1e-08
2024-08-29,16:18:20 | INFO |   lr_cooldown_end: 0.0
2024-08-29,16:18:20 | INFO |   lr_cooldown_power: 1.0
2024-08-29,16:18:20 | INFO |   lr_scheduler: cosine
2024-08-29,16:18:20 | INFO |   model: ViT-B-32
2024-08-29,16:18:20 | INFO |   name: 2024_08_29-16_18_17-model_ViT-B-32-lr_1e-08-b_100-j_0-p_fp32
2024-08-29,16:18:20 | INFO |   no_set_device_rank: False
2024-08-29,16:18:20 | INFO |   precision: fp32
2024-08-29,16:18:20 | INFO |   pretrained: laion2b_s34b_b79k
2024-08-29,16:18:20 | INFO |   pretrained_image: False
2024-08-29,16:18:20 | INFO |   rank: 0
2024-08-29,16:18:20 | INFO |   remote_sync: None
2024-08-29,16:18:20 | INFO |   remote_sync_frequency: 300
2024-08-29,16:18:20 | INFO |   remote_sync_protocol: s3
2024-08-29,16:18:20 | INFO |   report_to: 
2024-08-29,16:18:20 | INFO |   resume: None
2024-08-29,16:18:20 | INFO |   save_frequency: 1
2024-08-29,16:18:20 | INFO |   save_most_recent: False
2024-08-29,16:18:20 | INFO |   seed: 0
2024-08-29,16:18:20 | INFO |   siglip: False
2024-08-29,16:18:20 | INFO |   skip_scheduler: False
2024-08-29,16:18:20 | INFO |   tensorboard: False
2024-08-29,16:18:20 | INFO |   tensorboard_path: 
2024-08-29,16:18:20 | INFO |   torchcompile: False
2024-08-29,16:18:20 | INFO |   torchscript: False
2024-08-29,16:18:20 | INFO |   trace: False
2024-08-29,16:18:20 | INFO |   train_data: None
2024-08-29,16:18:20 | INFO |   train_data_upsampling_factors: None
2024-08-29,16:18:20 | INFO |   train_num_samples: None
2024-08-29,16:18:20 | INFO |   use_bn_sync: False
2024-08-29,16:18:20 | INFO |   use_bnb_linear: None
2024-08-29,16:18:20 | INFO |   val_data: None
2024-08-29,16:18:20 | INFO |   val_frequency: 1
2024-08-29,16:18:20 | INFO |   val_num_samples: None
2024-08-29,16:18:20 | INFO |   wandb: False
2024-08-29,16:18:20 | INFO |   wandb_notes: 
2024-08-29,16:18:20 | INFO |   wandb_project_name: open-clip
2024-08-29,16:18:20 | INFO |   warmup: 1
2024-08-29,16:18:20 | INFO |   wd: 0.2
2024-08-29,16:18:20 | INFO |   workers: 0
2024-08-29,16:18:20 | INFO |   world_size: 1
2024-08-29,16:18:20 | INFO |   zeroshot_frequency: 2
2024-08-29,16:18:21 | INFO | Eval Epoch: 0 [200 / 1000]	Clip Loss: 0.823747	
2024-08-29,16:18:22 | INFO | Eval Epoch: 0 image_to_text_mean_rank: 3.2850	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6240	image_to_text_R@5: 0.8760	image_to_text_R@10: 0.9420	text_to_image_mean_rank: 4.1280	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.5840	text_to_image_R@5: 0.8720	text_to_image_R@10: 0.9340	clip_val_loss: 0.6832	epoch: 0.0000	num_samples: 1000.0000
2024-08-29,16:18:22 | INFO | Start epoch 0
2024-08-29,16:18:22 | INFO | Train Epoch: 0 [   100/145000.0 (0%)] Data (t): 0.026 Batch (t): 0.528, 189.431/s, 189.431/s/gpu LR: 1.000000e-08 Logit Scale: 100.000 Contrastive_loss: 0.40538 (0.40538) Loss: 0.40538 (0.40538)
2024-08-29,16:18:41 | INFO | Train Epoch: 0 [ 10100/145000.0 (7%)] Data (t): 0.106 Batch (t): 0.190, 529.210/s, 529.210/s/gpu LR: 9.999989e-09 Logit Scale: 100.000 Contrastive_loss: 0.63180 (0.51859) Loss: 0.63180 (0.51859)
2024-08-29,16:19:00 | INFO | Train Epoch: 0 [ 20100/145000.0 (14%)] Data (t): 0.105 Batch (t): 0.189, 547.191/s, 547.191/s/gpu LR: 9.999957e-09 Logit Scale: 100.000 Contrastive_loss: 0.73070 (0.58929) Loss: 0.73070 (0.58929)
2024-08-29,16:19:19 | INFO | Train Epoch: 0 [ 30100/145000.0 (21%)] Data (t): 0.105 Batch (t): 0.190, 531.868/s, 531.868/s/gpu LR: 9.999902e-09 Logit Scale: 100.000 Contrastive_loss: 0.48562 (0.56337) Loss: 0.48562 (0.56337)
2024-08-29,16:19:38 | INFO | Train Epoch: 0 [ 40100/145000.0 (28%)] Data (t): 0.106 Batch (t): 0.190, 534.859/s, 534.859/s/gpu LR: 9.999825e-09 Logit Scale: 100.000 Contrastive_loss: 0.28780 (0.50826) Loss: 0.28780 (0.50826)
2024-08-29,16:19:57 | INFO | Train Epoch: 0 [ 50100/145000.0 (35%)] Data (t): 0.106 Batch (t): 0.190, 526.501/s, 526.501/s/gpu LR: 9.999727e-09 Logit Scale: 100.000 Contrastive_loss: 0.32489 (0.47770) Loss: 0.32489 (0.47770)
2024-08-29,16:20:16 | INFO | Train Epoch: 0 [ 60100/145000.0 (41%)] Data (t): 0.106 Batch (t): 0.191, 496.794/s, 496.794/s/gpu LR: 9.999607e-09 Logit Scale: 100.000 Contrastive_loss: 0.26986 (0.44801) Loss: 0.26986 (0.44801)
2024-08-29,16:20:36 | INFO | Train Epoch: 0 [ 70100/145000.0 (48%)] Data (t): 0.106 Batch (t): 0.191, 495.721/s, 495.721/s/gpu LR: 9.999464e-09 Logit Scale: 100.000 Contrastive_loss: 0.41306 (0.44364) Loss: 0.41306 (0.44364)
2024-08-29,16:20:55 | INFO | Train Epoch: 0 [ 80100/145000.0 (55%)] Data (t): 0.107 Batch (t): 0.191, 524.388/s, 524.388/s/gpu LR: 9.999300e-09 Logit Scale: 100.000 Contrastive_loss: 0.32320 (0.43026) Loss: 0.32320 (0.43026)
2024-08-29,16:21:14 | INFO | Train Epoch: 0 [ 90100/145000.0 (62%)] Data (t): 0.106 Batch (t): 0.192, 518.325/s, 518.325/s/gpu LR: 9.999114e-09 Logit Scale: 100.000 Contrastive_loss: 0.38338 (0.42557) Loss: 0.38338 (0.42557)
2024-08-29,16:21:33 | INFO | Train Epoch: 0 [100100/145000.0 (69%)] Data (t): 0.107 Batch (t): 0.192, 496.407/s, 496.407/s/gpu LR: 9.998906e-09 Logit Scale: 100.000 Contrastive_loss: 0.27996 (0.41233) Loss: 0.27996 (0.41233)
2024-08-29,16:21:52 | INFO | Train Epoch: 0 [110100/145000.0 (76%)] Data (t): 0.107 Batch (t): 0.192, 535.387/s, 535.387/s/gpu LR: 9.998676e-09 Logit Scale: 100.000 Contrastive_loss: 0.42006 (0.41298) Loss: 0.42006 (0.41298)
2024-08-29,16:22:11 | INFO | Train Epoch: 0 [120100/145000.0 (83%)] Data (t): 0.106 Batch (t): 0.191, 523.801/s, 523.801/s/gpu LR: 9.998424e-09 Logit Scale: 100.000 Contrastive_loss: 0.57952 (0.42579) Loss: 0.57952 (0.42579)
2024-08-29,16:22:31 | INFO | Train Epoch: 0 [130100/145000.0 (90%)] Data (t): 0.106 Batch (t): 0.191, 528.562/s, 528.562/s/gpu LR: 9.998150e-09 Logit Scale: 100.000 Contrastive_loss: 0.26484 (0.41429) Loss: 0.26484 (0.41429)
2024-08-29,16:22:50 | INFO | Train Epoch: 0 [140100/145000.0 (97%)] Data (t): 0.107 Batch (t): 0.192, 488.045/s, 488.045/s/gpu LR: 9.997854e-09 Logit Scale: 100.000 Contrastive_loss: 0.36913 (0.41128) Loss: 0.36913 (0.41128)
2024-08-29,16:22:59 | INFO | Train Epoch: 0 [145000/145000.0 (100%)] Data (t): 0.107 Batch (t): 0.192, 536.412/s, 536.412/s/gpu LR: 9.997701e-09 Logit Scale: 100.000 Contrastive_loss: 0.48352 (0.41580) Loss: 0.48352 (0.41580)
2024-08-29,16:23:09 | INFO | Train Epoch: 0 [150100/145000.0 (104%)] Data (t): 0.107 Batch (t): 0.191, 531.033/s, 531.033/s/gpu LR: 9.997536e-09 Logit Scale: 100.000 Contrastive_loss: 0.45209 (0.41793) Loss: 0.45209 (0.41793)
2024-08-29,16:23:28 | INFO | Train Epoch: 0 [160100/145000.0 (110%)] Data (t): 0.107 Batch (t): 0.192, 537.469/s, 537.469/s/gpu LR: 9.997196e-09 Logit Scale: 100.000 Contrastive_loss: 0.53648 (0.42452) Loss: 0.53648 (0.42452)
2024-08-29,16:23:47 | INFO | Train Epoch: 0 [170100/145000.0 (117%)] Data (t): 0.107 Batch (t): 0.192, 532.467/s, 532.467/s/gpu LR: 9.996835e-09 Logit Scale: 100.000 Contrastive_loss: 0.40150 (0.42331) Loss: 0.40150 (0.42331)
2024-08-29,16:24:07 | INFO | Train Epoch: 0 [180100/145000.0 (124%)] Data (t): 0.107 Batch (t): 0.192, 525.348/s, 525.348/s/gpu LR: 9.996451e-09 Logit Scale: 100.000 Contrastive_loss: 0.44165 (0.42422) Loss: 0.44165 (0.42422)
2024-08-29,16:24:26 | INFO | Train Epoch: 0 [190100/145000.0 (131%)] Data (t): 0.107 Batch (t): 0.192, 532.015/s, 532.015/s/gpu LR: 9.996046e-09 Logit Scale: 100.000 Contrastive_loss: 0.46540 (0.42618) Loss: 0.46540 (0.42618)
2024-08-29,16:24:45 | INFO | Train Epoch: 0 [200100/145000.0 (138%)] Data (t): 0.107 Batch (t): 0.192, 493.405/s, 493.405/s/gpu LR: 9.995618e-09 Logit Scale: 100.000 Contrastive_loss: 0.30118 (0.42050) Loss: 0.30118 (0.42050)
2024-08-29,16:25:04 | INFO | Train Epoch: 0 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 512.404/s, 512.404/s/gpu LR: 9.995169e-09 Logit Scale: 100.000 Contrastive_loss: 0.45759 (0.42211) Loss: 0.45759 (0.42211)
2024-08-29,16:25:24 | INFO | Train Epoch: 0 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 544.136/s, 544.136/s/gpu LR: 9.994698e-09 Logit Scale: 100.000 Contrastive_loss: 0.60746 (0.42984) Loss: 0.60746 (0.42984)
2024-08-29,16:25:43 | INFO | Train Epoch: 0 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 495.315/s, 495.315/s/gpu LR: 9.994205e-09 Logit Scale: 100.000 Contrastive_loss: 0.46671 (0.43131) Loss: 0.46671 (0.43131)
2024-08-29,16:26:02 | INFO | Train Epoch: 0 [240100/145000.0 (166%)] Data (t): 0.107 Batch (t): 0.192, 496.949/s, 496.949/s/gpu LR: 9.993690e-09 Logit Scale: 100.000 Contrastive_loss: 0.38023 (0.42935) Loss: 0.38023 (0.42935)
2024-08-29,16:26:21 | INFO | Train Epoch: 0 [250100/145000.0 (172%)] Data (t): 0.107 Batch (t): 0.193, 498.776/s, 498.776/s/gpu LR: 9.993153e-09 Logit Scale: 100.000 Contrastive_loss: 0.35847 (0.42672) Loss: 0.35847 (0.42672)
2024-08-29,16:26:41 | INFO | Train Epoch: 0 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.192, 500.376/s, 500.376/s/gpu LR: 9.992594e-09 Logit Scale: 100.000 Contrastive_loss: 0.44204 (0.42727) Loss: 0.44204 (0.42727)
2024-08-29,16:27:00 | INFO | Train Epoch: 0 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 529.572/s, 529.572/s/gpu LR: 9.992014e-09 Logit Scale: 100.000 Contrastive_loss: 0.37147 (0.42535) Loss: 0.37147 (0.42535)
2024-08-29,16:27:19 | INFO | Train Epoch: 0 [280100/145000.0 (193%)] Data (t): 0.107 Batch (t): 0.192, 538.061/s, 538.061/s/gpu LR: 9.991411e-09 Logit Scale: 100.000 Contrastive_loss: 0.32285 (0.42193) Loss: 0.32285 (0.42193)
2024-08-29,16:27:38 | INFO | Train Epoch: 0 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.193, 526.470/s, 526.470/s/gpu LR: 9.990786e-09 Logit Scale: 100.000 Contrastive_loss: 0.23518 (0.41590) Loss: 0.23518 (0.41590)
2024-08-29,16:27:57 | INFO | Train Epoch: 0 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 528.695/s, 528.695/s/gpu LR: 9.990140e-09 Logit Scale: 100.000 Contrastive_loss: 0.43733 (0.41657) Loss: 0.43733 (0.41657)
2024-08-29,16:28:17 | INFO | Train Epoch: 0 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 527.860/s, 527.860/s/gpu LR: 9.989472e-09 Logit Scale: 100.000 Contrastive_loss: 0.61398 (0.42256) Loss: 0.61398 (0.42256)
2024-08-29,16:28:36 | INFO | Train Epoch: 0 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 519.925/s, 519.925/s/gpu LR: 9.988782e-09 Logit Scale: 100.000 Contrastive_loss: 0.50818 (0.42507) Loss: 0.50818 (0.42507)
2024-08-29,16:28:56 | INFO | Train Epoch: 0 [330100/145000.0 (228%)] Data (t): 0.111 Batch (t): 0.198, 491.074/s, 491.074/s/gpu LR: 9.988070e-09 Logit Scale: 100.000 Contrastive_loss: 0.28020 (0.42094) Loss: 0.28020 (0.42094)
2024-08-29,16:29:15 | INFO | Train Epoch: 0 [340100/145000.0 (235%)] Data (t): 0.107 Batch (t): 0.192, 522.943/s, 522.943/s/gpu LR: 9.987336e-09 Logit Scale: 100.000 Contrastive_loss: 0.55598 (0.42469) Loss: 0.55598 (0.42469)
2024-08-29,16:29:34 | INFO | Train Epoch: 0 [350100/145000.0 (241%)] Data (t): 0.107 Batch (t): 0.192, 499.177/s, 499.177/s/gpu LR: 9.986580e-09 Logit Scale: 100.000 Contrastive_loss: 0.63340 (0.43033) Loss: 0.63340 (0.43033)
2024-08-29,16:29:53 | INFO | Train Epoch: 0 [360100/145000.0 (248%)] Data (t): 0.107 Batch (t): 0.192, 518.632/s, 518.632/s/gpu LR: 9.985802e-09 Logit Scale: 100.000 Contrastive_loss: 0.31762 (0.42736) Loss: 0.31762 (0.42736)
2024-08-29,16:30:13 | INFO | Train Epoch: 0 [370100/145000.0 (255%)] Data (t): 0.107 Batch (t): 0.192, 500.949/s, 500.949/s/gpu LR: 9.985003e-09 Logit Scale: 100.000 Contrastive_loss: 0.45328 (0.42803) Loss: 0.45328 (0.42803)
2024-08-29,16:30:32 | INFO | Train Epoch: 0 [380100/145000.0 (262%)] Data (t): 0.106 Batch (t): 0.191, 526.432/s, 526.432/s/gpu LR: 9.984181e-09 Logit Scale: 100.000 Contrastive_loss: 0.41103 (0.42760) Loss: 0.41103 (0.42760)
2024-08-29,16:30:51 | INFO | Train Epoch: 0 [390100/145000.0 (269%)] Data (t): 0.107 Batch (t): 0.192, 496.198/s, 496.198/s/gpu LR: 9.983338e-09 Logit Scale: 100.000 Contrastive_loss: 0.31168 (0.42477) Loss: 0.31168 (0.42477)
2024-08-29,16:31:10 | INFO | Train Epoch: 0 [400100/145000.0 (276%)] Data (t): 0.107 Batch (t): 0.192, 493.288/s, 493.288/s/gpu LR: 9.982473e-09 Logit Scale: 100.000 Contrastive_loss: 0.38000 (0.42371) Loss: 0.38000 (0.42371)
2024-08-29,16:31:29 | INFO | Train Epoch: 0 [410100/145000.0 (283%)] Data (t): 0.106 Batch (t): 0.191, 528.982/s, 528.982/s/gpu LR: 9.981586e-09 Logit Scale: 100.000 Contrastive_loss: 0.17205 (0.41786) Loss: 0.17205 (0.41786)
2024-08-29,16:31:37 | INFO | Eval Epoch: 1 [200 / 1000]	Clip Loss: 0.722903	
2024-08-29,16:31:38 | INFO | Eval Epoch: 1 image_to_text_mean_rank: 3.0230	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6310	image_to_text_R@5: 0.8860	image_to_text_R@10: 0.9470	text_to_image_mean_rank: 3.8810	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6140	text_to_image_R@5: 0.8690	text_to_image_R@10: 0.9440	clip_val_loss: 0.5994	epoch: 1.0000	num_samples: 1000.0000
2024-08-29,16:31:39 | INFO | Start epoch 1
2024-08-29,16:31:39 | INFO | Train Epoch: 1 [   100/145000.0 (0%)] Data (t): 0.021 Batch (t): 0.175, 571.971/s, 571.971/s/gpu LR: 9.997698e-09 Logit Scale: 100.000 Contrastive_loss: 0.33883 (0.33883) Loss: 0.33883 (0.33883)
2024-08-29,16:31:58 | INFO | Train Epoch: 1 [ 10100/145000.0 (7%)] Data (t): 0.108 Batch (t): 0.193, 537.611/s, 537.611/s/gpu LR: 9.997369e-09 Logit Scale: 100.000 Contrastive_loss: 0.58097 (0.45990) Loss: 0.58097 (0.45990)
2024-08-29,16:32:18 | INFO | Train Epoch: 1 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.193, 526.521/s, 526.521/s/gpu LR: 9.997018e-09 Logit Scale: 100.000 Contrastive_loss: 0.62253 (0.51411) Loss: 0.62253 (0.51411)
2024-08-29,16:32:37 | INFO | Train Epoch: 1 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.192, 522.657/s, 522.657/s/gpu LR: 9.996646e-09 Logit Scale: 100.000 Contrastive_loss: 0.41187 (0.48855) Loss: 0.41187 (0.48855)
2024-08-29,16:32:56 | INFO | Train Epoch: 1 [ 40100/145000.0 (28%)] Data (t): 0.107 Batch (t): 0.192, 528.549/s, 528.549/s/gpu LR: 9.996251e-09 Logit Scale: 100.000 Contrastive_loss: 0.24425 (0.43969) Loss: 0.24425 (0.43969)
2024-08-29,16:33:15 | INFO | Train Epoch: 1 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 532.246/s, 532.246/s/gpu LR: 9.995835e-09 Logit Scale: 100.000 Contrastive_loss: 0.27386 (0.41205) Loss: 0.27386 (0.41205)
2024-08-29,16:33:34 | INFO | Train Epoch: 1 [ 60100/145000.0 (41%)] Data (t): 0.107 Batch (t): 0.192, 531.281/s, 531.281/s/gpu LR: 9.995397e-09 Logit Scale: 100.000 Contrastive_loss: 0.23471 (0.38672) Loss: 0.23471 (0.38672)
2024-08-29,16:33:54 | INFO | Train Epoch: 1 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.192, 528.670/s, 528.670/s/gpu LR: 9.994936e-09 Logit Scale: 100.000 Contrastive_loss: 0.39026 (0.38716) Loss: 0.39026 (0.38716)
2024-08-29,16:34:14 | INFO | Train Epoch: 1 [ 80100/145000.0 (55%)] Data (t): 0.112 Batch (t): 0.199, 529.280/s, 529.280/s/gpu LR: 9.994454e-09 Logit Scale: 100.000 Contrastive_loss: 0.28497 (0.37580) Loss: 0.28497 (0.37580)
2024-08-29,16:34:33 | INFO | Train Epoch: 1 [ 90100/145000.0 (62%)] Data (t): 0.107 Batch (t): 0.192, 503.784/s, 503.784/s/gpu LR: 9.993950e-09 Logit Scale: 100.000 Contrastive_loss: 0.34428 (0.37265) Loss: 0.34428 (0.37265)
2024-08-29,16:34:52 | INFO | Train Epoch: 1 [100100/145000.0 (69%)] Data (t): 0.106 Batch (t): 0.191, 532.358/s, 532.358/s/gpu LR: 9.993424e-09 Logit Scale: 100.000 Contrastive_loss: 0.24795 (0.36132) Loss: 0.24795 (0.36132)
2024-08-29,16:35:11 | INFO | Train Epoch: 1 [110100/145000.0 (76%)] Data (t): 0.107 Batch (t): 0.192, 529.906/s, 529.906/s/gpu LR: 9.992876e-09 Logit Scale: 100.000 Contrastive_loss: 0.35761 (0.36101) Loss: 0.35761 (0.36101)
2024-08-29,16:35:30 | INFO | Train Epoch: 1 [120100/145000.0 (83%)] Data (t): 0.107 Batch (t): 0.192, 519.330/s, 519.330/s/gpu LR: 9.992307e-09 Logit Scale: 100.000 Contrastive_loss: 0.52118 (0.37333) Loss: 0.52118 (0.37333)
2024-08-29,16:35:50 | INFO | Train Epoch: 1 [130100/145000.0 (90%)] Data (t): 0.107 Batch (t): 0.192, 500.065/s, 500.065/s/gpu LR: 9.991715e-09 Logit Scale: 100.000 Contrastive_loss: 0.25134 (0.36462) Loss: 0.25134 (0.36462)
2024-08-29,16:36:09 | INFO | Train Epoch: 1 [140100/145000.0 (97%)] Data (t): 0.107 Batch (t): 0.192, 501.129/s, 501.129/s/gpu LR: 9.991101e-09 Logit Scale: 100.000 Contrastive_loss: 0.31372 (0.36122) Loss: 0.31372 (0.36122)
2024-08-29,16:36:18 | INFO | Train Epoch: 1 [145000/145000.0 (100%)] Data (t): 0.106 Batch (t): 0.191, 532.411/s, 532.411/s/gpu LR: 9.990793e-09 Logit Scale: 100.000 Contrastive_loss: 0.44520 (0.36647) Loss: 0.44520 (0.36647)
2024-08-29,16:36:28 | INFO | Train Epoch: 1 [150100/145000.0 (104%)] Data (t): 0.107 Batch (t): 0.192, 525.839/s, 525.839/s/gpu LR: 9.990466e-09 Logit Scale: 100.000 Contrastive_loss: 0.40706 (0.36886) Loss: 0.40706 (0.36886)
2024-08-29,16:36:47 | INFO | Train Epoch: 1 [160100/145000.0 (110%)] Data (t): 0.106 Batch (t): 0.192, 517.923/s, 517.923/s/gpu LR: 9.989809e-09 Logit Scale: 100.000 Contrastive_loss: 0.48988 (0.37558) Loss: 0.48988 (0.37558)
2024-08-29,16:37:06 | INFO | Train Epoch: 1 [170100/145000.0 (117%)] Data (t): 0.107 Batch (t): 0.191, 522.364/s, 522.364/s/gpu LR: 9.989129e-09 Logit Scale: 100.000 Contrastive_loss: 0.35041 (0.37426) Loss: 0.35041 (0.37426)
2024-08-29,16:37:25 | INFO | Train Epoch: 1 [180100/145000.0 (124%)] Data (t): 0.107 Batch (t): 0.192, 531.218/s, 531.218/s/gpu LR: 9.988428e-09 Logit Scale: 100.000 Contrastive_loss: 0.38134 (0.37461) Loss: 0.38134 (0.37461)
2024-08-29,16:37:45 | INFO | Train Epoch: 1 [190100/145000.0 (131%)] Data (t): 0.107 Batch (t): 0.192, 529.423/s, 529.423/s/gpu LR: 9.987705e-09 Logit Scale: 100.000 Contrastive_loss: 0.41065 (0.37633) Loss: 0.41065 (0.37633)
2024-08-29,16:38:04 | INFO | Train Epoch: 1 [200100/145000.0 (138%)] Data (t): 0.107 Batch (t): 0.191, 521.211/s, 521.211/s/gpu LR: 9.986961e-09 Logit Scale: 100.000 Contrastive_loss: 0.25923 (0.37100) Loss: 0.25923 (0.37100)
2024-08-29,16:38:23 | INFO | Train Epoch: 1 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 540.170/s, 540.170/s/gpu LR: 9.986194e-09 Logit Scale: 100.000 Contrastive_loss: 0.40929 (0.37267) Loss: 0.40929 (0.37267)
2024-08-29,16:38:42 | INFO | Train Epoch: 1 [220100/145000.0 (152%)] Data (t): 0.106 Batch (t): 0.192, 535.895/s, 535.895/s/gpu LR: 9.985405e-09 Logit Scale: 100.000 Contrastive_loss: 0.54279 (0.37976) Loss: 0.54279 (0.37976)
2024-08-29,16:39:01 | INFO | Train Epoch: 1 [230100/145000.0 (159%)] Data (t): 0.107 Batch (t): 0.192, 523.213/s, 523.213/s/gpu LR: 9.984595e-09 Logit Scale: 100.000 Contrastive_loss: 0.42077 (0.38140) Loss: 0.42077 (0.38140)
2024-08-29,16:39:20 | INFO | Train Epoch: 1 [240100/145000.0 (166%)] Data (t): 0.107 Batch (t): 0.192, 525.646/s, 525.646/s/gpu LR: 9.983762e-09 Logit Scale: 100.000 Contrastive_loss: 0.33502 (0.37961) Loss: 0.33502 (0.37961)
2024-08-29,16:39:40 | INFO | Train Epoch: 1 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 526.485/s, 526.485/s/gpu LR: 9.982908e-09 Logit Scale: 100.000 Contrastive_loss: 0.33823 (0.37808) Loss: 0.33823 (0.37808)
2024-08-29,16:39:59 | INFO | Train Epoch: 1 [260100/145000.0 (179%)] Data (t): 0.110 Batch (t): 0.195, 522.472/s, 522.472/s/gpu LR: 9.982032e-09 Logit Scale: 100.000 Contrastive_loss: 0.39613 (0.37873) Loss: 0.39613 (0.37873)
2024-08-29,16:40:19 | INFO | Train Epoch: 1 [270100/145000.0 (186%)] Data (t): 0.110 Batch (t): 0.196, 495.694/s, 495.694/s/gpu LR: 9.981134e-09 Logit Scale: 100.000 Contrastive_loss: 0.34835 (0.37768) Loss: 0.34835 (0.37768)
2024-08-29,16:40:38 | INFO | Train Epoch: 1 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.192, 519.402/s, 519.402/s/gpu LR: 9.980214e-09 Logit Scale: 100.000 Contrastive_loss: 0.29859 (0.37504) Loss: 0.29859 (0.37504)
2024-08-29,16:40:57 | INFO | Train Epoch: 1 [290100/145000.0 (200%)] Data (t): 0.107 Batch (t): 0.192, 497.564/s, 497.564/s/gpu LR: 9.979273e-09 Logit Scale: 100.000 Contrastive_loss: 0.22130 (0.37008) Loss: 0.22130 (0.37008)
2024-08-29,16:41:16 | INFO | Train Epoch: 1 [300100/145000.0 (207%)] Data (t): 0.106 Batch (t): 0.192, 530.282/s, 530.282/s/gpu LR: 9.978309e-09 Logit Scale: 100.000 Contrastive_loss: 0.41258 (0.37141) Loss: 0.41258 (0.37141)
2024-08-29,16:41:35 | INFO | Train Epoch: 1 [310100/145000.0 (214%)] Data (t): 0.107 Batch (t): 0.192, 533.076/s, 533.076/s/gpu LR: 9.977324e-09 Logit Scale: 100.000 Contrastive_loss: 0.53379 (0.37633) Loss: 0.53379 (0.37633)
2024-08-29,16:41:55 | INFO | Train Epoch: 1 [320100/145000.0 (221%)] Data (t): 0.107 Batch (t): 0.193, 539.891/s, 539.891/s/gpu LR: 9.976317e-09 Logit Scale: 100.000 Contrastive_loss: 0.46126 (0.37883) Loss: 0.46126 (0.37883)
2024-08-29,16:42:14 | INFO | Train Epoch: 1 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.196, 493.842/s, 493.842/s/gpu LR: 9.975288e-09 Logit Scale: 100.000 Contrastive_loss: 0.26624 (0.37561) Loss: 0.26624 (0.37561)
2024-08-29,16:42:34 | INFO | Train Epoch: 1 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.195, 518.213/s, 518.213/s/gpu LR: 9.974237e-09 Logit Scale: 100.000 Contrastive_loss: 0.49345 (0.37889) Loss: 0.49345 (0.37889)
2024-08-29,16:42:53 | INFO | Train Epoch: 1 [350100/145000.0 (241%)] Data (t): 0.107 Batch (t): 0.192, 518.536/s, 518.536/s/gpu LR: 9.973165e-09 Logit Scale: 100.000 Contrastive_loss: 0.56241 (0.38385) Loss: 0.56241 (0.38385)
2024-08-29,16:43:12 | INFO | Train Epoch: 1 [360100/145000.0 (248%)] Data (t): 0.107 Batch (t): 0.192, 535.473/s, 535.473/s/gpu LR: 9.972070e-09 Logit Scale: 100.000 Contrastive_loss: 0.28696 (0.38130) Loss: 0.28696 (0.38130)
2024-08-29,16:43:32 | INFO | Train Epoch: 1 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.196, 487.118/s, 487.118/s/gpu LR: 9.970954e-09 Logit Scale: 100.000 Contrastive_loss: 0.41543 (0.38217) Loss: 0.41543 (0.38217)
2024-08-29,16:43:51 | INFO | Train Epoch: 1 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.196, 496.746/s, 496.746/s/gpu LR: 9.969816e-09 Logit Scale: 100.000 Contrastive_loss: 0.37452 (0.38198) Loss: 0.37452 (0.38198)
2024-08-29,16:44:11 | INFO | Train Epoch: 1 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.195, 524.043/s, 524.043/s/gpu LR: 9.968656e-09 Logit Scale: 100.000 Contrastive_loss: 0.28337 (0.37958) Loss: 0.28337 (0.37958)
2024-08-29,16:44:30 | INFO | Train Epoch: 1 [400100/145000.0 (276%)] Data (t): 0.110 Batch (t): 0.197, 531.139/s, 531.139/s/gpu LR: 9.967474e-09 Logit Scale: 100.000 Contrastive_loss: 0.34868 (0.37884) Loss: 0.34868 (0.37884)
2024-08-29,16:44:50 | INFO | Train Epoch: 1 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.196, 504.933/s, 504.933/s/gpu LR: 9.966271e-09 Logit Scale: 100.000 Contrastive_loss: 0.14849 (0.37348) Loss: 0.14849 (0.37348)
2024-08-29,16:44:58 | INFO | Eval Epoch: 2 [200 / 1000]	Clip Loss: 0.666871	
2024-08-29,16:44:58 | INFO | Eval Epoch: 2 image_to_text_mean_rank: 2.9150	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6280	image_to_text_R@5: 0.8890	image_to_text_R@10: 0.9510	text_to_image_mean_rank: 3.7750	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6170	text_to_image_R@5: 0.8730	text_to_image_R@10: 0.9460	clip_val_loss: 0.5598	epoch: 2.0000	num_samples: 1000.0000
2024-08-29,16:45:00 | INFO | Start epoch 2
2024-08-29,16:45:00 | INFO | Train Epoch: 2 [   100/145000.0 (0%)] Data (t): 0.027 Batch (t): 0.108, 927.383/s, 927.383/s/gpu LR: 9.990786e-09 Logit Scale: 100.000 Contrastive_loss: 0.30934 (0.30934) Loss: 0.30934 (0.30934)
2024-08-29,16:45:19 | INFO | Train Epoch: 2 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.195, 490.237/s, 490.237/s/gpu LR: 9.990140e-09 Logit Scale: 100.000 Contrastive_loss: 0.54574 (0.42754) Loss: 0.54574 (0.42754)
2024-08-29,16:45:39 | INFO | Train Epoch: 2 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.195, 520.886/s, 520.886/s/gpu LR: 9.989472e-09 Logit Scale: 100.000 Contrastive_loss: 0.56421 (0.47310) Loss: 0.56421 (0.47310)
2024-08-29,16:45:58 | INFO | Train Epoch: 2 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.196, 496.421/s, 496.421/s/gpu LR: 9.988782e-09 Logit Scale: 100.000 Contrastive_loss: 0.36460 (0.44597) Loss: 0.36460 (0.44597)
2024-08-29,16:46:18 | INFO | Train Epoch: 2 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.195, 526.523/s, 526.523/s/gpu LR: 9.988070e-09 Logit Scale: 100.000 Contrastive_loss: 0.22519 (0.40182) Loss: 0.22519 (0.40182)
2024-08-29,16:46:37 | INFO | Train Epoch: 2 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.195, 527.491/s, 527.491/s/gpu LR: 9.987336e-09 Logit Scale: 100.000 Contrastive_loss: 0.25397 (0.37718) Loss: 0.25397 (0.37718)
2024-08-29,16:46:57 | INFO | Train Epoch: 2 [ 60100/145000.0 (41%)] Data (t): 0.110 Batch (t): 0.197, 451.862/s, 451.862/s/gpu LR: 9.986580e-09 Logit Scale: 100.000 Contrastive_loss: 0.21627 (0.35419) Loss: 0.21627 (0.35419)
2024-08-29,16:47:17 | INFO | Train Epoch: 2 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.195, 526.030/s, 526.030/s/gpu LR: 9.985802e-09 Logit Scale: 100.000 Contrastive_loss: 0.37353 (0.35661) Loss: 0.37353 (0.35661)
2024-08-29,16:47:36 | INFO | Train Epoch: 2 [ 80100/145000.0 (55%)] Data (t): 0.107 Batch (t): 0.193, 518.663/s, 518.663/s/gpu LR: 9.985003e-09 Logit Scale: 100.000 Contrastive_loss: 0.26199 (0.34609) Loss: 0.26199 (0.34609)
2024-08-29,16:47:55 | INFO | Train Epoch: 2 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.195, 540.007/s, 540.007/s/gpu LR: 9.984181e-09 Logit Scale: 100.000 Contrastive_loss: 0.32512 (0.34400) Loss: 0.32512 (0.34400)
2024-08-29,16:48:15 | INFO | Train Epoch: 2 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.194, 531.270/s, 531.270/s/gpu LR: 9.983338e-09 Logit Scale: 100.000 Contrastive_loss: 0.23406 (0.33400) Loss: 0.23406 (0.33400)
2024-08-29,16:48:34 | INFO | Train Epoch: 2 [110100/145000.0 (76%)] Data (t): 0.110 Batch (t): 0.196, 526.669/s, 526.669/s/gpu LR: 9.982473e-09 Logit Scale: 100.000 Contrastive_loss: 0.32056 (0.33288) Loss: 0.32056 (0.33288)
2024-08-29,16:48:54 | INFO | Train Epoch: 2 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.195, 523.843/s, 523.843/s/gpu LR: 9.981586e-09 Logit Scale: 100.000 Contrastive_loss: 0.48110 (0.34428) Loss: 0.48110 (0.34428)
2024-08-29,16:49:13 | INFO | Train Epoch: 2 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 490.476/s, 490.476/s/gpu LR: 9.980677e-09 Logit Scale: 100.000 Contrastive_loss: 0.24211 (0.33699) Loss: 0.24211 (0.33699)
2024-08-29,16:49:33 | INFO | Train Epoch: 2 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.195, 501.768/s, 501.768/s/gpu LR: 9.979746e-09 Logit Scale: 100.000 Contrastive_loss: 0.28571 (0.33357) Loss: 0.28571 (0.33357)
2024-08-29,16:49:42 | INFO | Train Epoch: 2 [145000/145000.0 (100%)] Data (t): 0.107 Batch (t): 0.193, 532.272/s, 532.272/s/gpu LR: 9.979282e-09 Logit Scale: 100.000 Contrastive_loss: 0.41133 (0.33843) Loss: 0.41133 (0.33843)
2024-08-29,16:49:52 | INFO | Train Epoch: 2 [150100/145000.0 (104%)] Data (t): 0.110 Batch (t): 0.198, 530.629/s, 530.629/s/gpu LR: 9.978794e-09 Logit Scale: 100.000 Contrastive_loss: 0.37781 (0.34074) Loss: 0.37781 (0.34074)
2024-08-29,16:50:11 | INFO | Train Epoch: 2 [160100/145000.0 (110%)] Data (t): 0.107 Batch (t): 0.192, 496.760/s, 496.760/s/gpu LR: 9.977819e-09 Logit Scale: 100.000 Contrastive_loss: 0.45960 (0.34735) Loss: 0.45960 (0.34735)
2024-08-29,16:50:31 | INFO | Train Epoch: 2 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.195, 526.409/s, 526.409/s/gpu LR: 9.976823e-09 Logit Scale: 100.000 Contrastive_loss: 0.32070 (0.34595) Loss: 0.32070 (0.34595)
2024-08-29,16:50:50 | INFO | Train Epoch: 2 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 529.234/s, 529.234/s/gpu LR: 9.975805e-09 Logit Scale: 100.000 Contrastive_loss: 0.34454 (0.34588) Loss: 0.34454 (0.34588)
2024-08-29,16:51:10 | INFO | Train Epoch: 2 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.195, 530.514/s, 530.514/s/gpu LR: 9.974765e-09 Logit Scale: 100.000 Contrastive_loss: 0.38450 (0.34771) Loss: 0.38450 (0.34771)
2024-08-29,16:51:29 | INFO | Train Epoch: 2 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.196, 525.796/s, 525.796/s/gpu LR: 9.973704e-09 Logit Scale: 100.000 Contrastive_loss: 0.23988 (0.34281) Loss: 0.23988 (0.34281)
2024-08-29,16:51:49 | INFO | Train Epoch: 2 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 490.699/s, 490.699/s/gpu LR: 9.972620e-09 Logit Scale: 100.000 Contrastive_loss: 0.38279 (0.34455) Loss: 0.38279 (0.34455)
2024-08-29,16:52:08 | INFO | Train Epoch: 2 [220100/145000.0 (152%)] Data (t): 0.107 Batch (t): 0.194, 533.230/s, 533.230/s/gpu LR: 9.971515e-09 Logit Scale: 100.000 Contrastive_loss: 0.50885 (0.35140) Loss: 0.50885 (0.35140)
2024-08-29,16:52:27 | INFO | Train Epoch: 2 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.195, 529.032/s, 529.032/s/gpu LR: 9.970388e-09 Logit Scale: 100.000 Contrastive_loss: 0.39579 (0.35317) Loss: 0.39579 (0.35317)
2024-08-29,16:52:47 | INFO | Train Epoch: 2 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.195, 499.835/s, 499.835/s/gpu LR: 9.969239e-09 Logit Scale: 100.000 Contrastive_loss: 0.30688 (0.35139) Loss: 0.30688 (0.35139)
2024-08-29,16:53:07 | INFO | Train Epoch: 2 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.197, 524.200/s, 524.200/s/gpu LR: 9.968068e-09 Logit Scale: 100.000 Contrastive_loss: 0.32172 (0.35029) Loss: 0.32172 (0.35029)
2024-08-29,16:53:26 | INFO | Train Epoch: 2 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 531.925/s, 531.925/s/gpu LR: 9.966875e-09 Logit Scale: 100.000 Contrastive_loss: 0.36851 (0.35094) Loss: 0.36851 (0.35094)
2024-08-29,16:53:46 | INFO | Train Epoch: 2 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.196, 526.753/s, 526.753/s/gpu LR: 9.965661e-09 Logit Scale: 100.000 Contrastive_loss: 0.33494 (0.35039) Loss: 0.33494 (0.35039)
2024-08-29,16:54:05 | INFO | Train Epoch: 2 [280100/145000.0 (193%)] Data (t): 0.110 Batch (t): 0.197, 532.295/s, 532.295/s/gpu LR: 9.964425e-09 Logit Scale: 100.000 Contrastive_loss: 0.28259 (0.34813) Loss: 0.28259 (0.34813)
2024-08-29,16:54:25 | INFO | Train Epoch: 2 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 522.958/s, 522.958/s/gpu LR: 9.963167e-09 Logit Scale: 100.000 Contrastive_loss: 0.21076 (0.34370) Loss: 0.21076 (0.34370)
2024-08-29,16:54:44 | INFO | Train Epoch: 2 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.196, 506.290/s, 506.290/s/gpu LR: 9.961888e-09 Logit Scale: 100.000 Contrastive_loss: 0.39217 (0.34522) Loss: 0.39217 (0.34522)
2024-08-29,16:55:03 | INFO | Train Epoch: 2 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 530.058/s, 530.058/s/gpu LR: 9.960586e-09 Logit Scale: 100.000 Contrastive_loss: 0.48158 (0.34935) Loss: 0.48158 (0.34935)
2024-08-29,16:55:23 | INFO | Train Epoch: 2 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.194, 494.841/s, 494.841/s/gpu LR: 9.959263e-09 Logit Scale: 100.000 Contrastive_loss: 0.43497 (0.35187) Loss: 0.43497 (0.35187)
2024-08-29,16:55:42 | INFO | Train Epoch: 2 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.196, 502.435/s, 502.435/s/gpu LR: 9.957918e-09 Logit Scale: 100.000 Contrastive_loss: 0.25516 (0.34910) Loss: 0.25516 (0.34910)
2024-08-29,16:56:02 | INFO | Train Epoch: 2 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.197, 498.342/s, 498.342/s/gpu LR: 9.956551e-09 Logit Scale: 100.000 Contrastive_loss: 0.45961 (0.35217) Loss: 0.45961 (0.35217)
2024-08-29,16:56:22 | INFO | Train Epoch: 2 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.196, 501.479/s, 501.479/s/gpu LR: 9.955163e-09 Logit Scale: 100.000 Contrastive_loss: 0.51634 (0.35661) Loss: 0.51634 (0.35661)
2024-08-29,16:56:41 | INFO | Train Epoch: 2 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.196, 493.587/s, 493.587/s/gpu LR: 9.953753e-09 Logit Scale: 100.000 Contrastive_loss: 0.26460 (0.35419) Loss: 0.26460 (0.35419)
2024-08-29,16:57:01 | INFO | Train Epoch: 2 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.195, 503.670/s, 503.670/s/gpu LR: 9.952321e-09 Logit Scale: 100.000 Contrastive_loss: 0.38786 (0.35505) Loss: 0.38786 (0.35505)
2024-08-29,16:57:20 | INFO | Train Epoch: 2 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.195, 528.539/s, 528.539/s/gpu LR: 9.950867e-09 Logit Scale: 100.000 Contrastive_loss: 0.35551 (0.35506) Loss: 0.35551 (0.35506)
2024-08-29,16:57:40 | INFO | Train Epoch: 2 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.196, 532.778/s, 532.778/s/gpu LR: 9.949392e-09 Logit Scale: 100.000 Contrastive_loss: 0.26451 (0.35285) Loss: 0.26451 (0.35285)
2024-08-29,16:58:00 | INFO | Train Epoch: 2 [400100/145000.0 (276%)] Data (t): 0.111 Batch (t): 0.198, 524.506/s, 524.506/s/gpu LR: 9.947895e-09 Logit Scale: 100.000 Contrastive_loss: 0.33088 (0.35233) Loss: 0.33088 (0.35233)
2024-08-29,16:58:19 | INFO | Train Epoch: 2 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.195, 520.019/s, 520.019/s/gpu LR: 9.946376e-09 Logit Scale: 100.000 Contrastive_loss: 0.13698 (0.34732) Loss: 0.13698 (0.34732)
2024-08-29,16:58:27 | INFO | Eval Epoch: 3 [200 / 1000]	Clip Loss: 0.626936	
2024-08-29,16:58:27 | INFO | Eval Epoch: 3 image_to_text_mean_rank: 2.8420	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6340	image_to_text_R@5: 0.8900	image_to_text_R@10: 0.9520	text_to_image_mean_rank: 3.6890	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6190	text_to_image_R@5: 0.8790	text_to_image_R@10: 0.9470	clip_val_loss: 0.5346	epoch: 3.0000	num_samples: 1000.0000
2024-08-29,16:58:29 | INFO | Start epoch 3
2024-08-29,16:58:29 | INFO | Train Epoch: 3 [   100/145000.0 (0%)] Data (t): 0.025 Batch (t): 0.111, 900.193/s, 900.193/s/gpu LR: 9.979273e-09 Logit Scale: 100.000 Contrastive_loss: 0.29155 (0.29155) Loss: 0.29155 (0.29155)
2024-08-29,16:58:48 | INFO | Train Epoch: 3 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.195, 492.236/s, 492.236/s/gpu LR: 9.978309e-09 Logit Scale: 100.000 Contrastive_loss: 0.51585 (0.40370) Loss: 0.51585 (0.40370)
2024-08-29,16:59:08 | INFO | Train Epoch: 3 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.194, 522.960/s, 522.960/s/gpu LR: 9.977324e-09 Logit Scale: 100.000 Contrastive_loss: 0.52634 (0.44458) Loss: 0.52634 (0.44458)
2024-08-29,16:59:27 | INFO | Train Epoch: 3 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.195, 521.498/s, 521.498/s/gpu LR: 9.976317e-09 Logit Scale: 100.000 Contrastive_loss: 0.33321 (0.41674) Loss: 0.33321 (0.41674)
2024-08-29,16:59:47 | INFO | Train Epoch: 3 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.197, 455.509/s, 455.509/s/gpu LR: 9.975288e-09 Logit Scale: 100.000 Contrastive_loss: 0.21349 (0.37609) Loss: 0.21349 (0.37609)
2024-08-29,17:00:07 | INFO | Train Epoch: 3 [ 50100/145000.0 (35%)] Data (t): 0.110 Batch (t): 0.196, 525.416/s, 525.416/s/gpu LR: 9.974237e-09 Logit Scale: 100.000 Contrastive_loss: 0.24234 (0.35380) Loss: 0.24234 (0.35380)
2024-08-29,17:00:26 | INFO | Train Epoch: 3 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.194, 521.738/s, 521.738/s/gpu LR: 9.973165e-09 Logit Scale: 100.000 Contrastive_loss: 0.20307 (0.33226) Loss: 0.20307 (0.33226)
2024-08-29,17:00:46 | INFO | Train Epoch: 3 [ 70100/145000.0 (48%)] Data (t): 0.110 Batch (t): 0.197, 529.805/s, 529.805/s/gpu LR: 9.972070e-09 Logit Scale: 100.000 Contrastive_loss: 0.35936 (0.33565) Loss: 0.35936 (0.33565)
2024-08-29,17:01:05 | INFO | Train Epoch: 3 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.195, 529.154/s, 529.154/s/gpu LR: 9.970954e-09 Logit Scale: 100.000 Contrastive_loss: 0.24569 (0.32566) Loss: 0.24569 (0.32566)
2024-08-29,17:01:25 | INFO | Train Epoch: 3 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.195, 531.154/s, 531.154/s/gpu LR: 9.969816e-09 Logit Scale: 100.000 Contrastive_loss: 0.31327 (0.32442) Loss: 0.31327 (0.32442)
2024-08-29,17:01:44 | INFO | Train Epoch: 3 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.195, 487.784/s, 487.784/s/gpu LR: 9.968656e-09 Logit Scale: 100.000 Contrastive_loss: 0.22585 (0.31546) Loss: 0.22585 (0.31546)
2024-08-29,17:02:03 | INFO | Train Epoch: 3 [110100/145000.0 (76%)] Data (t): 0.107 Batch (t): 0.192, 496.921/s, 496.921/s/gpu LR: 9.967474e-09 Logit Scale: 100.000 Contrastive_loss: 0.29592 (0.31383) Loss: 0.29592 (0.31383)
2024-08-29,17:02:23 | INFO | Train Epoch: 3 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 524.803/s, 524.803/s/gpu LR: 9.966271e-09 Logit Scale: 100.000 Contrastive_loss: 0.45286 (0.32452) Loss: 0.45286 (0.32452)
2024-08-29,17:02:42 | INFO | Train Epoch: 3 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.196, 472.916/s, 472.916/s/gpu LR: 9.965046e-09 Logit Scale: 100.000 Contrastive_loss: 0.23586 (0.31819) Loss: 0.23586 (0.31819)
2024-08-29,17:03:02 | INFO | Train Epoch: 3 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.195, 528.319/s, 528.319/s/gpu LR: 9.963799e-09 Logit Scale: 100.000 Contrastive_loss: 0.26885 (0.31490) Loss: 0.26885 (0.31490)
2024-08-29,17:03:11 | INFO | Train Epoch: 3 [145000/145000.0 (100%)] Data (t): 0.110 Batch (t): 0.196, 462.216/s, 462.216/s/gpu LR: 9.963180e-09 Logit Scale: 100.000 Contrastive_loss: 0.38315 (0.31917) Loss: 0.38315 (0.31917)
2024-08-29,17:03:21 | INFO | Train Epoch: 3 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.196, 524.352/s, 524.352/s/gpu LR: 9.962530e-09 Logit Scale: 100.000 Contrastive_loss: 0.35567 (0.32131) Loss: 0.35567 (0.32131)
2024-08-29,17:03:41 | INFO | Train Epoch: 3 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.196, 529.449/s, 529.449/s/gpu LR: 9.961240e-09 Logit Scale: 100.000 Contrastive_loss: 0.43479 (0.32762) Loss: 0.43479 (0.32762)
2024-08-29,17:04:00 | INFO | Train Epoch: 3 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.195, 497.917/s, 497.917/s/gpu LR: 9.959927e-09 Logit Scale: 100.000 Contrastive_loss: 0.30132 (0.32623) Loss: 0.30132 (0.32623)
2024-08-29,17:04:20 | INFO | Train Epoch: 3 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.195, 528.434/s, 528.434/s/gpu LR: 9.958593e-09 Logit Scale: 100.000 Contrastive_loss: 0.31938 (0.32589) Loss: 0.31938 (0.32589)
2024-08-29,17:04:39 | INFO | Train Epoch: 3 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.195, 524.639/s, 524.639/s/gpu LR: 9.957237e-09 Logit Scale: 100.000 Contrastive_loss: 0.36897 (0.32794) Loss: 0.36897 (0.32794)
2024-08-29,17:04:59 | INFO | Train Epoch: 3 [200100/145000.0 (138%)] Data (t): 0.110 Batch (t): 0.197, 518.078/s, 518.078/s/gpu LR: 9.955860e-09 Logit Scale: 100.000 Contrastive_loss: 0.22930 (0.32346) Loss: 0.22930 (0.32346)
2024-08-29,17:05:18 | INFO | Train Epoch: 3 [210100/145000.0 (145%)] Data (t): 0.107 Batch (t): 0.193, 495.853/s, 495.853/s/gpu LR: 9.954461e-09 Logit Scale: 100.000 Contrastive_loss: 0.36575 (0.32530) Loss: 0.36575 (0.32530)
2024-08-29,17:05:38 | INFO | Train Epoch: 3 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.195, 527.728/s, 527.728/s/gpu LR: 9.953040e-09 Logit Scale: 100.000 Contrastive_loss: 0.48757 (0.33206) Loss: 0.48757 (0.33206)
2024-08-29,17:05:57 | INFO | Train Epoch: 3 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.197, 486.152/s, 486.152/s/gpu LR: 9.951597e-09 Logit Scale: 100.000 Contrastive_loss: 0.38032 (0.33399) Loss: 0.38032 (0.33399)
2024-08-29,17:06:17 | INFO | Train Epoch: 3 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.194, 531.279/s, 531.279/s/gpu LR: 9.950132e-09 Logit Scale: 100.000 Contrastive_loss: 0.28741 (0.33220) Loss: 0.28741 (0.33220)
2024-08-29,17:06:36 | INFO | Train Epoch: 3 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.195, 523.536/s, 523.536/s/gpu LR: 9.948646e-09 Logit Scale: 100.000 Contrastive_loss: 0.30877 (0.33133) Loss: 0.30877 (0.33133)
2024-08-29,17:06:56 | INFO | Train Epoch: 3 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.195, 484.923/s, 484.923/s/gpu LR: 9.947138e-09 Logit Scale: 100.000 Contrastive_loss: 0.34918 (0.33197) Loss: 0.34918 (0.33197)
2024-08-29,17:07:15 | INFO | Train Epoch: 3 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.194, 498.213/s, 498.213/s/gpu LR: 9.945609e-09 Logit Scale: 100.000 Contrastive_loss: 0.32605 (0.33176) Loss: 0.32605 (0.33176)
2024-08-29,17:07:35 | INFO | Train Epoch: 3 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.195, 525.898/s, 525.898/s/gpu LR: 9.944057e-09 Logit Scale: 100.000 Contrastive_loss: 0.27043 (0.32972) Loss: 0.27043 (0.32972)
2024-08-29,17:07:54 | INFO | Train Epoch: 3 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.195, 538.490/s, 538.490/s/gpu LR: 9.942485e-09 Logit Scale: 100.000 Contrastive_loss: 0.20287 (0.32563) Loss: 0.20287 (0.32563)
2024-08-29,17:08:14 | INFO | Train Epoch: 3 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.194, 503.566/s, 503.566/s/gpu LR: 9.940890e-09 Logit Scale: 100.000 Contrastive_loss: 0.37621 (0.32721) Loss: 0.37621 (0.32721)
2024-08-29,17:08:33 | INFO | Train Epoch: 3 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.195, 528.981/s, 528.981/s/gpu LR: 9.939274e-09 Logit Scale: 100.000 Contrastive_loss: 0.44469 (0.33077) Loss: 0.44469 (0.33077)
2024-08-29,17:08:53 | INFO | Train Epoch: 3 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.195, 523.545/s, 523.545/s/gpu LR: 9.937636e-09 Logit Scale: 100.000 Contrastive_loss: 0.41817 (0.33334) Loss: 0.41817 (0.33334)
2024-08-29,17:09:12 | INFO | Train Epoch: 3 [330100/145000.0 (228%)] Data (t): 0.110 Batch (t): 0.197, 500.919/s, 500.919/s/gpu LR: 9.935976e-09 Logit Scale: 100.000 Contrastive_loss: 0.24699 (0.33087) Loss: 0.24699 (0.33087)
2024-08-29,17:09:32 | INFO | Train Epoch: 3 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.194, 528.895/s, 528.895/s/gpu LR: 9.934295e-09 Logit Scale: 100.000 Contrastive_loss: 0.43761 (0.33384) Loss: 0.43761 (0.33384)
2024-08-29,17:09:51 | INFO | Train Epoch: 3 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.195, 519.151/s, 519.151/s/gpu LR: 9.932592e-09 Logit Scale: 100.000 Contrastive_loss: 0.48491 (0.33792) Loss: 0.48491 (0.33792)
2024-08-29,17:10:11 | INFO | Train Epoch: 3 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.196, 522.126/s, 522.126/s/gpu LR: 9.930867e-09 Logit Scale: 100.000 Contrastive_loss: 0.24868 (0.33557) Loss: 0.24868 (0.33557)
2024-08-29,17:10:30 | INFO | Train Epoch: 3 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.195, 521.696/s, 521.696/s/gpu LR: 9.929121e-09 Logit Scale: 100.000 Contrastive_loss: 0.36789 (0.33640) Loss: 0.36789 (0.33640)
2024-08-29,17:10:50 | INFO | Train Epoch: 3 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.195, 491.094/s, 491.094/s/gpu LR: 9.927353e-09 Logit Scale: 100.000 Contrastive_loss: 0.34412 (0.33659) Loss: 0.34412 (0.33659)
2024-08-29,17:11:09 | INFO | Train Epoch: 3 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.194, 499.662/s, 499.662/s/gpu LR: 9.925564e-09 Logit Scale: 100.000 Contrastive_loss: 0.25045 (0.33449) Loss: 0.25045 (0.33449)
2024-08-29,17:11:29 | INFO | Train Epoch: 3 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.195, 528.661/s, 528.661/s/gpu LR: 9.923753e-09 Logit Scale: 100.000 Contrastive_loss: 0.31935 (0.33413) Loss: 0.31935 (0.33413)
2024-08-29,17:11:48 | INFO | Train Epoch: 3 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.197, 517.881/s, 517.881/s/gpu LR: 9.921920e-09 Logit Scale: 100.000 Contrastive_loss: 0.13132 (0.32941) Loss: 0.13132 (0.32941)
2024-08-29,17:11:56 | INFO | Eval Epoch: 4 [200 / 1000]	Clip Loss: 0.598513	
2024-08-29,17:11:57 | INFO | Eval Epoch: 4 image_to_text_mean_rank: 2.7880	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6330	image_to_text_R@5: 0.8960	image_to_text_R@10: 0.9550	text_to_image_mean_rank: 3.6190	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6300	text_to_image_R@5: 0.8830	text_to_image_R@10: 0.9460	clip_val_loss: 0.5176	epoch: 4.0000	num_samples: 1000.0000
2024-08-29,17:11:58 | INFO | Start epoch 4
2024-08-29,17:11:58 | INFO | Train Epoch: 4 [   100/145000.0 (0%)] Data (t): 0.024 Batch (t): 0.116, 862.092/s, 862.092/s/gpu LR: 9.963167e-09 Logit Scale: 100.000 Contrastive_loss: 0.27929 (0.27929) Loss: 0.27929 (0.27929)
2024-08-29,17:12:18 | INFO | Train Epoch: 4 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.195, 500.121/s, 500.121/s/gpu LR: 9.961888e-09 Logit Scale: 100.000 Contrastive_loss: 0.49288 (0.38608) Loss: 0.49288 (0.38608)
2024-08-29,17:12:37 | INFO | Train Epoch: 4 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.194, 497.833/s, 497.833/s/gpu LR: 9.960586e-09 Logit Scale: 100.000 Contrastive_loss: 0.50169 (0.42462) Loss: 0.50169 (0.42462)
2024-08-29,17:12:57 | INFO | Train Epoch: 4 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 530.292/s, 530.292/s/gpu LR: 9.959263e-09 Logit Scale: 100.000 Contrastive_loss: 0.31254 (0.39660) Loss: 0.31254 (0.39660)
2024-08-29,17:13:16 | INFO | Train Epoch: 4 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.194, 502.574/s, 502.574/s/gpu LR: 9.957918e-09 Logit Scale: 100.000 Contrastive_loss: 0.20531 (0.35834) Loss: 0.20531 (0.35834)
2024-08-29,17:13:35 | INFO | Train Epoch: 4 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.195, 523.951/s, 523.951/s/gpu LR: 9.956551e-09 Logit Scale: 100.000 Contrastive_loss: 0.23487 (0.33776) Loss: 0.23487 (0.33776)
2024-08-29,17:13:55 | INFO | Train Epoch: 4 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.194, 526.462/s, 526.462/s/gpu LR: 9.955163e-09 Logit Scale: 100.000 Contrastive_loss: 0.19355 (0.31716) Loss: 0.19355 (0.31716)
2024-08-29,17:14:14 | INFO | Train Epoch: 4 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.194, 529.744/s, 529.744/s/gpu LR: 9.953753e-09 Logit Scale: 100.000 Contrastive_loss: 0.34873 (0.32111) Loss: 0.34873 (0.32111)
2024-08-29,17:14:34 | INFO | Train Epoch: 4 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.195, 532.846/s, 532.846/s/gpu LR: 9.952321e-09 Logit Scale: 100.000 Contrastive_loss: 0.23461 (0.31150) Loss: 0.23461 (0.31150)
2024-08-29,17:14:53 | INFO | Train Epoch: 4 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.196, 457.576/s, 457.576/s/gpu LR: 9.950867e-09 Logit Scale: 100.000 Contrastive_loss: 0.30461 (0.31081) Loss: 0.30461 (0.31081)
2024-08-29,17:15:13 | INFO | Train Epoch: 4 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.195, 537.885/s, 537.885/s/gpu LR: 9.949392e-09 Logit Scale: 100.000 Contrastive_loss: 0.22007 (0.30256) Loss: 0.22007 (0.30256)
2024-08-29,17:15:32 | INFO | Train Epoch: 4 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.195, 525.568/s, 525.568/s/gpu LR: 9.947895e-09 Logit Scale: 100.000 Contrastive_loss: 0.27915 (0.30061) Loss: 0.27915 (0.30061)
2024-08-29,17:15:52 | INFO | Train Epoch: 4 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.196, 501.922/s, 501.922/s/gpu LR: 9.946376e-09 Logit Scale: 100.000 Contrastive_loss: 0.43286 (0.31078) Loss: 0.43286 (0.31078)
2024-08-29,17:16:11 | INFO | Train Epoch: 4 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.195, 523.596/s, 523.596/s/gpu LR: 9.944836e-09 Logit Scale: 100.000 Contrastive_loss: 0.23147 (0.30512) Loss: 0.23147 (0.30512)
2024-08-29,17:16:31 | INFO | Train Epoch: 4 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.196, 499.388/s, 499.388/s/gpu LR: 9.943274e-09 Logit Scale: 100.000 Contrastive_loss: 0.25797 (0.30197) Loss: 0.25797 (0.30197)
2024-08-29,17:16:41 | INFO | Train Epoch: 4 [145000/145000.0 (100%)] Data (t): 0.112 Batch (t): 0.199, 494.149/s, 494.149/s/gpu LR: 9.942500e-09 Logit Scale: 100.000 Contrastive_loss: 0.36179 (0.30571) Loss: 0.36179 (0.30571)
2024-08-29,17:16:51 | INFO | Train Epoch: 4 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.194, 527.076/s, 527.076/s/gpu LR: 9.941690e-09 Logit Scale: 100.000 Contrastive_loss: 0.33830 (0.30763) Loss: 0.33830 (0.30763)
2024-08-29,17:17:10 | INFO | Train Epoch: 4 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.195, 526.250/s, 526.250/s/gpu LR: 9.940084e-09 Logit Scale: 100.000 Contrastive_loss: 0.41408 (0.31354) Loss: 0.41408 (0.31354)
2024-08-29,17:17:29 | INFO | Train Epoch: 4 [170100/145000.0 (117%)] Data (t): 0.107 Batch (t): 0.193, 521.569/s, 521.569/s/gpu LR: 9.938457e-09 Logit Scale: 100.000 Contrastive_loss: 0.28820 (0.31221) Loss: 0.28820 (0.31221)
2024-08-29,17:17:49 | INFO | Train Epoch: 4 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.195, 483.068/s, 483.068/s/gpu LR: 9.936808e-09 Logit Scale: 100.000 Contrastive_loss: 0.30157 (0.31168) Loss: 0.30157 (0.31168)
2024-08-29,17:18:08 | INFO | Train Epoch: 4 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.195, 522.119/s, 522.119/s/gpu LR: 9.935138e-09 Logit Scale: 100.000 Contrastive_loss: 0.35865 (0.31391) Loss: 0.35865 (0.31391)
2024-08-29,17:18:28 | INFO | Train Epoch: 4 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.194, 495.780/s, 495.780/s/gpu LR: 9.933446e-09 Logit Scale: 100.000 Contrastive_loss: 0.22245 (0.30976) Loss: 0.22245 (0.30976)
2024-08-29,17:18:47 | INFO | Train Epoch: 4 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.194, 501.692/s, 501.692/s/gpu LR: 9.931732e-09 Logit Scale: 100.000 Contrastive_loss: 0.35444 (0.31170) Loss: 0.35444 (0.31170)
2024-08-29,17:19:07 | INFO | Train Epoch: 4 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.195, 527.605/s, 527.605/s/gpu LR: 9.929997e-09 Logit Scale: 100.000 Contrastive_loss: 0.47289 (0.31842) Loss: 0.47289 (0.31842)
2024-08-29,17:19:26 | INFO | Train Epoch: 4 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.196, 503.995/s, 503.995/s/gpu LR: 9.928240e-09 Logit Scale: 100.000 Contrastive_loss: 0.37100 (0.32052) Loss: 0.37100 (0.32052)
2024-08-29,17:19:46 | INFO | Train Epoch: 4 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.195, 524.710/s, 524.710/s/gpu LR: 9.926461e-09 Logit Scale: 100.000 Contrastive_loss: 0.27316 (0.31870) Loss: 0.27316 (0.31870)
2024-08-29,17:20:05 | INFO | Train Epoch: 4 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 505.150/s, 505.150/s/gpu LR: 9.924661e-09 Logit Scale: 100.000 Contrastive_loss: 0.29911 (0.31797) Loss: 0.29911 (0.31797)
2024-08-29,17:20:25 | INFO | Train Epoch: 4 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.195, 527.044/s, 527.044/s/gpu LR: 9.922839e-09 Logit Scale: 100.000 Contrastive_loss: 0.33602 (0.31862) Loss: 0.33602 (0.31862)
2024-08-29,17:20:44 | INFO | Train Epoch: 4 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.195, 524.605/s, 524.605/s/gpu LR: 9.920996e-09 Logit Scale: 100.000 Contrastive_loss: 0.31954 (0.31865) Loss: 0.31954 (0.31865)
2024-08-29,17:21:04 | INFO | Train Epoch: 4 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.195, 533.633/s, 533.633/s/gpu LR: 9.919131e-09 Logit Scale: 100.000 Contrastive_loss: 0.26070 (0.31672) Loss: 0.26070 (0.31672)
2024-08-29,17:21:23 | INFO | Train Epoch: 4 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.197, 502.472/s, 502.472/s/gpu LR: 9.917244e-09 Logit Scale: 100.000 Contrastive_loss: 0.19697 (0.31285) Loss: 0.19697 (0.31285)
2024-08-29,17:21:43 | INFO | Train Epoch: 4 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.194, 525.848/s, 525.848/s/gpu LR: 9.915336e-09 Logit Scale: 100.000 Contrastive_loss: 0.36443 (0.31447) Loss: 0.36443 (0.31447)
2024-08-29,17:22:02 | INFO | Train Epoch: 4 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.195, 516.856/s, 516.856/s/gpu LR: 9.913406e-09 Logit Scale: 100.000 Contrastive_loss: 0.41701 (0.31757) Loss: 0.41701 (0.31757)
2024-08-29,17:22:22 | INFO | Train Epoch: 4 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.194, 521.438/s, 521.438/s/gpu LR: 9.911455e-09 Logit Scale: 100.000 Contrastive_loss: 0.40680 (0.32020) Loss: 0.40680 (0.32020)
2024-08-29,17:22:41 | INFO | Train Epoch: 4 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.197, 487.088/s, 487.088/s/gpu LR: 9.909482e-09 Logit Scale: 100.000 Contrastive_loss: 0.24104 (0.31794) Loss: 0.24104 (0.31794)
2024-08-29,17:23:01 | INFO | Train Epoch: 4 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.195, 529.059/s, 529.059/s/gpu LR: 9.907487e-09 Logit Scale: 100.000 Contrastive_loss: 0.42095 (0.32080) Loss: 0.42095 (0.32080)
2024-08-29,17:23:20 | INFO | Train Epoch: 4 [350100/145000.0 (241%)] Data (t): 0.107 Batch (t): 0.192, 530.913/s, 530.913/s/gpu LR: 9.905472e-09 Logit Scale: 100.000 Contrastive_loss: 0.46309 (0.32464) Loss: 0.46309 (0.32464)
2024-08-29,17:23:39 | INFO | Train Epoch: 4 [360100/145000.0 (248%)] Data (t): 0.107 Batch (t): 0.192, 494.636/s, 494.636/s/gpu LR: 9.903434e-09 Logit Scale: 100.000 Contrastive_loss: 0.23778 (0.32236) Loss: 0.23778 (0.32236)
2024-08-29,17:23:58 | INFO | Train Epoch: 4 [370100/145000.0 (255%)] Data (t): 0.107 Batch (t): 0.192, 501.820/s, 501.820/s/gpu LR: 9.901375e-09 Logit Scale: 100.000 Contrastive_loss: 0.35390 (0.32317) Loss: 0.35390 (0.32317)
2024-08-29,17:24:17 | INFO | Train Epoch: 4 [380100/145000.0 (262%)] Data (t): 0.107 Batch (t): 0.191, 497.945/s, 497.945/s/gpu LR: 9.899295e-09 Logit Scale: 100.000 Contrastive_loss: 0.33636 (0.32349) Loss: 0.33636 (0.32349)
2024-08-29,17:24:37 | INFO | Train Epoch: 4 [390100/145000.0 (269%)] Data (t): 0.107 Batch (t): 0.192, 532.194/s, 532.194/s/gpu LR: 9.897193e-09 Logit Scale: 100.000 Contrastive_loss: 0.23951 (0.32145) Loss: 0.23951 (0.32145)
2024-08-29,17:24:56 | INFO | Train Epoch: 4 [400100/145000.0 (276%)] Data (t): 0.107 Batch (t): 0.192, 499.431/s, 499.431/s/gpu LR: 9.895070e-09 Logit Scale: 100.000 Contrastive_loss: 0.31088 (0.32119) Loss: 0.31088 (0.32119)
2024-08-29,17:25:15 | INFO | Train Epoch: 4 [410100/145000.0 (283%)] Data (t): 0.107 Batch (t): 0.191, 531.974/s, 531.974/s/gpu LR: 9.892925e-09 Logit Scale: 100.000 Contrastive_loss: 0.12865 (0.31672) Loss: 0.12865 (0.31672)
2024-08-29,17:25:22 | INFO | Eval Epoch: 5 [200 / 1000]	Clip Loss: 0.577974	
2024-08-29,17:25:23 | INFO | Eval Epoch: 5 image_to_text_mean_rank: 2.7400	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6370	image_to_text_R@5: 0.8940	image_to_text_R@10: 0.9530	text_to_image_mean_rank: 3.5570	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6330	text_to_image_R@5: 0.8830	text_to_image_R@10: 0.9490	clip_val_loss: 0.5054	epoch: 5.0000	num_samples: 1000.0000
2024-08-29,17:25:24 | INFO | Start epoch 5
2024-08-29,17:25:24 | INFO | Train Epoch: 5 [   100/145000.0 (0%)] Data (t): 0.029 Batch (t): 0.109, 919.398/s, 919.398/s/gpu LR: 9.942485e-09 Logit Scale: 100.000 Contrastive_loss: 0.26989 (0.26989) Loss: 0.26989 (0.26989)
2024-08-29,17:25:44 | INFO | Train Epoch: 5 [ 10100/145000.0 (7%)] Data (t): 0.108 Batch (t): 0.193, 522.771/s, 522.771/s/gpu LR: 9.940890e-09 Logit Scale: 100.000 Contrastive_loss: 0.47562 (0.37276) Loss: 0.47562 (0.37276)
2024-08-29,17:26:03 | INFO | Train Epoch: 5 [ 20100/145000.0 (14%)] Data (t): 0.107 Batch (t): 0.192, 490.327/s, 490.327/s/gpu LR: 9.939274e-09 Logit Scale: 100.000 Contrastive_loss: 0.48518 (0.41023) Loss: 0.48518 (0.41023)
2024-08-29,17:26:22 | INFO | Train Epoch: 5 [ 30100/145000.0 (21%)] Data (t): 0.107 Batch (t): 0.193, 494.934/s, 494.934/s/gpu LR: 9.937636e-09 Logit Scale: 100.000 Contrastive_loss: 0.29733 (0.38200) Loss: 0.29733 (0.38200)
2024-08-29,17:26:41 | INFO | Train Epoch: 5 [ 40100/145000.0 (28%)] Data (t): 0.106 Batch (t): 0.192, 525.925/s, 525.925/s/gpu LR: 9.935976e-09 Logit Scale: 100.000 Contrastive_loss: 0.19873 (0.34535) Loss: 0.19873 (0.34535)
2024-08-29,17:27:01 | INFO | Train Epoch: 5 [ 50100/145000.0 (35%)] Data (t): 0.107 Batch (t): 0.191, 528.781/s, 528.781/s/gpu LR: 9.934295e-09 Logit Scale: 100.000 Contrastive_loss: 0.22963 (0.32606) Loss: 0.22963 (0.32606)
2024-08-29,17:27:20 | INFO | Train Epoch: 5 [ 60100/145000.0 (41%)] Data (t): 0.107 Batch (t): 0.192, 529.459/s, 529.459/s/gpu LR: 9.932592e-09 Logit Scale: 100.000 Contrastive_loss: 0.18638 (0.30611) Loss: 0.18638 (0.30611)
2024-08-29,17:27:39 | INFO | Train Epoch: 5 [ 70100/145000.0 (48%)] Data (t): 0.107 Batch (t): 0.192, 528.328/s, 528.328/s/gpu LR: 9.930867e-09 Logit Scale: 100.000 Contrastive_loss: 0.34045 (0.31040) Loss: 0.34045 (0.31040)
2024-08-29,17:27:58 | INFO | Train Epoch: 5 [ 80100/145000.0 (55%)] Data (t): 0.107 Batch (t): 0.192, 502.717/s, 502.717/s/gpu LR: 9.929121e-09 Logit Scale: 100.000 Contrastive_loss: 0.22659 (0.30109) Loss: 0.22659 (0.30109)
2024-08-29,17:28:17 | INFO | Train Epoch: 5 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 529.765/s, 529.765/s/gpu LR: 9.927353e-09 Logit Scale: 100.000 Contrastive_loss: 0.29824 (0.30080) Loss: 0.29824 (0.30080)
2024-08-29,17:28:37 | INFO | Train Epoch: 5 [100100/145000.0 (69%)] Data (t): 0.110 Batch (t): 0.193, 535.564/s, 535.564/s/gpu LR: 9.925564e-09 Logit Scale: 100.000 Contrastive_loss: 0.21552 (0.29305) Loss: 0.21552 (0.29305)
2024-08-29,17:28:56 | INFO | Train Epoch: 5 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 518.565/s, 518.565/s/gpu LR: 9.923753e-09 Logit Scale: 100.000 Contrastive_loss: 0.26742 (0.29092) Loss: 0.26742 (0.29092)
2024-08-29,17:29:15 | INFO | Train Epoch: 5 [120100/145000.0 (83%)] Data (t): 0.107 Batch (t): 0.192, 501.225/s, 501.225/s/gpu LR: 9.921920e-09 Logit Scale: 100.000 Contrastive_loss: 0.41793 (0.30069) Loss: 0.41793 (0.30069)
2024-08-29,17:29:34 | INFO | Train Epoch: 5 [130100/145000.0 (90%)] Data (t): 0.106 Batch (t): 0.192, 537.631/s, 537.631/s/gpu LR: 9.920066e-09 Logit Scale: 100.000 Contrastive_loss: 0.22767 (0.29547) Loss: 0.22767 (0.29547)
2024-08-29,17:29:54 | INFO | Train Epoch: 5 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 527.428/s, 527.428/s/gpu LR: 9.918190e-09 Logit Scale: 100.000 Contrastive_loss: 0.25064 (0.29248) Loss: 0.25064 (0.29248)
2024-08-29,17:30:03 | INFO | Train Epoch: 5 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 536.545/s, 536.545/s/gpu LR: 9.917263e-09 Logit Scale: 100.000 Contrastive_loss: 0.34531 (0.29578) Loss: 0.34531 (0.29578)
2024-08-29,17:30:13 | INFO | Train Epoch: 5 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 505.833/s, 505.833/s/gpu LR: 9.916293e-09 Logit Scale: 100.000 Contrastive_loss: 0.32493 (0.29750) Loss: 0.32493 (0.29750)
2024-08-29,17:30:32 | INFO | Train Epoch: 5 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 489.945/s, 489.945/s/gpu LR: 9.914374e-09 Logit Scale: 100.000 Contrastive_loss: 0.39740 (0.30305) Loss: 0.39740 (0.30305)
2024-08-29,17:30:52 | INFO | Train Epoch: 5 [170100/145000.0 (117%)] Data (t): 0.107 Batch (t): 0.193, 519.129/s, 519.129/s/gpu LR: 9.912433e-09 Logit Scale: 100.000 Contrastive_loss: 0.27798 (0.30173) Loss: 0.27798 (0.30173)
2024-08-29,17:31:11 | INFO | Train Epoch: 5 [180100/145000.0 (124%)] Data (t): 0.110 Batch (t): 0.196, 532.470/s, 532.470/s/gpu LR: 9.910471e-09 Logit Scale: 100.000 Contrastive_loss: 0.28871 (0.30108) Loss: 0.28871 (0.30108)
2024-08-29,17:31:31 | INFO | Train Epoch: 5 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.196, 530.230/s, 530.230/s/gpu LR: 9.908487e-09 Logit Scale: 100.000 Contrastive_loss: 0.35006 (0.30341) Loss: 0.35006 (0.30341)
2024-08-29,17:31:50 | INFO | Train Epoch: 5 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.195, 528.344/s, 528.344/s/gpu LR: 9.906482e-09 Logit Scale: 100.000 Contrastive_loss: 0.21756 (0.29951) Loss: 0.21756 (0.29951)
2024-08-29,17:32:09 | INFO | Train Epoch: 5 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.194, 530.075/s, 530.075/s/gpu LR: 9.904456e-09 Logit Scale: 100.000 Contrastive_loss: 0.34557 (0.30151) Loss: 0.34557 (0.30151)
2024-08-29,17:32:29 | INFO | Train Epoch: 5 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.195, 496.839/s, 496.839/s/gpu LR: 9.902407e-09 Logit Scale: 100.000 Contrastive_loss: 0.46119 (0.30816) Loss: 0.46119 (0.30816)
2024-08-29,17:32:48 | INFO | Train Epoch: 5 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.195, 496.681/s, 496.681/s/gpu LR: 9.900338e-09 Logit Scale: 100.000 Contrastive_loss: 0.36461 (0.31042) Loss: 0.36461 (0.31042)
2024-08-29,17:33:08 | INFO | Train Epoch: 5 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.195, 491.337/s, 491.337/s/gpu LR: 9.898247e-09 Logit Scale: 100.000 Contrastive_loss: 0.26275 (0.30859) Loss: 0.26275 (0.30859)
2024-08-29,17:33:27 | INFO | Train Epoch: 5 [250100/145000.0 (172%)] Data (t): 0.107 Batch (t): 0.194, 531.368/s, 531.368/s/gpu LR: 9.896134e-09 Logit Scale: 100.000 Contrastive_loss: 0.29180 (0.30797) Loss: 0.29180 (0.30797)
2024-08-29,17:33:47 | INFO | Train Epoch: 5 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.194, 491.431/s, 491.431/s/gpu LR: 9.894000e-09 Logit Scale: 100.000 Contrastive_loss: 0.32592 (0.30861) Loss: 0.32592 (0.30861)
2024-08-29,17:34:06 | INFO | Train Epoch: 5 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.195, 499.850/s, 499.850/s/gpu LR: 9.891844e-09 Logit Scale: 100.000 Contrastive_loss: 0.31487 (0.30882) Loss: 0.31487 (0.30882)
2024-08-29,17:34:26 | INFO | Train Epoch: 5 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.195, 522.739/s, 522.739/s/gpu LR: 9.889667e-09 Logit Scale: 100.000 Contrastive_loss: 0.25324 (0.30697) Loss: 0.25324 (0.30697)
2024-08-29,17:34:45 | INFO | Train Epoch: 5 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.195, 515.082/s, 515.082/s/gpu LR: 9.887469e-09 Logit Scale: 100.000 Contrastive_loss: 0.19213 (0.30327) Loss: 0.19213 (0.30327)
2024-08-29,17:35:05 | INFO | Train Epoch: 5 [300100/145000.0 (207%)] Data (t): 0.107 Batch (t): 0.193, 524.972/s, 524.972/s/gpu LR: 9.885249e-09 Logit Scale: 100.000 Contrastive_loss: 0.35543 (0.30490) Loss: 0.35543 (0.30490)
2024-08-29,17:35:24 | INFO | Train Epoch: 5 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.196, 488.272/s, 488.272/s/gpu LR: 9.883007e-09 Logit Scale: 100.000 Contrastive_loss: 0.39557 (0.30764) Loss: 0.39557 (0.30764)
2024-08-29,17:35:44 | INFO | Train Epoch: 5 [320100/145000.0 (221%)] Data (t): 0.110 Batch (t): 0.196, 524.281/s, 524.281/s/gpu LR: 9.880744e-09 Logit Scale: 100.000 Contrastive_loss: 0.39869 (0.31032) Loss: 0.39869 (0.31032)
2024-08-29,17:36:03 | INFO | Train Epoch: 5 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.196, 519.274/s, 519.274/s/gpu LR: 9.878460e-09 Logit Scale: 100.000 Contrastive_loss: 0.23647 (0.30821) Loss: 0.23647 (0.30821)
2024-08-29,17:36:23 | INFO | Train Epoch: 5 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.194, 534.855/s, 534.855/s/gpu LR: 9.876155e-09 Logit Scale: 100.000 Contrastive_loss: 0.40757 (0.31097) Loss: 0.40757 (0.31097)
2024-08-29,17:36:42 | INFO | Train Epoch: 5 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.194, 497.515/s, 497.515/s/gpu LR: 9.873828e-09 Logit Scale: 100.000 Contrastive_loss: 0.44769 (0.31467) Loss: 0.44769 (0.31467)
2024-08-29,17:37:02 | INFO | Train Epoch: 5 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.195, 490.821/s, 490.821/s/gpu LR: 9.871479e-09 Logit Scale: 100.000 Contrastive_loss: 0.23037 (0.31245) Loss: 0.23037 (0.31245)
2024-08-29,17:37:21 | INFO | Train Epoch: 5 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.195, 536.696/s, 536.696/s/gpu LR: 9.869110e-09 Logit Scale: 100.000 Contrastive_loss: 0.34343 (0.31324) Loss: 0.34343 (0.31324)
2024-08-29,17:37:41 | INFO | Train Epoch: 5 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.196, 533.959/s, 533.959/s/gpu LR: 9.866718e-09 Logit Scale: 100.000 Contrastive_loss: 0.32998 (0.31366) Loss: 0.32998 (0.31366)
2024-08-29,17:38:00 | INFO | Train Epoch: 5 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 518.791/s, 518.791/s/gpu LR: 9.864306e-09 Logit Scale: 100.000 Contrastive_loss: 0.23052 (0.31163) Loss: 0.23052 (0.31163)
2024-08-29,17:38:20 | INFO | Train Epoch: 5 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.195, 528.401/s, 528.401/s/gpu LR: 9.861872e-09 Logit Scale: 100.000 Contrastive_loss: 0.30478 (0.31147) Loss: 0.30478 (0.31147)
2024-08-29,17:38:39 | INFO | Train Epoch: 5 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.194, 529.379/s, 529.379/s/gpu LR: 9.859417e-09 Logit Scale: 100.000 Contrastive_loss: 0.12688 (0.30718) Loss: 0.12688 (0.30718)
2024-08-29,17:38:47 | INFO | Eval Epoch: 6 [200 / 1000]	Clip Loss: 0.562782	
2024-08-29,17:38:47 | INFO | Eval Epoch: 6 image_to_text_mean_rank: 2.7040	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6390	image_to_text_R@5: 0.8960	image_to_text_R@10: 0.9550	text_to_image_mean_rank: 3.5310	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6350	text_to_image_R@5: 0.8830	text_to_image_R@10: 0.9490	clip_val_loss: 0.4964	epoch: 6.0000	num_samples: 1000.0000
2024-08-29,17:38:49 | INFO | Start epoch 6
2024-08-29,17:38:49 | INFO | Train Epoch: 6 [   100/145000.0 (0%)] Data (t): 0.024 Batch (t): 0.108, 925.921/s, 925.921/s/gpu LR: 9.917244e-09 Logit Scale: 100.000 Contrastive_loss: 0.26232 (0.26232) Loss: 0.26232 (0.26232)
2024-08-29,17:39:09 | INFO | Train Epoch: 6 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.197, 527.408/s, 527.408/s/gpu LR: 9.915336e-09 Logit Scale: 100.000 Contrastive_loss: 0.46281 (0.36256) Loss: 0.46281 (0.36256)
2024-08-29,17:39:28 | INFO | Train Epoch: 6 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.194, 518.258/s, 518.258/s/gpu LR: 9.913406e-09 Logit Scale: 100.000 Contrastive_loss: 0.47399 (0.39971) Loss: 0.47399 (0.39971)
2024-08-29,17:39:48 | INFO | Train Epoch: 6 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.196, 524.958/s, 524.958/s/gpu LR: 9.911455e-09 Logit Scale: 100.000 Contrastive_loss: 0.28601 (0.37128) Loss: 0.28601 (0.37128)
2024-08-29,17:40:07 | INFO | Train Epoch: 6 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.196, 532.088/s, 532.088/s/gpu LR: 9.909482e-09 Logit Scale: 100.000 Contrastive_loss: 0.19369 (0.33576) Loss: 0.19369 (0.33576)
2024-08-29,17:40:27 | INFO | Train Epoch: 6 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.194, 524.104/s, 524.104/s/gpu LR: 9.907487e-09 Logit Scale: 100.000 Contrastive_loss: 0.22582 (0.31744) Loss: 0.22582 (0.31744)
2024-08-29,17:40:46 | INFO | Train Epoch: 6 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.195, 530.913/s, 530.913/s/gpu LR: 9.905472e-09 Logit Scale: 100.000 Contrastive_loss: 0.18051 (0.29788) Loss: 0.18051 (0.29788)
2024-08-29,17:41:06 | INFO | Train Epoch: 6 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.194, 523.567/s, 523.567/s/gpu LR: 9.903434e-09 Logit Scale: 100.000 Contrastive_loss: 0.33392 (0.30238) Loss: 0.33392 (0.30238)
2024-08-29,17:41:25 | INFO | Train Epoch: 6 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.195, 497.315/s, 497.315/s/gpu LR: 9.901375e-09 Logit Scale: 100.000 Contrastive_loss: 0.22026 (0.29326) Loss: 0.22026 (0.29326)
2024-08-29,17:41:45 | INFO | Train Epoch: 6 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.195, 499.658/s, 499.658/s/gpu LR: 9.899295e-09 Logit Scale: 100.000 Contrastive_loss: 0.29288 (0.29322) Loss: 0.29288 (0.29322)
2024-08-29,17:42:04 | INFO | Train Epoch: 6 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.195, 491.570/s, 491.570/s/gpu LR: 9.897193e-09 Logit Scale: 100.000 Contrastive_loss: 0.21185 (0.28582) Loss: 0.21185 (0.28582)
2024-08-29,17:42:24 | INFO | Train Epoch: 6 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.195, 508.100/s, 508.100/s/gpu LR: 9.895070e-09 Logit Scale: 100.000 Contrastive_loss: 0.25917 (0.28360) Loss: 0.25917 (0.28360)
2024-08-29,17:42:43 | INFO | Train Epoch: 6 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.194, 456.129/s, 456.129/s/gpu LR: 9.892925e-09 Logit Scale: 100.000 Contrastive_loss: 0.40698 (0.29309) Loss: 0.40698 (0.29309)
2024-08-29,17:43:03 | INFO | Train Epoch: 6 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.194, 497.060/s, 497.060/s/gpu LR: 9.890758e-09 Logit Scale: 100.000 Contrastive_loss: 0.22398 (0.28816) Loss: 0.22398 (0.28816)
2024-08-29,17:43:22 | INFO | Train Epoch: 6 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.196, 529.787/s, 529.787/s/gpu LR: 9.888571e-09 Logit Scale: 100.000 Contrastive_loss: 0.24470 (0.28526) Loss: 0.24470 (0.28526)
2024-08-29,17:43:32 | INFO | Train Epoch: 6 [145000/145000.0 (100%)] Data (t): 0.111 Batch (t): 0.198, 501.375/s, 501.375/s/gpu LR: 9.887491e-09 Logit Scale: 100.000 Contrastive_loss: 0.33247 (0.28821) Loss: 0.33247 (0.28821)
2024-08-29,17:43:42 | INFO | Train Epoch: 6 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.194, 507.583/s, 507.583/s/gpu LR: 9.886361e-09 Logit Scale: 100.000 Contrastive_loss: 0.31425 (0.28974) Loss: 0.31425 (0.28974)
2024-08-29,17:44:01 | INFO | Train Epoch: 6 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.197, 493.056/s, 493.056/s/gpu LR: 9.884131e-09 Logit Scale: 100.000 Contrastive_loss: 0.38330 (0.29494) Loss: 0.38330 (0.29494)
2024-08-29,17:44:21 | INFO | Train Epoch: 6 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.194, 529.600/s, 529.600/s/gpu LR: 9.881879e-09 Logit Scale: 100.000 Contrastive_loss: 0.26986 (0.29362) Loss: 0.26986 (0.29362)
2024-08-29,17:44:40 | INFO | Train Epoch: 6 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.195, 504.443/s, 504.443/s/gpu LR: 9.879605e-09 Logit Scale: 100.000 Contrastive_loss: 0.27890 (0.29288) Loss: 0.27890 (0.29288)
2024-08-29,17:45:00 | INFO | Train Epoch: 6 [190100/145000.0 (131%)] Data (t): 0.107 Batch (t): 0.193, 493.093/s, 493.093/s/gpu LR: 9.877310e-09 Logit Scale: 100.000 Contrastive_loss: 0.34298 (0.29527) Loss: 0.34298 (0.29527)
2024-08-29,17:45:19 | INFO | Train Epoch: 6 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.195, 496.305/s, 496.305/s/gpu LR: 9.874994e-09 Logit Scale: 100.000 Contrastive_loss: 0.21382 (0.29157) Loss: 0.21382 (0.29157)
2024-08-29,17:45:39 | INFO | Train Epoch: 6 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.195, 533.323/s, 533.323/s/gpu LR: 9.872656e-09 Logit Scale: 100.000 Contrastive_loss: 0.33839 (0.29360) Loss: 0.33839 (0.29360)
2024-08-29,17:45:58 | INFO | Train Epoch: 6 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.195, 502.377/s, 502.377/s/gpu LR: 9.870297e-09 Logit Scale: 100.000 Contrastive_loss: 0.45189 (0.30020) Loss: 0.45189 (0.30020)
2024-08-29,17:46:18 | INFO | Train Epoch: 6 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.196, 522.462/s, 522.462/s/gpu LR: 9.867917e-09 Logit Scale: 100.000 Contrastive_loss: 0.35996 (0.30259) Loss: 0.35996 (0.30259)
2024-08-29,17:46:37 | INFO | Train Epoch: 6 [240100/145000.0 (166%)] Data (t): 0.106 Batch (t): 0.191, 534.429/s, 534.429/s/gpu LR: 9.865515e-09 Logit Scale: 100.000 Contrastive_loss: 0.25449 (0.30074) Loss: 0.25449 (0.30074)
2024-08-29,17:46:56 | INFO | Train Epoch: 6 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.195, 535.421/s, 535.421/s/gpu LR: 9.863092e-09 Logit Scale: 100.000 Contrastive_loss: 0.28589 (0.30019) Loss: 0.28589 (0.30019)
2024-08-29,17:47:16 | INFO | Train Epoch: 6 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.194, 514.606/s, 514.606/s/gpu LR: 9.860647e-09 Logit Scale: 100.000 Contrastive_loss: 0.31812 (0.30083) Loss: 0.31812 (0.30083)
2024-08-29,17:47:35 | INFO | Train Epoch: 6 [270100/145000.0 (186%)] Data (t): 0.107 Batch (t): 0.192, 530.070/s, 530.070/s/gpu LR: 9.858182e-09 Logit Scale: 100.000 Contrastive_loss: 0.31103 (0.30118) Loss: 0.31103 (0.30118)
2024-08-29,17:47:55 | INFO | Train Epoch: 6 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.196, 505.052/s, 505.052/s/gpu LR: 9.855694e-09 Logit Scale: 100.000 Contrastive_loss: 0.24719 (0.29938) Loss: 0.24719 (0.29938)
2024-08-29,17:48:14 | INFO | Train Epoch: 6 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.195, 525.709/s, 525.709/s/gpu LR: 9.853186e-09 Logit Scale: 100.000 Contrastive_loss: 0.18845 (0.29580) Loss: 0.18845 (0.29580)
2024-08-29,17:48:33 | INFO | Train Epoch: 6 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.195, 523.203/s, 523.203/s/gpu LR: 9.850656e-09 Logit Scale: 100.000 Contrastive_loss: 0.34824 (0.29744) Loss: 0.34824 (0.29744)
2024-08-29,17:48:53 | INFO | Train Epoch: 6 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.196, 498.714/s, 498.714/s/gpu LR: 9.848105e-09 Logit Scale: 100.000 Contrastive_loss: 0.37848 (0.29990) Loss: 0.37848 (0.29990)
2024-08-29,17:49:12 | INFO | Train Epoch: 6 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 523.456/s, 523.456/s/gpu LR: 9.845533e-09 Logit Scale: 100.000 Contrastive_loss: 0.39232 (0.30262) Loss: 0.39232 (0.30262)
2024-08-29,17:49:32 | INFO | Train Epoch: 6 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.194, 485.741/s, 485.741/s/gpu LR: 9.842940e-09 Logit Scale: 100.000 Contrastive_loss: 0.23268 (0.30062) Loss: 0.23268 (0.30062)
2024-08-29,17:49:51 | INFO | Train Epoch: 6 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.195, 495.620/s, 495.620/s/gpu LR: 9.840325e-09 Logit Scale: 100.000 Contrastive_loss: 0.39667 (0.30329) Loss: 0.39667 (0.30329)
2024-08-29,17:50:11 | INFO | Train Epoch: 6 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.195, 537.324/s, 537.324/s/gpu LR: 9.837689e-09 Logit Scale: 100.000 Contrastive_loss: 0.43604 (0.30687) Loss: 0.43604 (0.30687)
2024-08-29,17:50:30 | INFO | Train Epoch: 6 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.195, 531.725/s, 531.725/s/gpu LR: 9.835032e-09 Logit Scale: 100.000 Contrastive_loss: 0.22453 (0.30471) Loss: 0.22453 (0.30471)
2024-08-29,17:50:50 | INFO | Train Epoch: 6 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.194, 494.003/s, 494.003/s/gpu LR: 9.832354e-09 Logit Scale: 100.000 Contrastive_loss: 0.33573 (0.30550) Loss: 0.33573 (0.30550)
2024-08-29,17:51:09 | INFO | Train Epoch: 6 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.195, 503.427/s, 503.427/s/gpu LR: 9.829654e-09 Logit Scale: 100.000 Contrastive_loss: 0.32460 (0.30598) Loss: 0.32460 (0.30598)
2024-08-29,17:51:29 | INFO | Train Epoch: 6 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 519.634/s, 519.634/s/gpu LR: 9.826933e-09 Logit Scale: 100.000 Contrastive_loss: 0.22310 (0.30396) Loss: 0.22310 (0.30396)
2024-08-29,17:51:48 | INFO | Train Epoch: 6 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.196, 527.119/s, 527.119/s/gpu LR: 9.824191e-09 Logit Scale: 100.000 Contrastive_loss: 0.29987 (0.30386) Loss: 0.29987 (0.30386)
2024-08-29,17:52:08 | INFO | Train Epoch: 6 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.195, 531.792/s, 531.792/s/gpu LR: 9.821428e-09 Logit Scale: 100.000 Contrastive_loss: 0.12587 (0.29972) Loss: 0.12587 (0.29972)
2024-08-29,17:52:15 | INFO | Eval Epoch: 7 [200 / 1000]	Clip Loss: 0.551031	
2024-08-29,17:52:16 | INFO | Eval Epoch: 7 image_to_text_mean_rank: 2.6770	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6370	image_to_text_R@5: 0.8980	image_to_text_R@10: 0.9590	text_to_image_mean_rank: 3.4980	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6390	text_to_image_R@5: 0.8840	text_to_image_R@10: 0.9500	clip_val_loss: 0.4893	epoch: 7.0000	num_samples: 1000.0000
2024-08-29,17:52:17 | INFO | Start epoch 7
2024-08-29,17:52:17 | INFO | Train Epoch: 7 [   100/145000.0 (0%)] Data (t): 0.026 Batch (t): 0.112, 889.359/s, 889.359/s/gpu LR: 9.887469e-09 Logit Scale: 100.000 Contrastive_loss: 0.25578 (0.25578) Loss: 0.25578 (0.25578)
2024-08-29,17:52:37 | INFO | Train Epoch: 7 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.196, 522.310/s, 522.310/s/gpu LR: 9.885249e-09 Logit Scale: 100.000 Contrastive_loss: 0.45285 (0.35432) Loss: 0.45285 (0.35432)
2024-08-29,17:52:57 | INFO | Train Epoch: 7 [ 20100/145000.0 (14%)] Data (t): 0.110 Batch (t): 0.197, 533.098/s, 533.098/s/gpu LR: 9.883007e-09 Logit Scale: 100.000 Contrastive_loss: 0.46604 (0.39156) Loss: 0.46604 (0.39156)
2024-08-29,17:53:16 | INFO | Train Epoch: 7 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.192, 495.084/s, 495.084/s/gpu LR: 9.880744e-09 Logit Scale: 100.000 Contrastive_loss: 0.27724 (0.36298) Loss: 0.27724 (0.36298)
2024-08-29,17:53:35 | INFO | Train Epoch: 7 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.196, 516.390/s, 516.390/s/gpu LR: 9.878460e-09 Logit Scale: 100.000 Contrastive_loss: 0.18938 (0.32826) Loss: 0.18938 (0.32826)
2024-08-29,17:53:55 | INFO | Train Epoch: 7 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.194, 498.168/s, 498.168/s/gpu LR: 9.876155e-09 Logit Scale: 100.000 Contrastive_loss: 0.22292 (0.31070) Loss: 0.22292 (0.31070)
2024-08-29,17:54:14 | INFO | Train Epoch: 7 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.195, 520.239/s, 520.239/s/gpu LR: 9.873828e-09 Logit Scale: 100.000 Contrastive_loss: 0.17577 (0.29142) Loss: 0.17577 (0.29142)
2024-08-29,17:54:34 | INFO | Train Epoch: 7 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.194, 532.649/s, 532.649/s/gpu LR: 9.871479e-09 Logit Scale: 100.000 Contrastive_loss: 0.32817 (0.29602) Loss: 0.32817 (0.29602)
2024-08-29,17:54:53 | INFO | Train Epoch: 7 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.194, 528.763/s, 528.763/s/gpu LR: 9.869110e-09 Logit Scale: 100.000 Contrastive_loss: 0.21514 (0.28703) Loss: 0.21514 (0.28703)
2024-08-29,17:55:13 | INFO | Train Epoch: 7 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.195, 505.603/s, 505.603/s/gpu LR: 9.866718e-09 Logit Scale: 100.000 Contrastive_loss: 0.28847 (0.28718) Loss: 0.28847 (0.28718)
2024-08-29,17:55:32 | INFO | Train Epoch: 7 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.196, 500.791/s, 500.791/s/gpu LR: 9.864306e-09 Logit Scale: 100.000 Contrastive_loss: 0.20846 (0.28002) Loss: 0.20846 (0.28002)
2024-08-29,17:55:52 | INFO | Train Epoch: 7 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.195, 532.762/s, 532.762/s/gpu LR: 9.861872e-09 Logit Scale: 100.000 Contrastive_loss: 0.25250 (0.27773) Loss: 0.25250 (0.27773)
2024-08-29,17:56:11 | INFO | Train Epoch: 7 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.196, 532.485/s, 532.485/s/gpu LR: 9.859417e-09 Logit Scale: 100.000 Contrastive_loss: 0.39766 (0.28695) Loss: 0.39766 (0.28695)
2024-08-29,17:56:31 | INFO | Train Epoch: 7 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.194, 534.888/s, 534.888/s/gpu LR: 9.856941e-09 Logit Scale: 100.000 Contrastive_loss: 0.22015 (0.28218) Loss: 0.22015 (0.28218)
2024-08-29,17:56:50 | INFO | Train Epoch: 7 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.195, 522.732/s, 522.732/s/gpu LR: 9.854443e-09 Logit Scale: 100.000 Contrastive_loss: 0.23997 (0.27937) Loss: 0.23997 (0.27937)
2024-08-29,17:57:00 | INFO | Train Epoch: 7 [145000/145000.0 (100%)] Data (t): 0.107 Batch (t): 0.193, 522.454/s, 522.454/s/gpu LR: 9.853211e-09 Logit Scale: 100.000 Contrastive_loss: 0.32262 (0.28207) Loss: 0.32262 (0.28207)
2024-08-29,17:57:09 | INFO | Train Epoch: 7 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.195, 521.593/s, 521.593/s/gpu LR: 9.851924e-09 Logit Scale: 100.000 Contrastive_loss: 0.30553 (0.28345) Loss: 0.30553 (0.28345)
2024-08-29,17:57:29 | INFO | Train Epoch: 7 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 500.754/s, 500.754/s/gpu LR: 9.849384e-09 Logit Scale: 100.000 Contrastive_loss: 0.37172 (0.28835) Loss: 0.37172 (0.28835)
2024-08-29,17:57:48 | INFO | Train Epoch: 7 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.195, 523.834/s, 523.834/s/gpu LR: 9.846822e-09 Logit Scale: 100.000 Contrastive_loss: 0.26310 (0.28702) Loss: 0.26310 (0.28702)
2024-08-29,17:58:08 | INFO | Train Epoch: 7 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.196, 492.978/s, 492.978/s/gpu LR: 9.844239e-09 Logit Scale: 100.000 Contrastive_loss: 0.27109 (0.28623) Loss: 0.27109 (0.28623)
2024-08-29,17:58:27 | INFO | Train Epoch: 7 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.195, 531.776/s, 531.776/s/gpu LR: 9.841635e-09 Logit Scale: 100.000 Contrastive_loss: 0.33752 (0.28867) Loss: 0.33752 (0.28867)
2024-08-29,17:58:47 | INFO | Train Epoch: 7 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.194, 505.816/s, 505.816/s/gpu LR: 9.839010e-09 Logit Scale: 100.000 Contrastive_loss: 0.21059 (0.28512) Loss: 0.21059 (0.28512)
2024-08-29,17:59:06 | INFO | Train Epoch: 7 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 501.324/s, 501.324/s/gpu LR: 9.836363e-09 Logit Scale: 100.000 Contrastive_loss: 0.33242 (0.28718) Loss: 0.33242 (0.28718)
2024-08-29,17:59:26 | INFO | Train Epoch: 7 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.196, 524.025/s, 524.025/s/gpu LR: 9.833695e-09 Logit Scale: 100.000 Contrastive_loss: 0.44415 (0.29372) Loss: 0.44415 (0.29372)
2024-08-29,17:59:45 | INFO | Train Epoch: 7 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 526.796/s, 526.796/s/gpu LR: 9.831006e-09 Logit Scale: 100.000 Contrastive_loss: 0.35636 (0.29622) Loss: 0.35636 (0.29622)
2024-08-29,18:00:04 | INFO | Train Epoch: 7 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.195, 531.593/s, 531.593/s/gpu LR: 9.828296e-09 Logit Scale: 100.000 Contrastive_loss: 0.24815 (0.29437) Loss: 0.24815 (0.29437)
2024-08-29,18:00:24 | INFO | Train Epoch: 7 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.194, 531.609/s, 531.609/s/gpu LR: 9.825565e-09 Logit Scale: 100.000 Contrastive_loss: 0.28121 (0.29389) Loss: 0.28121 (0.29389)
2024-08-29,18:00:44 | INFO | Train Epoch: 7 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.196, 530.277/s, 530.277/s/gpu LR: 9.822812e-09 Logit Scale: 100.000 Contrastive_loss: 0.31161 (0.29452) Loss: 0.31161 (0.29452)
2024-08-29,18:01:03 | INFO | Train Epoch: 7 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.194, 498.312/s, 498.312/s/gpu LR: 9.820039e-09 Logit Scale: 100.000 Contrastive_loss: 0.30796 (0.29498) Loss: 0.30796 (0.29498)
2024-08-29,18:01:22 | INFO | Train Epoch: 7 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.196, 530.897/s, 530.897/s/gpu LR: 9.817244e-09 Logit Scale: 100.000 Contrastive_loss: 0.24206 (0.29322) Loss: 0.24206 (0.29322)
2024-08-29,18:01:42 | INFO | Train Epoch: 7 [290100/145000.0 (200%)] Data (t): 0.107 Batch (t): 0.193, 501.467/s, 501.467/s/gpu LR: 9.814428e-09 Logit Scale: 100.000 Contrastive_loss: 0.18500 (0.28973) Loss: 0.18500 (0.28973)
2024-08-29,18:02:01 | INFO | Train Epoch: 7 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.195, 477.366/s, 477.366/s/gpu LR: 9.811591e-09 Logit Scale: 100.000 Contrastive_loss: 0.34212 (0.29137) Loss: 0.34212 (0.29137)
2024-08-29,18:02:21 | INFO | Train Epoch: 7 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.194, 501.181/s, 501.181/s/gpu LR: 9.808733e-09 Logit Scale: 100.000 Contrastive_loss: 0.36415 (0.29357) Loss: 0.36415 (0.29357)
2024-08-29,18:02:40 | INFO | Train Epoch: 7 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.195, 558.526/s, 558.526/s/gpu LR: 9.805853e-09 Logit Scale: 100.000 Contrastive_loss: 0.38658 (0.29631) Loss: 0.38658 (0.29631)
2024-08-29,18:03:00 | INFO | Train Epoch: 7 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.195, 497.116/s, 497.116/s/gpu LR: 9.802953e-09 Logit Scale: 100.000 Contrastive_loss: 0.22983 (0.29441) Loss: 0.22983 (0.29441)
2024-08-29,18:03:19 | INFO | Train Epoch: 7 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.196, 500.766/s, 500.766/s/gpu LR: 9.800032e-09 Logit Scale: 100.000 Contrastive_loss: 0.38715 (0.29698) Loss: 0.38715 (0.29698)
2024-08-29,18:03:39 | INFO | Train Epoch: 7 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.194, 502.670/s, 502.670/s/gpu LR: 9.797089e-09 Logit Scale: 100.000 Contrastive_loss: 0.42676 (0.30049) Loss: 0.42676 (0.30049)
2024-08-29,18:03:58 | INFO | Train Epoch: 7 [360100/145000.0 (248%)] Data (t): 0.110 Batch (t): 0.197, 526.977/s, 526.977/s/gpu LR: 9.794126e-09 Logit Scale: 100.000 Contrastive_loss: 0.21987 (0.29837) Loss: 0.21987 (0.29837)
2024-08-29,18:04:18 | INFO | Train Epoch: 7 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.196, 500.609/s, 500.609/s/gpu LR: 9.791141e-09 Logit Scale: 100.000 Contrastive_loss: 0.32959 (0.29917) Loss: 0.32959 (0.29917)
2024-08-29,18:04:37 | INFO | Train Epoch: 7 [380100/145000.0 (262%)] Data (t): 0.107 Batch (t): 0.193, 524.124/s, 524.124/s/gpu LR: 9.788136e-09 Logit Scale: 100.000 Contrastive_loss: 0.31950 (0.29968) Loss: 0.31950 (0.29968)
2024-08-29,18:04:57 | INFO | Train Epoch: 7 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.195, 525.703/s, 525.703/s/gpu LR: 9.785109e-09 Logit Scale: 100.000 Contrastive_loss: 0.21676 (0.29766) Loss: 0.21676 (0.29766)
2024-08-29,18:05:17 | INFO | Train Epoch: 7 [400100/145000.0 (276%)] Data (t): 0.111 Batch (t): 0.201, 454.692/s, 454.692/s/gpu LR: 9.782061e-09 Logit Scale: 100.000 Contrastive_loss: 0.29578 (0.29761) Loss: 0.29578 (0.29761)
2024-08-29,18:05:36 | INFO | Train Epoch: 7 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.196, 501.629/s, 501.629/s/gpu LR: 9.778993e-09 Logit Scale: 100.000 Contrastive_loss: 0.12490 (0.29359) Loss: 0.12490 (0.29359)
2024-08-29,18:05:44 | INFO | Eval Epoch: 8 [200 / 1000]	Clip Loss: 0.541556	
2024-08-29,18:05:45 | INFO | Eval Epoch: 8 image_to_text_mean_rank: 2.6510	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6370	image_to_text_R@5: 0.9000	image_to_text_R@10: 0.9600	text_to_image_mean_rank: 3.4720	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6440	text_to_image_R@5: 0.8880	text_to_image_R@10: 0.9520	clip_val_loss: 0.4836	epoch: 8.0000	num_samples: 1000.0000
2024-08-29,18:05:46 | INFO | Start epoch 8
2024-08-29,18:05:46 | INFO | Train Epoch: 8 [   100/145000.0 (0%)] Data (t): 0.025 Batch (t): 0.106, 942.682/s, 942.682/s/gpu LR: 9.853186e-09 Logit Scale: 100.000 Contrastive_loss: 0.24994 (0.24994) Loss: 0.24994 (0.24994)
2024-08-29,18:06:06 | INFO | Train Epoch: 8 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.195, 502.895/s, 502.895/s/gpu LR: 9.850656e-09 Logit Scale: 100.000 Contrastive_loss: 0.44479 (0.34736) Loss: 0.44479 (0.34736)
2024-08-29,18:06:25 | INFO | Train Epoch: 8 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.195, 495.746/s, 495.746/s/gpu LR: 9.848105e-09 Logit Scale: 100.000 Contrastive_loss: 0.45940 (0.38471) Loss: 0.45940 (0.38471)
2024-08-29,18:06:45 | INFO | Train Epoch: 8 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.195, 489.402/s, 489.402/s/gpu LR: 9.845533e-09 Logit Scale: 100.000 Contrastive_loss: 0.27028 (0.35610) Loss: 0.27028 (0.35610)
2024-08-29,18:07:04 | INFO | Train Epoch: 8 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.195, 526.528/s, 526.528/s/gpu LR: 9.842940e-09 Logit Scale: 100.000 Contrastive_loss: 0.18580 (0.32204) Loss: 0.18580 (0.32204)
2024-08-29,18:07:24 | INFO | Train Epoch: 8 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.195, 528.309/s, 528.309/s/gpu LR: 9.840325e-09 Logit Scale: 100.000 Contrastive_loss: 0.22037 (0.30510) Loss: 0.22037 (0.30510)
2024-08-29,18:07:43 | INFO | Train Epoch: 8 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.195, 530.823/s, 530.823/s/gpu LR: 9.837689e-09 Logit Scale: 100.000 Contrastive_loss: 0.17164 (0.28603) Loss: 0.17164 (0.28603)
2024-08-29,18:08:03 | INFO | Train Epoch: 8 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 488.253/s, 488.253/s/gpu LR: 9.835032e-09 Logit Scale: 100.000 Contrastive_loss: 0.32344 (0.29071) Loss: 0.32344 (0.29071)
2024-08-29,18:08:22 | INFO | Train Epoch: 8 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.194, 523.684/s, 523.684/s/gpu LR: 9.832354e-09 Logit Scale: 100.000 Contrastive_loss: 0.21052 (0.28180) Loss: 0.21052 (0.28180)
2024-08-29,18:08:42 | INFO | Train Epoch: 8 [ 90100/145000.0 (62%)] Data (t): 0.110 Batch (t): 0.197, 521.660/s, 521.660/s/gpu LR: 9.829654e-09 Logit Scale: 100.000 Contrastive_loss: 0.28457 (0.28207) Loss: 0.28457 (0.28207)
2024-08-29,18:09:01 | INFO | Train Epoch: 8 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.195, 546.170/s, 546.170/s/gpu LR: 9.826933e-09 Logit Scale: 100.000 Contrastive_loss: 0.20534 (0.27510) Loss: 0.20534 (0.27510)
2024-08-29,18:09:21 | INFO | Train Epoch: 8 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.195, 518.948/s, 518.948/s/gpu LR: 9.824191e-09 Logit Scale: 100.000 Contrastive_loss: 0.24741 (0.27279) Loss: 0.24741 (0.27279)
2024-08-29,18:09:40 | INFO | Train Epoch: 8 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.195, 530.512/s, 530.512/s/gpu LR: 9.821428e-09 Logit Scale: 100.000 Contrastive_loss: 0.39003 (0.28181) Loss: 0.39003 (0.28181)
2024-08-29,18:10:00 | INFO | Train Epoch: 8 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.195, 518.696/s, 518.696/s/gpu LR: 9.818644e-09 Logit Scale: 100.000 Contrastive_loss: 0.21655 (0.27715) Loss: 0.21655 (0.27715)
2024-08-29,18:10:19 | INFO | Train Epoch: 8 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 528.224/s, 528.224/s/gpu LR: 9.815839e-09 Logit Scale: 100.000 Contrastive_loss: 0.23573 (0.27439) Loss: 0.23573 (0.27439)
2024-08-29,18:10:29 | INFO | Train Epoch: 8 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.196, 529.057/s, 529.057/s/gpu LR: 9.814456e-09 Logit Scale: 100.000 Contrastive_loss: 0.31449 (0.27689) Loss: 0.31449 (0.27689)
2024-08-29,18:10:38 | INFO | Train Epoch: 8 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.195, 525.185/s, 525.185/s/gpu LR: 9.813012e-09 Logit Scale: 100.000 Contrastive_loss: 0.29800 (0.27813) Loss: 0.29800 (0.27813)
2024-08-29,18:10:58 | INFO | Train Epoch: 8 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.194, 522.098/s, 522.098/s/gpu LR: 9.810164e-09 Logit Scale: 100.000 Contrastive_loss: 0.36243 (0.28282) Loss: 0.36243 (0.28282)
2024-08-29,18:11:17 | INFO | Train Epoch: 8 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.195, 523.989/s, 523.989/s/gpu LR: 9.807296e-09 Logit Scale: 100.000 Contrastive_loss: 0.25738 (0.28148) Loss: 0.25738 (0.28148)
2024-08-29,18:11:37 | INFO | Train Epoch: 8 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.195, 527.687/s, 527.687/s/gpu LR: 9.804406e-09 Logit Scale: 100.000 Contrastive_loss: 0.26476 (0.28064) Loss: 0.26476 (0.28064)
2024-08-29,18:11:56 | INFO | Train Epoch: 8 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.195, 523.889/s, 523.889/s/gpu LR: 9.801495e-09 Logit Scale: 100.000 Contrastive_loss: 0.33248 (0.28311) Loss: 0.33248 (0.28311)
2024-08-29,18:12:16 | INFO | Train Epoch: 8 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.195, 465.713/s, 465.713/s/gpu LR: 9.798563e-09 Logit Scale: 100.000 Contrastive_loss: 0.20794 (0.27969) Loss: 0.20794 (0.27969)
2024-08-29,18:12:35 | INFO | Train Epoch: 8 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.194, 533.548/s, 533.548/s/gpu LR: 9.795610e-09 Logit Scale: 100.000 Contrastive_loss: 0.32741 (0.28177) Loss: 0.32741 (0.28177)
2024-08-29,18:12:55 | INFO | Train Epoch: 8 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.195, 532.197/s, 532.197/s/gpu LR: 9.792636e-09 Logit Scale: 100.000 Contrastive_loss: 0.43723 (0.28825) Loss: 0.43723 (0.28825)
2024-08-29,18:13:14 | INFO | Train Epoch: 8 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.194, 524.319/s, 524.319/s/gpu LR: 9.789641e-09 Logit Scale: 100.000 Contrastive_loss: 0.35346 (0.29085) Loss: 0.35346 (0.29085)
2024-08-29,18:13:33 | INFO | Train Epoch: 8 [240100/145000.0 (166%)] Data (t): 0.107 Batch (t): 0.193, 531.549/s, 531.549/s/gpu LR: 9.786625e-09 Logit Scale: 100.000 Contrastive_loss: 0.24305 (0.28902) Loss: 0.24305 (0.28902)
2024-08-29,18:13:53 | INFO | Train Epoch: 8 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.194, 488.430/s, 488.430/s/gpu LR: 9.783588e-09 Logit Scale: 100.000 Contrastive_loss: 0.27707 (0.28857) Loss: 0.27707 (0.28857)
2024-08-29,18:14:14 | INFO | Train Epoch: 8 [260100/145000.0 (179%)] Data (t): 0.114 Batch (t): 0.209, 488.163/s, 488.163/s/gpu LR: 9.780530e-09 Logit Scale: 100.000 Contrastive_loss: 0.30590 (0.28919) Loss: 0.30590 (0.28919)
2024-08-29,18:14:34 | INFO | Train Epoch: 8 [270100/145000.0 (186%)] Data (t): 0.113 Batch (t): 0.205, 505.637/s, 505.637/s/gpu LR: 9.777451e-09 Logit Scale: 100.000 Contrastive_loss: 0.30541 (0.28975) Loss: 0.30541 (0.28975)
2024-08-29,18:14:55 | INFO | Train Epoch: 8 [280100/145000.0 (193%)] Data (t): 0.114 Batch (t): 0.208, 499.529/s, 499.529/s/gpu LR: 9.774351e-09 Logit Scale: 100.000 Contrastive_loss: 0.23764 (0.28801) Loss: 0.23764 (0.28801)
2024-08-29,18:15:15 | INFO | Train Epoch: 8 [290100/145000.0 (200%)] Data (t): 0.110 Batch (t): 0.197, 498.041/s, 498.041/s/gpu LR: 9.771230e-09 Logit Scale: 100.000 Contrastive_loss: 0.18236 (0.28461) Loss: 0.18236 (0.28461)
2024-08-29,18:15:34 | INFO | Train Epoch: 8 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 527.019/s, 527.019/s/gpu LR: 9.768088e-09 Logit Scale: 100.000 Contrastive_loss: 0.33715 (0.28625) Loss: 0.33715 (0.28625)
2024-08-29,18:15:53 | INFO | Train Epoch: 8 [310100/145000.0 (214%)] Data (t): 0.107 Batch (t): 0.194, 525.531/s, 525.531/s/gpu LR: 9.764925e-09 Logit Scale: 100.000 Contrastive_loss: 0.35191 (0.28824) Loss: 0.35191 (0.28824)
2024-08-29,18:16:13 | INFO | Train Epoch: 8 [320100/145000.0 (221%)] Data (t): 0.111 Batch (t): 0.200, 491.222/s, 491.222/s/gpu LR: 9.761742e-09 Logit Scale: 100.000 Contrastive_loss: 0.38163 (0.29098) Loss: 0.38163 (0.29098)
2024-08-29,18:16:34 | INFO | Train Epoch: 8 [330100/145000.0 (228%)] Data (t): 0.114 Batch (t): 0.206, 491.427/s, 491.427/s/gpu LR: 9.758537e-09 Logit Scale: 100.000 Contrastive_loss: 0.22733 (0.28917) Loss: 0.22733 (0.28917)
2024-08-29,18:16:54 | INFO | Train Epoch: 8 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.195, 526.454/s, 526.454/s/gpu LR: 9.755312e-09 Logit Scale: 100.000 Contrastive_loss: 0.37880 (0.29166) Loss: 0.37880 (0.29166)
2024-08-29,18:17:14 | INFO | Train Epoch: 8 [350100/145000.0 (241%)] Data (t): 0.112 Batch (t): 0.202, 517.083/s, 517.083/s/gpu LR: 9.752065e-09 Logit Scale: 100.000 Contrastive_loss: 0.41922 (0.29510) Loss: 0.41922 (0.29510)
2024-08-29,18:17:34 | INFO | Train Epoch: 8 [360100/145000.0 (248%)] Data (t): 0.113 Batch (t): 0.206, 492.989/s, 492.989/s/gpu LR: 9.748798e-09 Logit Scale: 100.000 Contrastive_loss: 0.21579 (0.29302) Loss: 0.21579 (0.29302)
2024-08-29,18:17:54 | INFO | Train Epoch: 8 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.197, 461.144/s, 461.144/s/gpu LR: 9.745510e-09 Logit Scale: 100.000 Contrastive_loss: 0.32469 (0.29383) Loss: 0.32469 (0.29383)
2024-08-29,18:18:14 | INFO | Train Epoch: 8 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.195, 492.230/s, 492.230/s/gpu LR: 9.742202e-09 Logit Scale: 100.000 Contrastive_loss: 0.31498 (0.29436) Loss: 0.31498 (0.29436)
2024-08-29,18:18:33 | INFO | Train Epoch: 8 [390100/145000.0 (269%)] Data (t): 0.106 Batch (t): 0.191, 539.226/s, 539.226/s/gpu LR: 9.738872e-09 Logit Scale: 100.000 Contrastive_loss: 0.21122 (0.29233) Loss: 0.21122 (0.29233)
2024-08-29,18:18:52 | INFO | Train Epoch: 8 [400100/145000.0 (276%)] Data (t): 0.107 Batch (t): 0.193, 530.759/s, 530.759/s/gpu LR: 9.735522e-09 Logit Scale: 100.000 Contrastive_loss: 0.29227 (0.29233) Loss: 0.29227 (0.29233)
2024-08-29,18:19:11 | INFO | Train Epoch: 8 [410100/145000.0 (283%)] Data (t): 0.110 Batch (t): 0.195, 492.795/s, 492.795/s/gpu LR: 9.732151e-09 Logit Scale: 100.000 Contrastive_loss: 0.12408 (0.28841) Loss: 0.12408 (0.28841)
2024-08-29,18:19:19 | INFO | Eval Epoch: 9 [200 / 1000]	Clip Loss: 0.533672	
2024-08-29,18:19:20 | INFO | Eval Epoch: 9 image_to_text_mean_rank: 2.6230	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6390	image_to_text_R@5: 0.9030	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.4680	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6460	text_to_image_R@5: 0.8880	text_to_image_R@10: 0.9540	clip_val_loss: 0.4787	epoch: 9.0000	num_samples: 1000.0000
2024-08-29,18:19:21 | INFO | Start epoch 9
2024-08-29,18:19:21 | INFO | Train Epoch: 9 [   100/145000.0 (0%)] Data (t): 0.023 Batch (t): 0.106, 942.108/s, 942.108/s/gpu LR: 9.814428e-09 Logit Scale: 100.000 Contrastive_loss: 0.24468 (0.24468) Loss: 0.24468 (0.24468)
2024-08-29,18:19:41 | INFO | Train Epoch: 9 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.195, 528.981/s, 528.981/s/gpu LR: 9.811591e-09 Logit Scale: 100.000 Contrastive_loss: 0.43794 (0.34131) Loss: 0.43794 (0.34131)
2024-08-29,18:20:00 | INFO | Train Epoch: 9 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.195, 489.867/s, 489.867/s/gpu LR: 9.808733e-09 Logit Scale: 100.000 Contrastive_loss: 0.45432 (0.37898) Loss: 0.45432 (0.37898)
2024-08-29,18:20:20 | INFO | Train Epoch: 9 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.194, 498.723/s, 498.723/s/gpu LR: 9.805853e-09 Logit Scale: 100.000 Contrastive_loss: 0.26425 (0.35030) Loss: 0.26425 (0.35030)
2024-08-29,18:20:39 | INFO | Train Epoch: 9 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.194, 495.379/s, 495.379/s/gpu LR: 9.802953e-09 Logit Scale: 100.000 Contrastive_loss: 0.18281 (0.31680) Loss: 0.18281 (0.31680)
2024-08-29,18:20:58 | INFO | Train Epoch: 9 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.194, 504.509/s, 504.509/s/gpu LR: 9.800032e-09 Logit Scale: 100.000 Contrastive_loss: 0.21830 (0.30038) Loss: 0.21830 (0.30038)
2024-08-29,18:21:18 | INFO | Train Epoch: 9 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.194, 498.528/s, 498.528/s/gpu LR: 9.797089e-09 Logit Scale: 100.000 Contrastive_loss: 0.16782 (0.28144) Loss: 0.16782 (0.28144)
2024-08-29,18:21:37 | INFO | Train Epoch: 9 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 527.050/s, 527.050/s/gpu LR: 9.794126e-09 Logit Scale: 100.000 Contrastive_loss: 0.31893 (0.28613) Loss: 0.31893 (0.28613)
2024-08-29,18:21:57 | INFO | Train Epoch: 9 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.194, 533.291/s, 533.291/s/gpu LR: 9.791141e-09 Logit Scale: 100.000 Contrastive_loss: 0.20654 (0.27729) Loss: 0.20654 (0.27729)
2024-08-29,18:22:16 | INFO | Train Epoch: 9 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.194, 528.048/s, 528.048/s/gpu LR: 9.788136e-09 Logit Scale: 100.000 Contrastive_loss: 0.28117 (0.27767) Loss: 0.28117 (0.27767)
2024-08-29,18:22:35 | INFO | Train Epoch: 9 [100100/145000.0 (69%)] Data (t): 0.110 Batch (t): 0.195, 491.115/s, 491.115/s/gpu LR: 9.785109e-09 Logit Scale: 100.000 Contrastive_loss: 0.20270 (0.27086) Loss: 0.20270 (0.27086)
2024-08-29,18:22:55 | INFO | Train Epoch: 9 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.194, 530.313/s, 530.313/s/gpu LR: 9.782061e-09 Logit Scale: 100.000 Contrastive_loss: 0.24305 (0.26854) Loss: 0.24305 (0.26854)
2024-08-29,18:23:14 | INFO | Train Epoch: 9 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.194, 528.547/s, 528.547/s/gpu LR: 9.778993e-09 Logit Scale: 100.000 Contrastive_loss: 0.38332 (0.27737) Loss: 0.38332 (0.27737)
2024-08-29,18:23:34 | INFO | Train Epoch: 9 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.194, 535.946/s, 535.946/s/gpu LR: 9.775903e-09 Logit Scale: 100.000 Contrastive_loss: 0.21315 (0.27278) Loss: 0.21315 (0.27278)
2024-08-29,18:23:53 | INFO | Train Epoch: 9 [140100/145000.0 (97%)] Data (t): 0.111 Batch (t): 0.196, 487.049/s, 487.049/s/gpu LR: 9.772793e-09 Logit Scale: 100.000 Contrastive_loss: 0.23203 (0.27007) Loss: 0.23203 (0.27007)
2024-08-29,18:24:03 | INFO | Train Epoch: 9 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.194, 517.058/s, 517.058/s/gpu LR: 9.771261e-09 Logit Scale: 100.000 Contrastive_loss: 0.30763 (0.27241) Loss: 0.30763 (0.27241)
2024-08-29,18:24:13 | INFO | Train Epoch: 9 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.194, 526.034/s, 526.034/s/gpu LR: 9.769662e-09 Logit Scale: 100.000 Contrastive_loss: 0.29131 (0.27353) Loss: 0.29131 (0.27353)
2024-08-29,18:24:32 | INFO | Train Epoch: 9 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 494.174/s, 494.174/s/gpu LR: 9.766509e-09 Logit Scale: 100.000 Contrastive_loss: 0.35412 (0.27800) Loss: 0.35412 (0.27800)
2024-08-29,18:24:51 | INFO | Train Epoch: 9 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 528.431/s, 528.431/s/gpu LR: 9.763336e-09 Logit Scale: 100.000 Contrastive_loss: 0.25212 (0.27664) Loss: 0.25212 (0.27664)
2024-08-29,18:25:10 | INFO | Train Epoch: 9 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 499.212/s, 499.212/s/gpu LR: 9.760142e-09 Logit Scale: 100.000 Contrastive_loss: 0.25933 (0.27578) Loss: 0.25933 (0.27578)
2024-08-29,18:25:30 | INFO | Train Epoch: 9 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 516.348/s, 516.348/s/gpu LR: 9.756927e-09 Logit Scale: 100.000 Contrastive_loss: 0.32786 (0.27826) Loss: 0.32786 (0.27826)
2024-08-29,18:25:49 | INFO | Train Epoch: 9 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 506.106/s, 506.106/s/gpu LR: 9.753691e-09 Logit Scale: 100.000 Contrastive_loss: 0.20560 (0.27495) Loss: 0.20560 (0.27495)
2024-08-29,18:26:08 | INFO | Train Epoch: 9 [210100/145000.0 (145%)] Data (t): 0.107 Batch (t): 0.192, 535.783/s, 535.783/s/gpu LR: 9.750434e-09 Logit Scale: 100.000 Contrastive_loss: 0.32294 (0.27704) Loss: 0.32294 (0.27704)
2024-08-29,18:26:28 | INFO | Train Epoch: 9 [220100/145000.0 (152%)] Data (t): 0.107 Batch (t): 0.192, 493.784/s, 493.784/s/gpu LR: 9.747157e-09 Logit Scale: 100.000 Contrastive_loss: 0.43091 (0.28345) Loss: 0.43091 (0.28345)
2024-08-29,18:26:47 | INFO | Train Epoch: 9 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 519.718/s, 519.718/s/gpu LR: 9.743859e-09 Logit Scale: 100.000 Contrastive_loss: 0.35109 (0.28616) Loss: 0.35109 (0.28616)
2024-08-29,18:27:06 | INFO | Train Epoch: 9 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 497.984/s, 497.984/s/gpu LR: 9.740539e-09 Logit Scale: 100.000 Contrastive_loss: 0.23868 (0.28433) Loss: 0.23868 (0.28433)
2024-08-29,18:27:25 | INFO | Train Epoch: 9 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 526.676/s, 526.676/s/gpu LR: 9.737199e-09 Logit Scale: 100.000 Contrastive_loss: 0.27361 (0.28393) Loss: 0.27361 (0.28393)
2024-08-29,18:27:45 | INFO | Train Epoch: 9 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.196, 497.399/s, 497.399/s/gpu LR: 9.733839e-09 Logit Scale: 100.000 Contrastive_loss: 0.30083 (0.28454) Loss: 0.30083 (0.28454)
2024-08-29,18:28:04 | INFO | Train Epoch: 9 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.194, 538.777/s, 538.777/s/gpu LR: 9.730457e-09 Logit Scale: 100.000 Contrastive_loss: 0.30313 (0.28518) Loss: 0.30313 (0.28518)
2024-08-29,18:28:24 | INFO | Train Epoch: 9 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 497.340/s, 497.340/s/gpu LR: 9.727055e-09 Logit Scale: 100.000 Contrastive_loss: 0.23373 (0.28346) Loss: 0.23373 (0.28346)
2024-08-29,18:28:43 | INFO | Train Epoch: 9 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.194, 499.300/s, 499.300/s/gpu LR: 9.723632e-09 Logit Scale: 100.000 Contrastive_loss: 0.17992 (0.28012) Loss: 0.17992 (0.28012)
2024-08-29,18:29:02 | INFO | Train Epoch: 9 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.194, 539.651/s, 539.651/s/gpu LR: 9.720188e-09 Logit Scale: 100.000 Contrastive_loss: 0.33290 (0.28177) Loss: 0.33290 (0.28177)
2024-08-29,18:29:22 | INFO | Train Epoch: 9 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.194, 497.538/s, 497.538/s/gpu LR: 9.716724e-09 Logit Scale: 100.000 Contrastive_loss: 0.34118 (0.28357) Loss: 0.34118 (0.28357)
2024-08-29,18:29:41 | INFO | Train Epoch: 9 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 525.393/s, 525.393/s/gpu LR: 9.713239e-09 Logit Scale: 100.000 Contrastive_loss: 0.37701 (0.28632) Loss: 0.37701 (0.28632)
2024-08-29,18:30:01 | INFO | Train Epoch: 9 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.194, 522.461/s, 522.461/s/gpu LR: 9.709733e-09 Logit Scale: 100.000 Contrastive_loss: 0.22501 (0.28457) Loss: 0.22501 (0.28457)
2024-08-29,18:30:20 | INFO | Train Epoch: 9 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.194, 500.803/s, 500.803/s/gpu LR: 9.706206e-09 Logit Scale: 100.000 Contrastive_loss: 0.37143 (0.28698) Loss: 0.37143 (0.28698)
2024-08-29,18:30:40 | INFO | Train Epoch: 9 [350100/145000.0 (241%)] Data (t): 0.110 Batch (t): 0.197, 422.479/s, 422.479/s/gpu LR: 9.702659e-09 Logit Scale: 100.000 Contrastive_loss: 0.41278 (0.29038) Loss: 0.41278 (0.29038)
2024-08-29,18:30:59 | INFO | Train Epoch: 9 [360100/145000.0 (248%)] Data (t): 0.110 Batch (t): 0.194, 524.756/s, 524.756/s/gpu LR: 9.699092e-09 Logit Scale: 100.000 Contrastive_loss: 0.21220 (0.28832) Loss: 0.21220 (0.28832)
2024-08-29,18:31:19 | INFO | Train Epoch: 9 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.195, 523.341/s, 523.341/s/gpu LR: 9.695503e-09 Logit Scale: 100.000 Contrastive_loss: 0.32051 (0.28915) Loss: 0.32051 (0.28915)
2024-08-29,18:31:38 | INFO | Train Epoch: 9 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.194, 526.780/s, 526.780/s/gpu LR: 9.691894e-09 Logit Scale: 100.000 Contrastive_loss: 0.31073 (0.28969) Loss: 0.31073 (0.28969)
2024-08-29,18:31:57 | INFO | Train Epoch: 9 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.194, 529.252/s, 529.252/s/gpu LR: 9.688265e-09 Logit Scale: 100.000 Contrastive_loss: 0.20635 (0.28766) Loss: 0.20635 (0.28766)
2024-08-29,18:32:17 | INFO | Train Epoch: 9 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.194, 524.574/s, 524.574/s/gpu LR: 9.684615e-09 Logit Scale: 100.000 Contrastive_loss: 0.28931 (0.28770) Loss: 0.28931 (0.28770)
2024-08-29,18:32:36 | INFO | Train Epoch: 9 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.194, 497.946/s, 497.946/s/gpu LR: 9.680944e-09 Logit Scale: 100.000 Contrastive_loss: 0.12312 (0.28387) Loss: 0.12312 (0.28387)
2024-08-29,18:32:44 | INFO | Eval Epoch: 10 [200 / 1000]	Clip Loss: 0.527123	
2024-08-29,18:32:45 | INFO | Eval Epoch: 10 image_to_text_mean_rank: 2.6040	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6430	image_to_text_R@5: 0.9030	image_to_text_R@10: 0.9600	text_to_image_mean_rank: 3.4570	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6520	text_to_image_R@5: 0.8870	text_to_image_R@10: 0.9540	clip_val_loss: 0.4745	epoch: 10.0000	num_samples: 1000.0000
2024-08-29,18:32:46 | INFO | Start epoch 10
2024-08-29,18:32:46 | INFO | Train Epoch: 10 [   100/145000.0 (0%)] Data (t): 0.036 Batch (t): 0.122, 819.728/s, 819.728/s/gpu LR: 9.771230e-09 Logit Scale: 100.000 Contrastive_loss: 0.23992 (0.23992) Loss: 0.23992 (0.23992)
2024-08-29,18:33:06 | INFO | Train Epoch: 10 [ 10100/145000.0 (7%)] Data (t): 0.112 Batch (t): 0.200, 501.221/s, 501.221/s/gpu LR: 9.768088e-09 Logit Scale: 100.000 Contrastive_loss: 0.43167 (0.33580) Loss: 0.43167 (0.33580)
2024-08-29,18:33:25 | INFO | Train Epoch: 10 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.194, 493.018/s, 493.018/s/gpu LR: 9.764925e-09 Logit Scale: 100.000 Contrastive_loss: 0.44959 (0.37373) Loss: 0.44959 (0.37373)
2024-08-29,18:33:45 | INFO | Train Epoch: 10 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.194, 533.165/s, 533.165/s/gpu LR: 9.761742e-09 Logit Scale: 100.000 Contrastive_loss: 0.25919 (0.34509) Loss: 0.25919 (0.34509)
2024-08-29,18:34:04 | INFO | Train Epoch: 10 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.194, 502.715/s, 502.715/s/gpu LR: 9.758537e-09 Logit Scale: 100.000 Contrastive_loss: 0.17994 (0.31206) Loss: 0.17994 (0.31206)
2024-08-29,18:34:24 | INFO | Train Epoch: 10 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.194, 521.298/s, 521.298/s/gpu LR: 9.755312e-09 Logit Scale: 100.000 Contrastive_loss: 0.21613 (0.29607) Loss: 0.21613 (0.29607)
2024-08-29,18:34:43 | INFO | Train Epoch: 10 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 493.123/s, 493.123/s/gpu LR: 9.752065e-09 Logit Scale: 100.000 Contrastive_loss: 0.16452 (0.27728) Loss: 0.16452 (0.27728)
2024-08-29,18:35:02 | INFO | Train Epoch: 10 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 538.256/s, 538.256/s/gpu LR: 9.748798e-09 Logit Scale: 100.000 Contrastive_loss: 0.31463 (0.28195) Loss: 0.31463 (0.28195)
2024-08-29,18:35:22 | INFO | Train Epoch: 10 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 532.020/s, 532.020/s/gpu LR: 9.745510e-09 Logit Scale: 100.000 Contrastive_loss: 0.20257 (0.27313) Loss: 0.20257 (0.27313)
2024-08-29,18:35:41 | INFO | Train Epoch: 10 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.194, 497.203/s, 497.203/s/gpu LR: 9.742202e-09 Logit Scale: 100.000 Contrastive_loss: 0.27794 (0.27361) Loss: 0.27794 (0.27361)
2024-08-29,18:36:01 | INFO | Train Epoch: 10 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.194, 529.317/s, 529.317/s/gpu LR: 9.738872e-09 Logit Scale: 100.000 Contrastive_loss: 0.20028 (0.26694) Loss: 0.20028 (0.26694)
2024-08-29,18:36:20 | INFO | Train Epoch: 10 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.194, 504.660/s, 504.660/s/gpu LR: 9.735522e-09 Logit Scale: 100.000 Contrastive_loss: 0.23929 (0.26464) Loss: 0.23929 (0.26464)
2024-08-29,18:36:39 | INFO | Train Epoch: 10 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.194, 497.306/s, 497.306/s/gpu LR: 9.732151e-09 Logit Scale: 100.000 Contrastive_loss: 0.37735 (0.27331) Loss: 0.37735 (0.27331)
2024-08-29,18:36:59 | INFO | Train Epoch: 10 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.194, 517.165/s, 517.165/s/gpu LR: 9.728759e-09 Logit Scale: 100.000 Contrastive_loss: 0.20982 (0.26877) Loss: 0.20982 (0.26877)
2024-08-29,18:37:18 | INFO | Train Epoch: 10 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.194, 497.150/s, 497.150/s/gpu LR: 9.725346e-09 Logit Scale: 100.000 Contrastive_loss: 0.22866 (0.26610) Loss: 0.22866 (0.26610)
2024-08-29,18:37:28 | INFO | Train Epoch: 10 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.195, 537.665/s, 537.665/s/gpu LR: 9.723666e-09 Logit Scale: 100.000 Contrastive_loss: 0.30183 (0.26833) Loss: 0.30183 (0.26833)
2024-08-29,18:37:38 | INFO | Train Epoch: 10 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.194, 533.732/s, 533.732/s/gpu LR: 9.721913e-09 Logit Scale: 100.000 Contrastive_loss: 0.28570 (0.26935) Loss: 0.28570 (0.26935)
2024-08-29,18:37:57 | INFO | Train Epoch: 10 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.196, 492.308/s, 492.308/s/gpu LR: 9.718459e-09 Logit Scale: 100.000 Contrastive_loss: 0.34693 (0.27366) Loss: 0.34693 (0.27366)
2024-08-29,18:38:16 | INFO | Train Epoch: 10 [170100/145000.0 (117%)] Data (t): 0.105 Batch (t): 0.192, 532.037/s, 532.037/s/gpu LR: 9.714984e-09 Logit Scale: 100.000 Contrastive_loss: 0.24777 (0.27230) Loss: 0.24777 (0.27230)
2024-08-29,18:38:36 | INFO | Train Epoch: 10 [180100/145000.0 (124%)] Data (t): 0.105 Batch (t): 0.192, 508.938/s, 508.938/s/gpu LR: 9.711488e-09 Logit Scale: 100.000 Contrastive_loss: 0.25454 (0.27141) Loss: 0.25454 (0.27141)
2024-08-29,18:38:55 | INFO | Train Epoch: 10 [190100/145000.0 (131%)] Data (t): 0.105 Batch (t): 0.193, 532.673/s, 532.673/s/gpu LR: 9.707972e-09 Logit Scale: 100.000 Contrastive_loss: 0.32372 (0.27390) Loss: 0.32372 (0.27390)
2024-08-29,18:39:14 | INFO | Train Epoch: 10 [200100/145000.0 (138%)] Data (t): 0.104 Batch (t): 0.192, 510.347/s, 510.347/s/gpu LR: 9.704435e-09 Logit Scale: 100.000 Contrastive_loss: 0.20349 (0.27070) Loss: 0.20349 (0.27070)
2024-08-29,18:39:34 | INFO | Train Epoch: 10 [210100/145000.0 (145%)] Data (t): 0.106 Batch (t): 0.195, 483.229/s, 483.229/s/gpu LR: 9.700878e-09 Logit Scale: 100.000 Contrastive_loss: 0.31872 (0.27279) Loss: 0.31872 (0.27279)
2024-08-29,18:39:53 | INFO | Train Epoch: 10 [220100/145000.0 (152%)] Data (t): 0.103 Batch (t): 0.193, 533.335/s, 533.335/s/gpu LR: 9.697300e-09 Logit Scale: 100.000 Contrastive_loss: 0.42508 (0.27914) Loss: 0.42508 (0.27914)
2024-08-29,18:40:12 | INFO | Train Epoch: 10 [230100/145000.0 (159%)] Data (t): 0.104 Batch (t): 0.193, 555.453/s, 555.453/s/gpu LR: 9.693701e-09 Logit Scale: 100.000 Contrastive_loss: 0.34909 (0.28193) Loss: 0.34909 (0.28193)
2024-08-29,18:40:32 | INFO | Train Epoch: 10 [240100/145000.0 (166%)] Data (t): 0.106 Batch (t): 0.193, 489.892/s, 489.892/s/gpu LR: 9.690082e-09 Logit Scale: 100.000 Contrastive_loss: 0.23494 (0.28013) Loss: 0.23494 (0.28013)
2024-08-29,18:40:51 | INFO | Train Epoch: 10 [250100/145000.0 (172%)] Data (t): 0.105 Batch (t): 0.193, 508.078/s, 508.078/s/gpu LR: 9.686442e-09 Logit Scale: 100.000 Contrastive_loss: 0.27032 (0.27976) Loss: 0.27032 (0.27976)
2024-08-29,18:41:10 | INFO | Train Epoch: 10 [260100/145000.0 (179%)] Data (t): 0.105 Batch (t): 0.193, 532.244/s, 532.244/s/gpu LR: 9.682782e-09 Logit Scale: 100.000 Contrastive_loss: 0.29633 (0.28036) Loss: 0.29633 (0.28036)
2024-08-29,18:41:29 | INFO | Train Epoch: 10 [270100/145000.0 (186%)] Data (t): 0.105 Batch (t): 0.192, 548.022/s, 548.022/s/gpu LR: 9.679101e-09 Logit Scale: 100.000 Contrastive_loss: 0.30112 (0.28107) Loss: 0.30112 (0.28107)
2024-08-29,18:41:49 | INFO | Train Epoch: 10 [280100/145000.0 (193%)] Data (t): 0.106 Batch (t): 0.193, 502.294/s, 502.294/s/gpu LR: 9.675400e-09 Logit Scale: 100.000 Contrastive_loss: 0.23011 (0.27937) Loss: 0.23011 (0.27937)
2024-08-29,18:42:08 | INFO | Train Epoch: 10 [290100/145000.0 (200%)] Data (t): 0.105 Batch (t): 0.194, 504.433/s, 504.433/s/gpu LR: 9.671678e-09 Logit Scale: 100.000 Contrastive_loss: 0.17771 (0.27609) Loss: 0.17771 (0.27609)
2024-08-29,18:42:27 | INFO | Train Epoch: 10 [300100/145000.0 (207%)] Data (t): 0.107 Batch (t): 0.193, 535.005/s, 535.005/s/gpu LR: 9.667935e-09 Logit Scale: 100.000 Contrastive_loss: 0.32912 (0.27775) Loss: 0.32912 (0.27775)
2024-08-29,18:42:47 | INFO | Train Epoch: 10 [310100/145000.0 (214%)] Data (t): 0.106 Batch (t): 0.194, 502.858/s, 502.858/s/gpu LR: 9.664172e-09 Logit Scale: 100.000 Contrastive_loss: 0.33177 (0.27939) Loss: 0.33177 (0.27939)
2024-08-29,18:43:06 | INFO | Train Epoch: 10 [320100/145000.0 (221%)] Data (t): 0.105 Batch (t): 0.194, 497.256/s, 497.256/s/gpu LR: 9.660389e-09 Logit Scale: 100.000 Contrastive_loss: 0.37268 (0.28213) Loss: 0.37268 (0.28213)
2024-08-29,18:43:26 | INFO | Train Epoch: 10 [330100/145000.0 (228%)] Data (t): 0.106 Batch (t): 0.194, 491.380/s, 491.380/s/gpu LR: 9.656585e-09 Logit Scale: 100.000 Contrastive_loss: 0.22312 (0.28045) Loss: 0.22312 (0.28045)
2024-08-29,18:43:45 | INFO | Train Epoch: 10 [340100/145000.0 (235%)] Data (t): 0.105 Batch (t): 0.192, 533.334/s, 533.334/s/gpu LR: 9.652761e-09 Logit Scale: 100.000 Contrastive_loss: 0.36464 (0.28278) Loss: 0.36464 (0.28278)
2024-08-29,18:44:04 | INFO | Train Epoch: 10 [350100/145000.0 (241%)] Data (t): 0.104 Batch (t): 0.193, 492.315/s, 492.315/s/gpu LR: 9.648916e-09 Logit Scale: 100.000 Contrastive_loss: 0.40718 (0.28615) Loss: 0.40718 (0.28615)
2024-08-29,18:44:23 | INFO | Train Epoch: 10 [360100/145000.0 (248%)] Data (t): 0.103 Batch (t): 0.192, 539.478/s, 539.478/s/gpu LR: 9.645051e-09 Logit Scale: 100.000 Contrastive_loss: 0.20894 (0.28411) Loss: 0.20894 (0.28411)
2024-08-29,18:44:42 | INFO | Train Epoch: 10 [370100/145000.0 (255%)] Data (t): 0.103 Batch (t): 0.193, 491.249/s, 491.249/s/gpu LR: 9.641166e-09 Logit Scale: 100.000 Contrastive_loss: 0.31684 (0.28495) Loss: 0.31684 (0.28495)
2024-08-29,18:45:02 | INFO | Train Epoch: 10 [380100/145000.0 (262%)] Data (t): 0.103 Batch (t): 0.192, 533.417/s, 533.417/s/gpu LR: 9.637260e-09 Logit Scale: 100.000 Contrastive_loss: 0.30688 (0.28550) Loss: 0.30688 (0.28550)
2024-08-29,18:45:21 | INFO | Train Epoch: 10 [390100/145000.0 (269%)] Data (t): 0.104 Batch (t): 0.192, 533.337/s, 533.337/s/gpu LR: 9.633334e-09 Logit Scale: 100.000 Contrastive_loss: 0.20186 (0.28346) Loss: 0.20186 (0.28346)
2024-08-29,18:45:40 | INFO | Train Epoch: 10 [400100/145000.0 (276%)] Data (t): 0.106 Batch (t): 0.192, 519.444/s, 519.444/s/gpu LR: 9.629388e-09 Logit Scale: 100.000 Contrastive_loss: 0.28658 (0.28354) Loss: 0.28658 (0.28354)
2024-08-29,18:46:00 | INFO | Train Epoch: 10 [410100/145000.0 (283%)] Data (t): 0.107 Batch (t): 0.196, 528.791/s, 528.791/s/gpu LR: 9.625421e-09 Logit Scale: 100.000 Contrastive_loss: 0.12226 (0.27979) Loss: 0.12226 (0.27979)
2024-08-29,18:46:07 | INFO | Eval Epoch: 11 [200 / 1000]	Clip Loss: 0.521415	
2024-08-29,18:46:08 | INFO | Eval Epoch: 11 image_to_text_mean_rank: 2.5870	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6420	image_to_text_R@5: 0.9040	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.4570	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6550	text_to_image_R@5: 0.8880	text_to_image_R@10: 0.9520	clip_val_loss: 0.4708	epoch: 11.0000	num_samples: 1000.0000
2024-08-29,18:46:09 | INFO | Start epoch 11
2024-08-29,18:46:09 | INFO | Train Epoch: 11 [   100/145000.0 (0%)] Data (t): 0.016 Batch (t): 0.109, 914.290/s, 914.290/s/gpu LR: 9.723632e-09 Logit Scale: 100.000 Contrastive_loss: 0.23547 (0.23547) Loss: 0.23547 (0.23547)
2024-08-29,18:46:29 | INFO | Train Epoch: 11 [ 10100/145000.0 (7%)] Data (t): 0.108 Batch (t): 0.198, 492.309/s, 492.309/s/gpu LR: 9.720188e-09 Logit Scale: 100.000 Contrastive_loss: 0.42601 (0.33074) Loss: 0.42601 (0.33074)
2024-08-29,18:46:49 | INFO | Train Epoch: 11 [ 20100/145000.0 (14%)] Data (t): 0.106 Batch (t): 0.194, 500.342/s, 500.342/s/gpu LR: 9.716724e-09 Logit Scale: 100.000 Contrastive_loss: 0.44561 (0.36903) Loss: 0.44561 (0.36903)
2024-08-29,18:47:08 | INFO | Train Epoch: 11 [ 30100/145000.0 (21%)] Data (t): 0.106 Batch (t): 0.194, 532.003/s, 532.003/s/gpu LR: 9.713239e-09 Logit Scale: 100.000 Contrastive_loss: 0.25466 (0.34044) Loss: 0.25466 (0.34044)
2024-08-29,18:47:27 | INFO | Train Epoch: 11 [ 40100/145000.0 (28%)] Data (t): 0.106 Batch (t): 0.193, 512.520/s, 512.520/s/gpu LR: 9.709733e-09 Logit Scale: 100.000 Contrastive_loss: 0.17736 (0.30782) Loss: 0.17736 (0.30782)
2024-08-29,18:47:47 | INFO | Train Epoch: 11 [ 50100/145000.0 (35%)] Data (t): 0.104 Batch (t): 0.193, 532.241/s, 532.241/s/gpu LR: 9.706206e-09 Logit Scale: 100.000 Contrastive_loss: 0.21422 (0.29222) Loss: 0.21422 (0.29222)
2024-08-29,18:48:06 | INFO | Train Epoch: 11 [ 60100/145000.0 (41%)] Data (t): 0.103 Batch (t): 0.194, 533.331/s, 533.331/s/gpu LR: 9.702659e-09 Logit Scale: 100.000 Contrastive_loss: 0.16157 (0.27356) Loss: 0.16157 (0.27356)
2024-08-29,18:48:25 | INFO | Train Epoch: 11 [ 70100/145000.0 (48%)] Data (t): 0.103 Batch (t): 0.192, 491.379/s, 491.379/s/gpu LR: 9.699092e-09 Logit Scale: 100.000 Contrastive_loss: 0.31073 (0.27820) Loss: 0.31073 (0.27820)
2024-08-29,18:48:45 | INFO | Train Epoch: 11 [ 80100/145000.0 (55%)] Data (t): 0.104 Batch (t): 0.192, 491.192/s, 491.192/s/gpu LR: 9.695503e-09 Logit Scale: 100.000 Contrastive_loss: 0.19898 (0.26940) Loss: 0.19898 (0.26940)
2024-08-29,18:49:04 | INFO | Train Epoch: 11 [ 90100/145000.0 (62%)] Data (t): 0.104 Batch (t): 0.193, 492.303/s, 492.303/s/gpu LR: 9.691894e-09 Logit Scale: 100.000 Contrastive_loss: 0.27512 (0.26997) Loss: 0.27512 (0.26997)
2024-08-29,18:49:23 | INFO | Train Epoch: 11 [100100/145000.0 (69%)] Data (t): 0.104 Batch (t): 0.194, 492.115/s, 492.115/s/gpu LR: 9.688265e-09 Logit Scale: 100.000 Contrastive_loss: 0.19792 (0.26342) Loss: 0.19792 (0.26342)
2024-08-29,18:49:42 | INFO | Train Epoch: 11 [110100/145000.0 (76%)] Data (t): 0.104 Batch (t): 0.193, 533.332/s, 533.332/s/gpu LR: 9.684615e-09 Logit Scale: 100.000 Contrastive_loss: 0.23583 (0.26112) Loss: 0.23583 (0.26112)
2024-08-29,18:50:02 | INFO | Train Epoch: 11 [120100/145000.0 (83%)] Data (t): 0.103 Batch (t): 0.193, 492.342/s, 492.342/s/gpu LR: 9.680944e-09 Logit Scale: 100.000 Contrastive_loss: 0.37159 (0.26962) Loss: 0.37159 (0.26962)
2024-08-29,18:50:21 | INFO | Train Epoch: 11 [130100/145000.0 (90%)] Data (t): 0.103 Batch (t): 0.193, 533.335/s, 533.335/s/gpu LR: 9.677253e-09 Logit Scale: 100.000 Contrastive_loss: 0.20667 (0.26512) Loss: 0.20667 (0.26512)
2024-08-29,18:50:40 | INFO | Train Epoch: 11 [140100/145000.0 (97%)] Data (t): 0.104 Batch (t): 0.193, 533.649/s, 533.649/s/gpu LR: 9.673541e-09 Logit Scale: 100.000 Contrastive_loss: 0.22569 (0.26250) Loss: 0.22569 (0.26250)
2024-08-29,18:50:50 | INFO | Train Epoch: 11 [145000/145000.0 (100%)] Data (t): 0.103 Batch (t): 0.192, 533.358/s, 533.358/s/gpu LR: 9.671715e-09 Logit Scale: 100.000 Contrastive_loss: 0.29689 (0.26465) Loss: 0.29689 (0.26465)
2024-08-29,18:51:00 | INFO | Train Epoch: 11 [150100/145000.0 (104%)] Data (t): 0.104 Batch (t): 0.193, 533.333/s, 533.333/s/gpu LR: 9.669809e-09 Logit Scale: 100.000 Contrastive_loss: 0.28054 (0.26558) Loss: 0.28054 (0.26558)
2024-08-29,18:51:19 | INFO | Train Epoch: 11 [160100/145000.0 (110%)] Data (t): 0.104 Batch (t): 0.192, 533.361/s, 533.361/s/gpu LR: 9.666056e-09 Logit Scale: 100.000 Contrastive_loss: 0.34014 (0.26972) Loss: 0.34014 (0.26972)
2024-08-29,18:51:38 | INFO | Train Epoch: 11 [170100/145000.0 (117%)] Data (t): 0.103 Batch (t): 0.193, 533.334/s, 533.334/s/gpu LR: 9.662283e-09 Logit Scale: 100.000 Contrastive_loss: 0.24351 (0.26834) Loss: 0.24351 (0.26834)
2024-08-29,18:51:57 | INFO | Train Epoch: 11 [180100/145000.0 (124%)] Data (t): 0.103 Batch (t): 0.192, 533.334/s, 533.334/s/gpu LR: 9.658490e-09 Logit Scale: 100.000 Contrastive_loss: 0.25033 (0.26744) Loss: 0.25033 (0.26744)
2024-08-29,18:52:17 | INFO | Train Epoch: 11 [190100/145000.0 (131%)] Data (t): 0.103 Batch (t): 0.193, 533.331/s, 533.331/s/gpu LR: 9.654676e-09 Logit Scale: 100.000 Contrastive_loss: 0.31985 (0.26994) Loss: 0.31985 (0.26994)
2024-08-29,18:52:36 | INFO | Train Epoch: 11 [200100/145000.0 (138%)] Data (t): 0.105 Batch (t): 0.193, 492.307/s, 492.307/s/gpu LR: 9.650841e-09 Logit Scale: 100.000 Contrastive_loss: 0.20175 (0.26684) Loss: 0.20175 (0.26684)
2024-08-29,18:52:55 | INFO | Train Epoch: 11 [210100/145000.0 (145%)] Data (t): 0.104 Batch (t): 0.193, 533.647/s, 533.647/s/gpu LR: 9.646986e-09 Logit Scale: 100.000 Contrastive_loss: 0.31532 (0.26895) Loss: 0.31532 (0.26895)
2024-08-29,18:53:14 | INFO | Train Epoch: 11 [220100/145000.0 (152%)] Data (t): 0.103 Batch (t): 0.193, 532.616/s, 532.616/s/gpu LR: 9.643111e-09 Logit Scale: 100.000 Contrastive_loss: 0.41982 (0.27523) Loss: 0.41982 (0.27523)
2024-08-29,18:53:34 | INFO | Train Epoch: 11 [230100/145000.0 (159%)] Data (t): 0.103 Batch (t): 0.192, 533.337/s, 533.337/s/gpu LR: 9.639216e-09 Logit Scale: 100.000 Contrastive_loss: 0.34751 (0.27812) Loss: 0.34751 (0.27812)
2024-08-29,18:53:53 | INFO | Train Epoch: 11 [240100/145000.0 (166%)] Data (t): 0.104 Batch (t): 0.193, 533.558/s, 533.558/s/gpu LR: 9.635300e-09 Logit Scale: 100.000 Contrastive_loss: 0.23166 (0.27634) Loss: 0.23166 (0.27634)
2024-08-29,18:54:12 | INFO | Train Epoch: 11 [250100/145000.0 (172%)] Data (t): 0.103 Batch (t): 0.193, 534.806/s, 534.806/s/gpu LR: 9.631363e-09 Logit Scale: 100.000 Contrastive_loss: 0.26721 (0.27600) Loss: 0.26721 (0.27600)
2024-08-29,18:54:31 | INFO | Train Epoch: 11 [260100/145000.0 (179%)] Data (t): 0.104 Batch (t): 0.193, 492.291/s, 492.291/s/gpu LR: 9.627407e-09 Logit Scale: 100.000 Contrastive_loss: 0.29206 (0.27657) Loss: 0.29206 (0.27657)
2024-08-29,18:54:51 | INFO | Train Epoch: 11 [270100/145000.0 (186%)] Data (t): 0.103 Batch (t): 0.192, 491.950/s, 491.950/s/gpu LR: 9.623430e-09 Logit Scale: 100.000 Contrastive_loss: 0.29920 (0.27735) Loss: 0.29920 (0.27735)
2024-08-29,18:55:10 | INFO | Train Epoch: 11 [280100/145000.0 (193%)] Data (t): 0.103 Batch (t): 0.192, 531.015/s, 531.015/s/gpu LR: 9.619433e-09 Logit Scale: 100.000 Contrastive_loss: 0.22679 (0.27567) Loss: 0.22679 (0.27567)
2024-08-29,18:55:29 | INFO | Train Epoch: 11 [290100/145000.0 (200%)] Data (t): 0.104 Batch (t): 0.192, 531.989/s, 531.989/s/gpu LR: 9.615415e-09 Logit Scale: 100.000 Contrastive_loss: 0.17581 (0.27245) Loss: 0.17581 (0.27245)
2024-08-29,18:55:48 | INFO | Train Epoch: 11 [300100/145000.0 (207%)] Data (t): 0.102 Batch (t): 0.192, 492.581/s, 492.581/s/gpu LR: 9.611378e-09 Logit Scale: 100.000 Contrastive_loss: 0.32593 (0.27412) Loss: 0.32593 (0.27412)
2024-08-29,18:56:08 | INFO | Train Epoch: 11 [310100/145000.0 (214%)] Data (t): 0.105 Batch (t): 0.193, 533.045/s, 533.045/s/gpu LR: 9.607320e-09 Logit Scale: 100.000 Contrastive_loss: 0.32339 (0.27561) Loss: 0.32339 (0.27561)
2024-08-29,18:56:27 | INFO | Train Epoch: 11 [320100/145000.0 (221%)] Data (t): 0.103 Batch (t): 0.193, 531.596/s, 531.596/s/gpu LR: 9.603242e-09 Logit Scale: 100.000 Contrastive_loss: 0.36876 (0.27835) Loss: 0.36876 (0.27835)
2024-08-29,18:56:46 | INFO | Train Epoch: 11 [330100/145000.0 (228%)] Data (t): 0.104 Batch (t): 0.193, 533.309/s, 533.309/s/gpu LR: 9.599143e-09 Logit Scale: 100.000 Contrastive_loss: 0.22130 (0.27672) Loss: 0.22130 (0.27672)
2024-08-29,18:57:05 | INFO | Train Epoch: 11 [340100/145000.0 (235%)] Data (t): 0.103 Batch (t): 0.193, 492.285/s, 492.285/s/gpu LR: 9.595025e-09 Logit Scale: 100.000 Contrastive_loss: 0.35855 (0.27899) Loss: 0.35855 (0.27899)
2024-08-29,18:57:25 | INFO | Train Epoch: 11 [350100/145000.0 (241%)] Data (t): 0.104 Batch (t): 0.192, 533.403/s, 533.403/s/gpu LR: 9.590886e-09 Logit Scale: 100.000 Contrastive_loss: 0.40223 (0.28232) Loss: 0.40223 (0.28232)
2024-08-29,18:57:44 | INFO | Train Epoch: 11 [360100/145000.0 (248%)] Data (t): 0.103 Batch (t): 0.192, 533.334/s, 533.334/s/gpu LR: 9.586727e-09 Logit Scale: 100.000 Contrastive_loss: 0.20604 (0.28032) Loss: 0.20604 (0.28032)
2024-08-29,18:58:03 | INFO | Train Epoch: 11 [370100/145000.0 (255%)] Data (t): 0.104 Batch (t): 0.193, 495.016/s, 495.016/s/gpu LR: 9.582549e-09 Logit Scale: 100.000 Contrastive_loss: 0.31372 (0.28117) Loss: 0.31372 (0.28117)
2024-08-29,18:58:22 | INFO | Train Epoch: 11 [380100/145000.0 (262%)] Data (t): 0.105 Batch (t): 0.190, 533.333/s, 533.333/s/gpu LR: 9.578350e-09 Logit Scale: 100.000 Contrastive_loss: 0.30337 (0.28173) Loss: 0.30337 (0.28173)
2024-08-29,18:58:41 | INFO | Train Epoch: 11 [390100/145000.0 (269%)] Data (t): 0.104 Batch (t): 0.191, 533.644/s, 533.644/s/gpu LR: 9.574130e-09 Logit Scale: 100.000 Contrastive_loss: 0.19783 (0.27968) Loss: 0.19783 (0.27968)
2024-08-29,18:59:00 | INFO | Train Epoch: 11 [400100/145000.0 (276%)] Data (t): 0.102 Batch (t): 0.191, 533.327/s, 533.327/s/gpu LR: 9.569891e-09 Logit Scale: 100.000 Contrastive_loss: 0.28402 (0.27978) Loss: 0.28402 (0.27978)
2024-08-29,18:59:19 | INFO | Train Epoch: 11 [410100/145000.0 (283%)] Data (t): 0.104 Batch (t): 0.191, 533.533/s, 533.533/s/gpu LR: 9.565632e-09 Logit Scale: 100.000 Contrastive_loss: 0.12140 (0.27610) Loss: 0.12140 (0.27610)
2024-08-29,18:59:27 | INFO | Eval Epoch: 12 [200 / 1000]	Clip Loss: 0.516730	
2024-08-29,18:59:28 | INFO | Eval Epoch: 12 image_to_text_mean_rank: 2.5830	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6430	image_to_text_R@5: 0.9040	image_to_text_R@10: 0.9600	text_to_image_mean_rank: 3.4560	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6580	text_to_image_R@5: 0.8890	text_to_image_R@10: 0.9550	clip_val_loss: 0.4676	epoch: 12.0000	num_samples: 1000.0000
2024-08-29,18:59:29 | INFO | Start epoch 12
2024-08-29,18:59:29 | INFO | Train Epoch: 12 [   100/145000.0 (0%)] Data (t): 0.016 Batch (t): 0.109, 915.136/s, 915.136/s/gpu LR: 9.671678e-09 Logit Scale: 100.000 Contrastive_loss: 0.23155 (0.23155) Loss: 0.23155 (0.23155)
2024-08-29,18:59:48 | INFO | Train Epoch: 12 [ 10100/145000.0 (7%)] Data (t): 0.103 Batch (t): 0.192, 492.307/s, 492.307/s/gpu LR: 9.667935e-09 Logit Scale: 100.000 Contrastive_loss: 0.42076 (0.32616) Loss: 0.42076 (0.32616)
2024-08-29,19:00:07 | INFO | Train Epoch: 12 [ 20100/145000.0 (14%)] Data (t): 0.104 Batch (t): 0.192, 533.334/s, 533.334/s/gpu LR: 9.664172e-09 Logit Scale: 100.000 Contrastive_loss: 0.44167 (0.36466) Loss: 0.44167 (0.36466)
2024-08-29,19:00:27 | INFO | Train Epoch: 12 [ 30100/145000.0 (21%)] Data (t): 0.103 Batch (t): 0.192, 533.339/s, 533.339/s/gpu LR: 9.660389e-09 Logit Scale: 100.000 Contrastive_loss: 0.25072 (0.33618) Loss: 0.25072 (0.33618)
2024-08-29,19:00:46 | INFO | Train Epoch: 12 [ 40100/145000.0 (28%)] Data (t): 0.103 Batch (t): 0.191, 533.360/s, 533.360/s/gpu LR: 9.656585e-09 Logit Scale: 100.000 Contrastive_loss: 0.17512 (0.30396) Loss: 0.17512 (0.30396)
2024-08-29,19:01:05 | INFO | Train Epoch: 12 [ 50100/145000.0 (35%)] Data (t): 0.101 Batch (t): 0.191, 492.295/s, 492.295/s/gpu LR: 9.652761e-09 Logit Scale: 100.000 Contrastive_loss: 0.21232 (0.28869) Loss: 0.21232 (0.28869)
2024-08-29,19:01:24 | INFO | Train Epoch: 12 [ 60100/145000.0 (41%)] Data (t): 0.103 Batch (t): 0.191, 533.333/s, 533.333/s/gpu LR: 9.648916e-09 Logit Scale: 100.000 Contrastive_loss: 0.15883 (0.27014) Loss: 0.15883 (0.27014)
2024-08-29,19:01:43 | INFO | Train Epoch: 12 [ 70100/145000.0 (48%)] Data (t): 0.102 Batch (t): 0.192, 533.334/s, 533.334/s/gpu LR: 9.645051e-09 Logit Scale: 100.000 Contrastive_loss: 0.30698 (0.27474) Loss: 0.30698 (0.27474)
2024-08-29,19:02:02 | INFO | Train Epoch: 12 [ 80100/145000.0 (55%)] Data (t): 0.103 Batch (t): 0.191, 533.335/s, 533.335/s/gpu LR: 9.641166e-09 Logit Scale: 100.000 Contrastive_loss: 0.19565 (0.26596) Loss: 0.19565 (0.26596)
2024-08-29,19:02:21 | INFO | Train Epoch: 12 [ 90100/145000.0 (62%)] Data (t): 0.103 Batch (t): 0.191, 533.333/s, 533.333/s/gpu LR: 9.637260e-09 Logit Scale: 100.000 Contrastive_loss: 0.27239 (0.26660) Loss: 0.27239 (0.26660)
2024-08-29,19:02:41 | INFO | Train Epoch: 12 [100100/145000.0 (69%)] Data (t): 0.103 Batch (t): 0.191, 533.332/s, 533.332/s/gpu LR: 9.633334e-09 Logit Scale: 100.000 Contrastive_loss: 0.19604 (0.26018) Loss: 0.19604 (0.26018)
2024-08-29,19:03:00 | INFO | Train Epoch: 12 [110100/145000.0 (76%)] Data (t): 0.103 Batch (t): 0.191, 536.298/s, 536.298/s/gpu LR: 9.629388e-09 Logit Scale: 100.000 Contrastive_loss: 0.23278 (0.25790) Loss: 0.23278 (0.25790)
2024-08-29,19:03:19 | INFO | Train Epoch: 12 [120100/145000.0 (83%)] Data (t): 0.103 Batch (t): 0.191, 492.318/s, 492.318/s/gpu LR: 9.625421e-09 Logit Scale: 100.000 Contrastive_loss: 0.36653 (0.26626) Loss: 0.36653 (0.26626)
2024-08-29,19:03:38 | INFO | Train Epoch: 12 [130100/145000.0 (90%)] Data (t): 0.103 Batch (t): 0.191, 533.331/s, 533.331/s/gpu LR: 9.621434e-09 Logit Scale: 100.000 Contrastive_loss: 0.20381 (0.26180) Loss: 0.20381 (0.26180)
2024-08-29,19:03:57 | INFO | Train Epoch: 12 [140100/145000.0 (97%)] Data (t): 0.101 Batch (t): 0.191, 492.310/s, 492.310/s/gpu LR: 9.617427e-09 Logit Scale: 100.000 Contrastive_loss: 0.22295 (0.25921) Loss: 0.22295 (0.25921)
2024-08-29,19:04:06 | INFO | Train Epoch: 12 [145000/145000.0 (100%)] Data (t): 0.105 Batch (t): 0.191, 533.335/s, 533.335/s/gpu LR: 9.615456e-09 Logit Scale: 100.000 Contrastive_loss: 0.29239 (0.26128) Loss: 0.29239 (0.26128)
2024-08-29,19:04:16 | INFO | Train Epoch: 12 [150100/145000.0 (104%)] Data (t): 0.105 Batch (t): 0.191, 533.331/s, 533.331/s/gpu LR: 9.613399e-09 Logit Scale: 100.000 Contrastive_loss: 0.27565 (0.26213) Loss: 0.27565 (0.26213)
2024-08-29,19:04:35 | INFO | Train Epoch: 12 [160100/145000.0 (110%)] Data (t): 0.101 Batch (t): 0.191, 533.337/s, 533.337/s/gpu LR: 9.609351e-09 Logit Scale: 100.000 Contrastive_loss: 0.33406 (0.26612) Loss: 0.33406 (0.26612)
2024-08-29,19:04:54 | INFO | Train Epoch: 12 [170100/145000.0 (117%)] Data (t): 0.102 Batch (t): 0.191, 533.054/s, 533.054/s/gpu LR: 9.605283e-09 Logit Scale: 100.000 Contrastive_loss: 0.23970 (0.26473) Loss: 0.23970 (0.26473)
2024-08-29,19:05:13 | INFO | Train Epoch: 12 [180100/145000.0 (124%)] Data (t): 0.103 Batch (t): 0.191, 533.481/s, 533.481/s/gpu LR: 9.601195e-09 Logit Scale: 100.000 Contrastive_loss: 0.24668 (0.26383) Loss: 0.24668 (0.26383)
2024-08-29,19:05:32 | INFO | Train Epoch: 12 [190100/145000.0 (131%)] Data (t): 0.102 Batch (t): 0.191, 533.335/s, 533.335/s/gpu LR: 9.597087e-09 Logit Scale: 100.000 Contrastive_loss: 0.31619 (0.26632) Loss: 0.31619 (0.26632)
2024-08-29,19:05:51 | INFO | Train Epoch: 12 [200100/145000.0 (138%)] Data (t): 0.103 Batch (t): 0.191, 491.381/s, 491.381/s/gpu LR: 9.592958e-09 Logit Scale: 100.000 Contrastive_loss: 0.20016 (0.26331) Loss: 0.20016 (0.26331)
2024-08-29,19:06:11 | INFO | Train Epoch: 12 [210100/145000.0 (145%)] Data (t): 0.103 Batch (t): 0.191, 533.333/s, 533.333/s/gpu LR: 9.588809e-09 Logit Scale: 100.000 Contrastive_loss: 0.31184 (0.26542) Loss: 0.31184 (0.26542)
2024-08-29,19:06:30 | INFO | Train Epoch: 12 [220100/145000.0 (152%)] Data (t): 0.101 Batch (t): 0.190, 492.305/s, 492.305/s/gpu LR: 9.584640e-09 Logit Scale: 100.000 Contrastive_loss: 0.41477 (0.27165) Loss: 0.41477 (0.27165)
2024-08-29,19:06:49 | INFO | Train Epoch: 12 [230100/145000.0 (159%)] Data (t): 0.101 Batch (t): 0.190, 533.333/s, 533.333/s/gpu LR: 9.580452e-09 Logit Scale: 100.000 Contrastive_loss: 0.34588 (0.27462) Loss: 0.34588 (0.27462)
2024-08-29,19:07:08 | INFO | Train Epoch: 12 [240100/145000.0 (166%)] Data (t): 0.103 Batch (t): 0.191, 491.915/s, 491.915/s/gpu LR: 9.576242e-09 Logit Scale: 100.000 Contrastive_loss: 0.22870 (0.27285) Loss: 0.22870 (0.27285)
2024-08-29,19:07:27 | INFO | Train Epoch: 12 [250100/145000.0 (172%)] Data (t): 0.103 Batch (t): 0.191, 533.339/s, 533.339/s/gpu LR: 9.572013e-09 Logit Scale: 100.000 Contrastive_loss: 0.26451 (0.27254) Loss: 0.26451 (0.27254)
2024-08-29,19:07:46 | INFO | Train Epoch: 12 [260100/145000.0 (179%)] Data (t): 0.104 Batch (t): 0.191, 492.307/s, 492.307/s/gpu LR: 9.567764e-09 Logit Scale: 100.000 Contrastive_loss: 0.28811 (0.27310) Loss: 0.28811 (0.27310)
2024-08-29,19:08:05 | INFO | Train Epoch: 12 [270100/145000.0 (186%)] Data (t): 0.105 Batch (t): 0.192, 532.248/s, 532.248/s/gpu LR: 9.563495e-09 Logit Scale: 100.000 Contrastive_loss: 0.29720 (0.27393) Loss: 0.29720 (0.27393)
2024-08-29,19:08:24 | INFO | Train Epoch: 12 [280100/145000.0 (193%)] Data (t): 0.103 Batch (t): 0.191, 533.332/s, 533.332/s/gpu LR: 9.559206e-09 Logit Scale: 100.000 Contrastive_loss: 0.22378 (0.27226) Loss: 0.22378 (0.27226)
2024-08-29,19:08:43 | INFO | Train Epoch: 12 [290100/145000.0 (200%)] Data (t): 0.103 Batch (t): 0.191, 492.309/s, 492.309/s/gpu LR: 9.554897e-09 Logit Scale: 100.000 Contrastive_loss: 0.17399 (0.26909) Loss: 0.17399 (0.26909)
2024-08-29,19:09:02 | INFO | Train Epoch: 12 [300100/145000.0 (207%)] Data (t): 0.101 Batch (t): 0.190, 532.490/s, 532.490/s/gpu LR: 9.550567e-09 Logit Scale: 100.000 Contrastive_loss: 0.32300 (0.27077) Loss: 0.32300 (0.27077)
2024-08-29,19:09:21 | INFO | Train Epoch: 12 [310100/145000.0 (214%)] Data (t): 0.103 Batch (t): 0.190, 533.335/s, 533.335/s/gpu LR: 9.546218e-09 Logit Scale: 100.000 Contrastive_loss: 0.31561 (0.27213) Loss: 0.31561 (0.27213)
2024-08-29,19:09:40 | INFO | Train Epoch: 12 [320100/145000.0 (221%)] Data (t): 0.103 Batch (t): 0.191, 533.004/s, 533.004/s/gpu LR: 9.541849e-09 Logit Scale: 100.000 Contrastive_loss: 0.36479 (0.27486) Loss: 0.36479 (0.27486)
2024-08-29,19:10:00 | INFO | Train Epoch: 12 [330100/145000.0 (228%)] Data (t): 0.103 Batch (t): 0.191, 492.305/s, 492.305/s/gpu LR: 9.537460e-09 Logit Scale: 100.000 Contrastive_loss: 0.21965 (0.27328) Loss: 0.21965 (0.27328)
2024-08-29,19:10:19 | INFO | Train Epoch: 12 [340100/145000.0 (235%)] Data (t): 0.102 Batch (t): 0.191, 533.335/s, 533.335/s/gpu LR: 9.533051e-09 Logit Scale: 100.000 Contrastive_loss: 0.35291 (0.27549) Loss: 0.35291 (0.27549)
2024-08-29,19:10:38 | INFO | Train Epoch: 12 [350100/145000.0 (241%)] Data (t): 0.102 Batch (t): 0.191, 533.333/s, 533.333/s/gpu LR: 9.528622e-09 Logit Scale: 100.000 Contrastive_loss: 0.39755 (0.27879) Loss: 0.39755 (0.27879)
2024-08-29,19:10:57 | INFO | Train Epoch: 12 [360100/145000.0 (248%)] Data (t): 0.103 Batch (t): 0.191, 491.267/s, 491.267/s/gpu LR: 9.524174e-09 Logit Scale: 100.000 Contrastive_loss: 0.20340 (0.27681) Loss: 0.20340 (0.27681)
2024-08-29,19:11:16 | INFO | Train Epoch: 12 [370100/145000.0 (255%)] Data (t): 0.103 Batch (t): 0.191, 533.358/s, 533.358/s/gpu LR: 9.519705e-09 Logit Scale: 100.000 Contrastive_loss: 0.31084 (0.27768) Loss: 0.31084 (0.27768)
2024-08-29,19:11:35 | INFO | Train Epoch: 12 [380100/145000.0 (262%)] Data (t): 0.102 Batch (t): 0.191, 533.334/s, 533.334/s/gpu LR: 9.515217e-09 Logit Scale: 100.000 Contrastive_loss: 0.30011 (0.27824) Loss: 0.30011 (0.27824)
2024-08-29,19:11:54 | INFO | Train Epoch: 12 [390100/145000.0 (269%)] Data (t): 0.103 Batch (t): 0.191, 533.333/s, 533.333/s/gpu LR: 9.510709e-09 Logit Scale: 100.000 Contrastive_loss: 0.19417 (0.27619) Loss: 0.19417 (0.27619)
2024-08-29,19:12:13 | INFO | Train Epoch: 12 [400100/145000.0 (276%)] Data (t): 0.101 Batch (t): 0.190, 533.323/s, 533.323/s/gpu LR: 9.506181e-09 Logit Scale: 100.000 Contrastive_loss: 0.28171 (0.27632) Loss: 0.28171 (0.27632)
2024-08-29,19:12:32 | INFO | Train Epoch: 12 [410100/145000.0 (283%)] Data (t): 0.105 Batch (t): 0.191, 492.302/s, 492.302/s/gpu LR: 9.501633e-09 Logit Scale: 100.000 Contrastive_loss: 0.12041 (0.27269) Loss: 0.12041 (0.27269)
2024-08-29,19:12:40 | INFO | Eval Epoch: 13 [200 / 1000]	Clip Loss: 0.512685	
2024-08-29,19:12:40 | INFO | Eval Epoch: 13 image_to_text_mean_rank: 2.5690	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6450	image_to_text_R@5: 0.9050	image_to_text_R@10: 0.9600	text_to_image_mean_rank: 3.4410	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6620	text_to_image_R@5: 0.8900	text_to_image_R@10: 0.9580	clip_val_loss: 0.4648	epoch: 13.0000	num_samples: 1000.0000
2024-08-29,19:12:42 | INFO | Start epoch 13
2024-08-29,19:12:42 | INFO | Train Epoch: 13 [   100/145000.0 (0%)] Data (t): 0.031 Batch (t): 0.109, 916.031/s, 916.031/s/gpu LR: 9.615415e-09 Logit Scale: 100.000 Contrastive_loss: 0.22797 (0.22797) Loss: 0.22797 (0.22797)
2024-08-29,19:13:01 | INFO | Train Epoch: 13 [ 10100/145000.0 (7%)] Data (t): 0.105 Batch (t): 0.193, 481.574/s, 481.574/s/gpu LR: 9.611378e-09 Logit Scale: 100.000 Contrastive_loss: 0.41578 (0.32187) Loss: 0.41578 (0.32187)
2024-08-29,19:13:20 | INFO | Train Epoch: 13 [ 20100/145000.0 (14%)] Data (t): 0.105 Batch (t): 0.191, 492.307/s, 492.307/s/gpu LR: 9.607320e-09 Logit Scale: 100.000 Contrastive_loss: 0.43790 (0.36055) Loss: 0.43790 (0.36055)
2024-08-29,19:13:39 | INFO | Train Epoch: 13 [ 30100/145000.0 (21%)] Data (t): 0.105 Batch (t): 0.192, 531.936/s, 531.936/s/gpu LR: 9.603242e-09 Logit Scale: 100.000 Contrastive_loss: 0.24702 (0.33217) Loss: 0.24702 (0.33217)
2024-08-29,19:13:59 | INFO | Train Epoch: 13 [ 40100/145000.0 (28%)] Data (t): 0.103 Batch (t): 0.191, 491.610/s, 491.610/s/gpu LR: 9.599143e-09 Logit Scale: 100.000 Contrastive_loss: 0.17294 (0.30032) Loss: 0.17294 (0.30032)
2024-08-29,19:14:18 | INFO | Train Epoch: 13 [ 50100/145000.0 (35%)] Data (t): 0.102 Batch (t): 0.191, 531.220/s, 531.220/s/gpu LR: 9.595025e-09 Logit Scale: 100.000 Contrastive_loss: 0.21066 (0.28538) Loss: 0.21066 (0.28538)
2024-08-29,19:14:37 | INFO | Train Epoch: 13 [ 60100/145000.0 (41%)] Data (t): 0.104 Batch (t): 0.191, 493.806/s, 493.806/s/gpu LR: 9.590886e-09 Logit Scale: 100.000 Contrastive_loss: 0.15630 (0.26694) Loss: 0.15630 (0.26694)
2024-08-29,19:14:56 | INFO | Train Epoch: 13 [ 70100/145000.0 (48%)] Data (t): 0.102 Batch (t): 0.192, 533.335/s, 533.335/s/gpu LR: 9.586727e-09 Logit Scale: 100.000 Contrastive_loss: 0.30339 (0.27149) Loss: 0.30339 (0.27149)
2024-08-29,19:15:15 | INFO | Train Epoch: 13 [ 80100/145000.0 (55%)] Data (t): 0.102 Batch (t): 0.191, 491.962/s, 491.962/s/gpu LR: 9.582549e-09 Logit Scale: 100.000 Contrastive_loss: 0.19257 (0.26272) Loss: 0.19257 (0.26272)
2024-08-29,19:15:34 | INFO | Train Epoch: 13 [ 90100/145000.0 (62%)] Data (t): 0.102 Batch (t): 0.191, 533.178/s, 533.178/s/gpu LR: 9.578350e-09 Logit Scale: 100.000 Contrastive_loss: 0.26975 (0.26343) Loss: 0.26975 (0.26343)
2024-08-29,19:15:53 | INFO | Train Epoch: 13 [100100/145000.0 (69%)] Data (t): 0.104 Batch (t): 0.191, 491.582/s, 491.582/s/gpu LR: 9.574130e-09 Logit Scale: 100.000 Contrastive_loss: 0.19406 (0.25712) Loss: 0.19406 (0.25712)
2024-08-29,19:16:12 | INFO | Train Epoch: 13 [110100/145000.0 (76%)] Data (t): 0.102 Batch (t): 0.191, 492.325/s, 492.325/s/gpu LR: 9.569891e-09 Logit Scale: 100.000 Contrastive_loss: 0.22985 (0.25485) Loss: 0.22985 (0.25485)
2024-08-29,19:16:32 | INFO | Train Epoch: 13 [120100/145000.0 (83%)] Data (t): 0.105 Batch (t): 0.193, 490.421/s, 490.421/s/gpu LR: 9.565632e-09 Logit Scale: 100.000 Contrastive_loss: 0.36156 (0.26306) Loss: 0.36156 (0.26306)
2024-08-29,19:16:52 | INFO | Train Epoch: 13 [130100/145000.0 (90%)] Data (t): 0.110 Batch (t): 0.199, 530.170/s, 530.170/s/gpu LR: 9.561353e-09 Logit Scale: 100.000 Contrastive_loss: 0.20094 (0.25862) Loss: 0.20094 (0.25862)
2024-08-29,19:17:11 | INFO | Train Epoch: 13 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.198, 497.352/s, 497.352/s/gpu LR: 9.557054e-09 Logit Scale: 100.000 Contrastive_loss: 0.22035 (0.25607) Loss: 0.22035 (0.25607)
2024-08-29,19:17:21 | INFO | Train Epoch: 13 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.201, 492.099/s, 492.099/s/gpu LR: 9.554940e-09 Logit Scale: 100.000 Contrastive_loss: 0.28853 (0.25810) Loss: 0.28853 (0.25810)
2024-08-29,19:17:31 | INFO | Train Epoch: 13 [150100/145000.0 (104%)] Data (t): 0.110 Batch (t): 0.200, 511.488/s, 511.488/s/gpu LR: 9.552734e-09 Logit Scale: 100.000 Contrastive_loss: 0.27145 (0.25888) Loss: 0.27145 (0.25888)
2024-08-29,19:17:51 | INFO | Train Epoch: 13 [160100/145000.0 (110%)] Data (t): 0.111 Batch (t): 0.199, 533.226/s, 533.226/s/gpu LR: 9.548395e-09 Logit Scale: 100.000 Contrastive_loss: 0.32828 (0.26274) Loss: 0.32828 (0.26274)
2024-08-29,19:18:11 | INFO | Train Epoch: 13 [170100/145000.0 (117%)] Data (t): 0.110 Batch (t): 0.200, 492.309/s, 492.309/s/gpu LR: 9.544036e-09 Logit Scale: 100.000 Contrastive_loss: 0.23626 (0.26135) Loss: 0.23626 (0.26135)
2024-08-29,19:18:31 | INFO | Train Epoch: 13 [180100/145000.0 (124%)] Data (t): 0.110 Batch (t): 0.199, 492.287/s, 492.287/s/gpu LR: 9.539657e-09 Logit Scale: 100.000 Contrastive_loss: 0.24348 (0.26045) Loss: 0.24348 (0.26045)
2024-08-29,19:18:51 | INFO | Train Epoch: 13 [190100/145000.0 (131%)] Data (t): 0.110 Batch (t): 0.199, 492.096/s, 492.096/s/gpu LR: 9.535258e-09 Logit Scale: 100.000 Contrastive_loss: 0.31274 (0.26294) Loss: 0.31274 (0.26294)
2024-08-29,19:19:11 | INFO | Train Epoch: 13 [200100/145000.0 (138%)] Data (t): 0.111 Batch (t): 0.201, 513.327/s, 513.327/s/gpu LR: 9.530839e-09 Logit Scale: 100.000 Contrastive_loss: 0.19871 (0.26002) Loss: 0.19871 (0.26002)
2024-08-29,19:19:31 | INFO | Train Epoch: 13 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.199, 513.142/s, 513.142/s/gpu LR: 9.526400e-09 Logit Scale: 100.000 Contrastive_loss: 0.30894 (0.26215) Loss: 0.30894 (0.26215)
2024-08-29,19:19:51 | INFO | Train Epoch: 13 [220100/145000.0 (152%)] Data (t): 0.110 Batch (t): 0.200, 500.947/s, 500.947/s/gpu LR: 9.521942e-09 Logit Scale: 100.000 Contrastive_loss: 0.41018 (0.26832) Loss: 0.41018 (0.26832)
2024-08-29,19:20:11 | INFO | Train Epoch: 13 [230100/145000.0 (159%)] Data (t): 0.110 Batch (t): 0.200, 533.381/s, 533.381/s/gpu LR: 9.517463e-09 Logit Scale: 100.000 Contrastive_loss: 0.34447 (0.27136) Loss: 0.34447 (0.27136)
2024-08-29,19:20:31 | INFO | Train Epoch: 13 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.199, 492.277/s, 492.277/s/gpu LR: 9.512965e-09 Logit Scale: 100.000 Contrastive_loss: 0.22591 (0.26961) Loss: 0.22591 (0.26961)
2024-08-29,19:20:51 | INFO | Train Epoch: 13 [250100/145000.0 (172%)] Data (t): 0.110 Batch (t): 0.199, 505.200/s, 505.200/s/gpu LR: 9.508447e-09 Logit Scale: 100.000 Contrastive_loss: 0.26198 (0.26933) Loss: 0.26198 (0.26933)
2024-08-29,19:21:11 | INFO | Train Epoch: 13 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.199, 533.322/s, 533.322/s/gpu LR: 9.503909e-09 Logit Scale: 100.000 Contrastive_loss: 0.28424 (0.26986) Loss: 0.28424 (0.26986)
2024-08-29,19:21:31 | INFO | Train Epoch: 13 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.199, 491.248/s, 491.248/s/gpu LR: 9.499352e-09 Logit Scale: 100.000 Contrastive_loss: 0.29564 (0.27075) Loss: 0.29564 (0.27075)
2024-08-29,19:21:50 | INFO | Train Epoch: 13 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.199, 486.058/s, 486.058/s/gpu LR: 9.494774e-09 Logit Scale: 100.000 Contrastive_loss: 0.22085 (0.26909) Loss: 0.22085 (0.26909)
2024-08-29,19:22:10 | INFO | Train Epoch: 13 [290100/145000.0 (200%)] Data (t): 0.110 Batch (t): 0.199, 513.350/s, 513.350/s/gpu LR: 9.490177e-09 Logit Scale: 100.000 Contrastive_loss: 0.17244 (0.26597) Loss: 0.17244 (0.26597)
2024-08-29,19:22:30 | INFO | Train Epoch: 13 [300100/145000.0 (207%)] Data (t): 0.111 Batch (t): 0.200, 513.156/s, 513.156/s/gpu LR: 9.485561e-09 Logit Scale: 100.000 Contrastive_loss: 0.32026 (0.26767) Loss: 0.32026 (0.26767)
2024-08-29,19:22:50 | INFO | Train Epoch: 13 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.199, 533.339/s, 533.339/s/gpu LR: 9.480924e-09 Logit Scale: 100.000 Contrastive_loss: 0.30892 (0.26892) Loss: 0.30892 (0.26892)
2024-08-29,19:23:10 | INFO | Train Epoch: 13 [320100/145000.0 (221%)] Data (t): 0.110 Batch (t): 0.201, 487.787/s, 487.787/s/gpu LR: 9.476268e-09 Logit Scale: 100.000 Contrastive_loss: 0.36100 (0.27163) Loss: 0.36100 (0.27163)
2024-08-29,19:23:31 | INFO | Train Epoch: 13 [330100/145000.0 (228%)] Data (t): 0.112 Batch (t): 0.203, 532.209/s, 532.209/s/gpu LR: 9.471592e-09 Logit Scale: 100.000 Contrastive_loss: 0.21814 (0.27010) Loss: 0.21814 (0.27010)
2024-08-29,19:23:51 | INFO | Train Epoch: 13 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.199, 533.347/s, 533.347/s/gpu LR: 9.466897e-09 Logit Scale: 100.000 Contrastive_loss: 0.34779 (0.27226) Loss: 0.34779 (0.27226)
2024-08-29,19:24:10 | INFO | Train Epoch: 13 [350100/145000.0 (241%)] Data (t): 0.110 Batch (t): 0.199, 492.063/s, 492.063/s/gpu LR: 9.462182e-09 Logit Scale: 100.000 Contrastive_loss: 0.39322 (0.27553) Loss: 0.39322 (0.27553)
2024-08-29,19:24:30 | INFO | Train Epoch: 13 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.199, 533.390/s, 533.390/s/gpu LR: 9.457448e-09 Logit Scale: 100.000 Contrastive_loss: 0.20109 (0.27357) Loss: 0.20109 (0.27357)
2024-08-29,19:24:49 | INFO | Train Epoch: 13 [370100/145000.0 (255%)] Data (t): 0.105 Batch (t): 0.190, 564.648/s, 564.648/s/gpu LR: 9.452694e-09 Logit Scale: 100.000 Contrastive_loss: 0.30823 (0.27446) Loss: 0.30823 (0.27446)
2024-08-29,19:25:08 | INFO | Train Epoch: 13 [380100/145000.0 (262%)] Data (t): 0.105 Batch (t): 0.191, 490.240/s, 490.240/s/gpu LR: 9.447920e-09 Logit Scale: 100.000 Contrastive_loss: 0.29689 (0.27502) Loss: 0.29689 (0.27502)
2024-08-29,19:25:28 | INFO | Train Epoch: 13 [390100/145000.0 (269%)] Data (t): 0.106 Batch (t): 0.191, 526.893/s, 526.893/s/gpu LR: 9.443127e-09 Logit Scale: 100.000 Contrastive_loss: 0.19087 (0.27296) Loss: 0.19087 (0.27296)
2024-08-29,19:25:47 | INFO | Train Epoch: 13 [400100/145000.0 (276%)] Data (t): 0.107 Batch (t): 0.192, 520.810/s, 520.810/s/gpu LR: 9.438314e-09 Logit Scale: 100.000 Contrastive_loss: 0.27936 (0.27312) Loss: 0.27936 (0.27312)
2024-08-29,19:26:06 | INFO | Train Epoch: 13 [410100/145000.0 (283%)] Data (t): 0.107 Batch (t): 0.192, 522.107/s, 522.107/s/gpu LR: 9.433482e-09 Logit Scale: 100.000 Contrastive_loss: 0.11952 (0.26954) Loss: 0.11952 (0.26954)
2024-08-29,19:26:13 | INFO | Eval Epoch: 14 [200 / 1000]	Clip Loss: 0.509107	
2024-08-29,19:26:14 | INFO | Eval Epoch: 14 image_to_text_mean_rank: 2.5630	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6440	image_to_text_R@5: 0.9070	image_to_text_R@10: 0.9600	text_to_image_mean_rank: 3.4390	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6630	text_to_image_R@5: 0.8910	text_to_image_R@10: 0.9580	clip_val_loss: 0.4623	epoch: 14.0000	num_samples: 1000.0000
2024-08-29,19:26:15 | INFO | Start epoch 14
2024-08-29,19:26:15 | INFO | Train Epoch: 14 [   100/145000.0 (0%)] Data (t): 0.027 Batch (t): 0.109, 913.471/s, 913.471/s/gpu LR: 9.554897e-09 Logit Scale: 100.000 Contrastive_loss: 0.22456 (0.22456) Loss: 0.22456 (0.22456)
2024-08-29,19:26:35 | INFO | Train Epoch: 14 [ 10100/145000.0 (7%)] Data (t): 0.108 Batch (t): 0.193, 526.501/s, 526.501/s/gpu LR: 9.550567e-09 Logit Scale: 100.000 Contrastive_loss: 0.41116 (0.31786) Loss: 0.41116 (0.31786)
2024-08-29,19:26:54 | INFO | Train Epoch: 14 [ 20100/145000.0 (14%)] Data (t): 0.106 Batch (t): 0.192, 531.808/s, 531.808/s/gpu LR: 9.546218e-09 Logit Scale: 100.000 Contrastive_loss: 0.43431 (0.35668) Loss: 0.43431 (0.35668)
2024-08-29,19:27:13 | INFO | Train Epoch: 14 [ 30100/145000.0 (21%)] Data (t): 0.107 Batch (t): 0.192, 504.191/s, 504.191/s/gpu LR: 9.541849e-09 Logit Scale: 100.000 Contrastive_loss: 0.24366 (0.32842) Loss: 0.24366 (0.32842)
2024-08-29,19:27:32 | INFO | Train Epoch: 14 [ 40100/145000.0 (28%)] Data (t): 0.107 Batch (t): 0.192, 530.663/s, 530.663/s/gpu LR: 9.537460e-09 Logit Scale: 100.000 Contrastive_loss: 0.17095 (0.29693) Loss: 0.17095 (0.29693)
2024-08-29,19:27:51 | INFO | Train Epoch: 14 [ 50100/145000.0 (35%)] Data (t): 0.107 Batch (t): 0.191, 491.928/s, 491.928/s/gpu LR: 9.533051e-09 Logit Scale: 100.000 Contrastive_loss: 0.20904 (0.28228) Loss: 0.20904 (0.28228)
2024-08-29,19:28:10 | INFO | Train Epoch: 14 [ 60100/145000.0 (41%)] Data (t): 0.107 Batch (t): 0.191, 496.149/s, 496.149/s/gpu LR: 9.528622e-09 Logit Scale: 100.000 Contrastive_loss: 0.15406 (0.26396) Loss: 0.15406 (0.26396)
2024-08-29,19:28:30 | INFO | Train Epoch: 14 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.197, 476.190/s, 476.190/s/gpu LR: 9.524174e-09 Logit Scale: 100.000 Contrastive_loss: 0.30008 (0.26848) Loss: 0.30008 (0.26848)
2024-08-29,19:28:50 | INFO | Train Epoch: 14 [ 80100/145000.0 (55%)] Data (t): 0.110 Batch (t): 0.200, 476.973/s, 476.973/s/gpu LR: 9.519705e-09 Logit Scale: 100.000 Contrastive_loss: 0.18952 (0.25970) Loss: 0.18952 (0.25970)
2024-08-29,19:29:10 | INFO | Train Epoch: 14 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.197, 497.433/s, 497.433/s/gpu LR: 9.515217e-09 Logit Scale: 100.000 Contrastive_loss: 0.26742 (0.26048) Loss: 0.26742 (0.26048)
2024-08-29,19:29:30 | INFO | Train Epoch: 14 [100100/145000.0 (69%)] Data (t): 0.111 Batch (t): 0.201, 432.358/s, 432.358/s/gpu LR: 9.510709e-09 Logit Scale: 100.000 Contrastive_loss: 0.19228 (0.25428) Loss: 0.19228 (0.25428)
2024-08-29,19:29:49 | INFO | Train Epoch: 14 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.195, 530.971/s, 530.971/s/gpu LR: 9.506181e-09 Logit Scale: 100.000 Contrastive_loss: 0.22698 (0.25200) Loss: 0.22698 (0.25200)
2024-08-29,19:30:09 | INFO | Train Epoch: 14 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.197, 524.507/s, 524.507/s/gpu LR: 9.501633e-09 Logit Scale: 100.000 Contrastive_loss: 0.35686 (0.26007) Loss: 0.35686 (0.26007)
2024-08-29,19:30:28 | INFO | Train Epoch: 14 [130100/145000.0 (90%)] Data (t): 0.107 Batch (t): 0.193, 523.387/s, 523.387/s/gpu LR: 9.497065e-09 Logit Scale: 100.000 Contrastive_loss: 0.19834 (0.25566) Loss: 0.19834 (0.25566)
2024-08-29,19:30:48 | INFO | Train Epoch: 14 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.196, 522.478/s, 522.478/s/gpu LR: 9.492478e-09 Logit Scale: 100.000 Contrastive_loss: 0.21787 (0.25314) Loss: 0.21787 (0.25314)
2024-08-29,19:30:58 | INFO | Train Epoch: 14 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.197, 500.030/s, 500.030/s/gpu LR: 9.490223e-09 Logit Scale: 100.000 Contrastive_loss: 0.28475 (0.25511) Loss: 0.28475 (0.25511)
2024-08-29,19:31:08 | INFO | Train Epoch: 14 [150100/145000.0 (104%)] Data (t): 0.110 Batch (t): 0.197, 482.878/s, 482.878/s/gpu LR: 9.487871e-09 Logit Scale: 100.000 Contrastive_loss: 0.26742 (0.25584) Loss: 0.26742 (0.25584)
2024-08-29,19:31:27 | INFO | Train Epoch: 14 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.197, 535.490/s, 535.490/s/gpu LR: 9.483245e-09 Logit Scale: 100.000 Contrastive_loss: 0.32306 (0.25957) Loss: 0.32306 (0.25957)
2024-08-29,19:31:47 | INFO | Train Epoch: 14 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.196, 524.218/s, 524.218/s/gpu LR: 9.478599e-09 Logit Scale: 100.000 Contrastive_loss: 0.23311 (0.25818) Loss: 0.23311 (0.25818)
2024-08-29,19:32:07 | INFO | Train Epoch: 14 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.196, 492.192/s, 492.192/s/gpu LR: 9.473933e-09 Logit Scale: 100.000 Contrastive_loss: 0.24045 (0.25729) Loss: 0.24045 (0.25729)
2024-08-29,19:32:26 | INFO | Train Epoch: 14 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.197, 507.048/s, 507.048/s/gpu LR: 9.469247e-09 Logit Scale: 100.000 Contrastive_loss: 0.30926 (0.25977) Loss: 0.30926 (0.25977)
2024-08-29,19:32:46 | INFO | Train Epoch: 14 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.195, 531.495/s, 531.495/s/gpu LR: 9.464542e-09 Logit Scale: 100.000 Contrastive_loss: 0.19744 (0.25694) Loss: 0.19744 (0.25694)
2024-08-29,19:33:06 | INFO | Train Epoch: 14 [210100/145000.0 (145%)] Data (t): 0.110 Batch (t): 0.198, 490.620/s, 490.620/s/gpu LR: 9.459817e-09 Logit Scale: 100.000 Contrastive_loss: 0.30602 (0.25907) Loss: 0.30602 (0.25907)
2024-08-29,19:33:25 | INFO | Train Epoch: 14 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.196, 493.801/s, 493.801/s/gpu LR: 9.455073e-09 Logit Scale: 100.000 Contrastive_loss: 0.40562 (0.26518) Loss: 0.40562 (0.26518)
2024-08-29,19:33:45 | INFO | Train Epoch: 14 [230100/145000.0 (159%)] Data (t): 0.110 Batch (t): 0.199, 496.893/s, 496.893/s/gpu LR: 9.450309e-09 Logit Scale: 100.000 Contrastive_loss: 0.34299 (0.26829) Loss: 0.34299 (0.26829)
2024-08-29,19:34:05 | INFO | Train Epoch: 14 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.197, 452.454/s, 452.454/s/gpu LR: 9.445526e-09 Logit Scale: 100.000 Contrastive_loss: 0.22337 (0.26656) Loss: 0.22337 (0.26656)
2024-08-29,19:34:25 | INFO | Train Epoch: 14 [250100/145000.0 (172%)] Data (t): 0.110 Batch (t): 0.197, 536.700/s, 536.700/s/gpu LR: 9.440723e-09 Logit Scale: 100.000 Contrastive_loss: 0.25974 (0.26631) Loss: 0.25974 (0.26631)
2024-08-29,19:34:44 | INFO | Train Epoch: 14 [260100/145000.0 (179%)] Data (t): 0.110 Batch (t): 0.198, 490.496/s, 490.496/s/gpu LR: 9.435901e-09 Logit Scale: 100.000 Contrastive_loss: 0.28083 (0.26683) Loss: 0.28083 (0.26683)
2024-08-29,19:35:04 | INFO | Train Epoch: 14 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.197, 509.632/s, 509.632/s/gpu LR: 9.431059e-09 Logit Scale: 100.000 Contrastive_loss: 0.29406 (0.26777) Loss: 0.29406 (0.26777)
2024-08-29,19:35:24 | INFO | Train Epoch: 14 [280100/145000.0 (193%)] Data (t): 0.110 Batch (t): 0.198, 490.414/s, 490.414/s/gpu LR: 9.426198e-09 Logit Scale: 100.000 Contrastive_loss: 0.21830 (0.26612) Loss: 0.21830 (0.26612)
2024-08-29,19:35:43 | INFO | Train Epoch: 14 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.196, 457.627/s, 457.627/s/gpu LR: 9.421317e-09 Logit Scale: 100.000 Contrastive_loss: 0.17092 (0.26305) Loss: 0.17092 (0.26305)
2024-08-29,19:36:03 | INFO | Train Epoch: 14 [300100/145000.0 (207%)] Data (t): 0.110 Batch (t): 0.198, 499.389/s, 499.389/s/gpu LR: 9.416417e-09 Logit Scale: 100.000 Contrastive_loss: 0.31772 (0.26475) Loss: 0.31772 (0.26475)
2024-08-29,19:36:23 | INFO | Train Epoch: 14 [310100/145000.0 (214%)] Data (t): 0.110 Batch (t): 0.198, 492.905/s, 492.905/s/gpu LR: 9.411498e-09 Logit Scale: 100.000 Contrastive_loss: 0.30261 (0.26590) Loss: 0.30261 (0.26590)
2024-08-29,19:36:43 | INFO | Train Epoch: 14 [320100/145000.0 (221%)] Data (t): 0.111 Batch (t): 0.199, 527.309/s, 527.309/s/gpu LR: 9.406559e-09 Logit Scale: 100.000 Contrastive_loss: 0.35744 (0.26859) Loss: 0.35744 (0.26859)
2024-08-29,19:37:03 | INFO | Train Epoch: 14 [330100/145000.0 (228%)] Data (t): 0.110 Batch (t): 0.197, 507.421/s, 507.421/s/gpu LR: 9.401601e-09 Logit Scale: 100.000 Contrastive_loss: 0.21664 (0.26711) Loss: 0.21664 (0.26711)
2024-08-29,19:37:22 | INFO | Train Epoch: 14 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.196, 494.732/s, 494.732/s/gpu LR: 9.396624e-09 Logit Scale: 100.000 Contrastive_loss: 0.34293 (0.26922) Loss: 0.34293 (0.26922)
2024-08-29,19:37:42 | INFO | Train Epoch: 14 [350100/145000.0 (241%)] Data (t): 0.110 Batch (t): 0.198, 531.100/s, 531.100/s/gpu LR: 9.391627e-09 Logit Scale: 100.000 Contrastive_loss: 0.38903 (0.27245) Loss: 0.38903 (0.27245)
2024-08-29,19:38:02 | INFO | Train Epoch: 14 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.194, 519.669/s, 519.669/s/gpu LR: 9.386611e-09 Logit Scale: 100.000 Contrastive_loss: 0.19887 (0.27052) Loss: 0.19887 (0.27052)
2024-08-29,19:38:21 | INFO | Train Epoch: 14 [370100/145000.0 (255%)] Data (t): 0.111 Batch (t): 0.198, 523.686/s, 523.686/s/gpu LR: 9.381576e-09 Logit Scale: 100.000 Contrastive_loss: 0.30595 (0.27143) Loss: 0.30595 (0.27143)
2024-08-29,19:38:41 | INFO | Train Epoch: 14 [380100/145000.0 (262%)] Data (t): 0.110 Batch (t): 0.198, 492.043/s, 492.043/s/gpu LR: 9.376521e-09 Logit Scale: 100.000 Contrastive_loss: 0.29377 (0.27198) Loss: 0.29377 (0.27198)
2024-08-29,19:39:01 | INFO | Train Epoch: 14 [390100/145000.0 (269%)] Data (t): 0.110 Batch (t): 0.197, 522.875/s, 522.875/s/gpu LR: 9.371448e-09 Logit Scale: 100.000 Contrastive_loss: 0.18772 (0.26993) Loss: 0.18772 (0.26993)
2024-08-29,19:39:20 | INFO | Train Epoch: 14 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.197, 531.333/s, 531.333/s/gpu LR: 9.366355e-09 Logit Scale: 100.000 Contrastive_loss: 0.27697 (0.27010) Loss: 0.27697 (0.27010)
2024-08-29,19:39:40 | INFO | Train Epoch: 14 [410100/145000.0 (283%)] Data (t): 0.110 Batch (t): 0.197, 487.352/s, 487.352/s/gpu LR: 9.361243e-09 Logit Scale: 100.000 Contrastive_loss: 0.11841 (0.26657) Loss: 0.11841 (0.26657)
2024-08-29,19:39:48 | INFO | Eval Epoch: 15 [200 / 1000]	Clip Loss: 0.506023	
2024-08-29,19:39:49 | INFO | Eval Epoch: 15 image_to_text_mean_rank: 2.5520	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6460	image_to_text_R@5: 0.9070	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.4380	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6650	text_to_image_R@5: 0.8930	text_to_image_R@10: 0.9590	clip_val_loss: 0.4600	epoch: 15.0000	num_samples: 1000.0000
2024-08-29,19:39:50 | INFO | Start epoch 15
2024-08-29,19:39:50 | INFO | Train Epoch: 15 [   100/145000.0 (0%)] Data (t): 0.025 Batch (t): 0.108, 929.543/s, 929.543/s/gpu LR: 9.490177e-09 Logit Scale: 100.000 Contrastive_loss: 0.22136 (0.22136) Loss: 0.22136 (0.22136)
2024-08-29,19:40:10 | INFO | Train Epoch: 15 [ 10100/145000.0 (7%)] Data (t): 0.112 Batch (t): 0.200, 526.275/s, 526.275/s/gpu LR: 9.485561e-09 Logit Scale: 100.000 Contrastive_loss: 0.40697 (0.31417) Loss: 0.40697 (0.31417)
2024-08-29,19:40:30 | INFO | Train Epoch: 15 [ 20100/145000.0 (14%)] Data (t): 0.111 Batch (t): 0.198, 524.200/s, 524.200/s/gpu LR: 9.480924e-09 Logit Scale: 100.000 Contrastive_loss: 0.43085 (0.35306) Loss: 0.43085 (0.35306)
2024-08-29,19:40:50 | INFO | Train Epoch: 15 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.196, 524.326/s, 524.326/s/gpu LR: 9.476268e-09 Logit Scale: 100.000 Contrastive_loss: 0.24083 (0.32500) Loss: 0.24083 (0.32500)
2024-08-29,19:41:09 | INFO | Train Epoch: 15 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.196, 525.960/s, 525.960/s/gpu LR: 9.471592e-09 Logit Scale: 100.000 Contrastive_loss: 0.16909 (0.29382) Loss: 0.16909 (0.29382)
2024-08-29,19:41:29 | INFO | Train Epoch: 15 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.196, 522.409/s, 522.409/s/gpu LR: 9.466897e-09 Logit Scale: 100.000 Contrastive_loss: 0.20719 (0.27938) Loss: 0.20719 (0.27938)
2024-08-29,19:41:48 | INFO | Train Epoch: 15 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.196, 520.547/s, 520.547/s/gpu LR: 9.462182e-09 Logit Scale: 100.000 Contrastive_loss: 0.15195 (0.26118) Loss: 0.15195 (0.26118)
2024-08-29,19:42:08 | INFO | Train Epoch: 15 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.194, 491.319/s, 491.319/s/gpu LR: 9.457448e-09 Logit Scale: 100.000 Contrastive_loss: 0.29687 (0.26564) Loss: 0.29687 (0.26564)
2024-08-29,19:42:28 | INFO | Train Epoch: 15 [ 80100/145000.0 (55%)] Data (t): 0.110 Batch (t): 0.196, 507.755/s, 507.755/s/gpu LR: 9.452694e-09 Logit Scale: 100.000 Contrastive_loss: 0.18676 (0.25688) Loss: 0.18676 (0.25688)
2024-08-29,19:42:47 | INFO | Train Epoch: 15 [ 90100/145000.0 (62%)] Data (t): 0.110 Batch (t): 0.197, 525.792/s, 525.792/s/gpu LR: 9.447920e-09 Logit Scale: 100.000 Contrastive_loss: 0.26492 (0.25768) Loss: 0.26492 (0.25768)
2024-08-29,19:43:07 | INFO | Train Epoch: 15 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.196, 503.450/s, 503.450/s/gpu LR: 9.443127e-09 Logit Scale: 100.000 Contrastive_loss: 0.19061 (0.25158) Loss: 0.19061 (0.25158)
2024-08-29,19:43:27 | INFO | Train Epoch: 15 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.197, 518.646/s, 518.646/s/gpu LR: 9.438314e-09 Logit Scale: 100.000 Contrastive_loss: 0.22426 (0.24931) Loss: 0.22426 (0.24931)
2024-08-29,19:43:46 | INFO | Train Epoch: 15 [120100/145000.0 (83%)] Data (t): 0.110 Batch (t): 0.197, 527.005/s, 527.005/s/gpu LR: 9.433482e-09 Logit Scale: 100.000 Contrastive_loss: 0.35245 (0.25724) Loss: 0.35245 (0.25724)
2024-08-29,19:44:06 | INFO | Train Epoch: 15 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.196, 495.211/s, 495.211/s/gpu LR: 9.428631e-09 Logit Scale: 100.000 Contrastive_loss: 0.19580 (0.25285) Loss: 0.19580 (0.25285)
2024-08-29,19:44:26 | INFO | Train Epoch: 15 [140100/145000.0 (97%)] Data (t): 0.111 Batch (t): 0.198, 522.799/s, 522.799/s/gpu LR: 9.423760e-09 Logit Scale: 100.000 Contrastive_loss: 0.21561 (0.25037) Loss: 0.21561 (0.25037)
2024-08-29,19:44:35 | INFO | Train Epoch: 15 [145000/145000.0 (100%)] Data (t): 0.111 Batch (t): 0.201, 502.901/s, 502.901/s/gpu LR: 9.421366e-09 Logit Scale: 100.000 Contrastive_loss: 0.28150 (0.25231) Loss: 0.28150 (0.25231)
2024-08-29,19:44:45 | INFO | Train Epoch: 15 [150100/145000.0 (104%)] Data (t): 0.110 Batch (t): 0.196, 497.024/s, 497.024/s/gpu LR: 9.418870e-09 Logit Scale: 100.000 Contrastive_loss: 0.26373 (0.25299) Loss: 0.26373 (0.25299)
2024-08-29,19:45:05 | INFO | Train Epoch: 15 [160100/145000.0 (110%)] Data (t): 0.110 Batch (t): 0.197, 523.471/s, 523.471/s/gpu LR: 9.413960e-09 Logit Scale: 100.000 Contrastive_loss: 0.31818 (0.25661) Loss: 0.31818 (0.25661)
2024-08-29,19:45:25 | INFO | Train Epoch: 15 [170100/145000.0 (117%)] Data (t): 0.111 Batch (t): 0.199, 425.513/s, 425.513/s/gpu LR: 9.409031e-09 Logit Scale: 100.000 Contrastive_loss: 0.23016 (0.25522) Loss: 0.23016 (0.25522)
2024-08-29,19:45:45 | INFO | Train Epoch: 15 [180100/145000.0 (124%)] Data (t): 0.110 Batch (t): 0.197, 515.861/s, 515.861/s/gpu LR: 9.404082e-09 Logit Scale: 100.000 Contrastive_loss: 0.23775 (0.25434) Loss: 0.23775 (0.25434)
2024-08-29,19:46:05 | INFO | Train Epoch: 15 [190100/145000.0 (131%)] Data (t): 0.110 Batch (t): 0.198, 493.207/s, 493.207/s/gpu LR: 9.399115e-09 Logit Scale: 100.000 Contrastive_loss: 0.30622 (0.25681) Loss: 0.30622 (0.25681)
2024-08-29,19:46:24 | INFO | Train Epoch: 15 [200100/145000.0 (138%)] Data (t): 0.110 Batch (t): 0.197, 492.685/s, 492.685/s/gpu LR: 9.394128e-09 Logit Scale: 100.000 Contrastive_loss: 0.19619 (0.25406) Loss: 0.19619 (0.25406)
2024-08-29,19:46:44 | INFO | Train Epoch: 15 [210100/145000.0 (145%)] Data (t): 0.111 Batch (t): 0.199, 513.257/s, 513.257/s/gpu LR: 9.389121e-09 Logit Scale: 100.000 Contrastive_loss: 0.30338 (0.25620) Loss: 0.30338 (0.25620)
2024-08-29,19:47:04 | INFO | Train Epoch: 15 [220100/145000.0 (152%)] Data (t): 0.111 Batch (t): 0.199, 522.285/s, 522.285/s/gpu LR: 9.384096e-09 Logit Scale: 100.000 Contrastive_loss: 0.40137 (0.26225) Loss: 0.40137 (0.26225)
2024-08-29,19:47:24 | INFO | Train Epoch: 15 [230100/145000.0 (159%)] Data (t): 0.110 Batch (t): 0.196, 494.836/s, 494.836/s/gpu LR: 9.379051e-09 Logit Scale: 100.000 Contrastive_loss: 0.34171 (0.26543) Loss: 0.34171 (0.26543)
2024-08-29,19:47:43 | INFO | Train Epoch: 15 [240100/145000.0 (166%)] Data (t): 0.110 Batch (t): 0.198, 500.136/s, 500.136/s/gpu LR: 9.373987e-09 Logit Scale: 100.000 Contrastive_loss: 0.22068 (0.26371) Loss: 0.22068 (0.26371)
2024-08-29,19:48:03 | INFO | Train Epoch: 15 [250100/145000.0 (172%)] Data (t): 0.111 Batch (t): 0.200, 517.791/s, 517.791/s/gpu LR: 9.368904e-09 Logit Scale: 100.000 Contrastive_loss: 0.25744 (0.26348) Loss: 0.25744 (0.26348)
2024-08-29,19:48:23 | INFO | Train Epoch: 15 [260100/145000.0 (179%)] Data (t): 0.111 Batch (t): 0.198, 492.713/s, 492.713/s/gpu LR: 9.363801e-09 Logit Scale: 100.000 Contrastive_loss: 0.27751 (0.26398) Loss: 0.27751 (0.26398)
2024-08-29,19:48:43 | INFO | Train Epoch: 15 [270100/145000.0 (186%)] Data (t): 0.110 Batch (t): 0.197, 495.191/s, 495.191/s/gpu LR: 9.358680e-09 Logit Scale: 100.000 Contrastive_loss: 0.29242 (0.26496) Loss: 0.29242 (0.26496)
2024-08-29,19:49:03 | INFO | Train Epoch: 15 [280100/145000.0 (193%)] Data (t): 0.110 Batch (t): 0.198, 525.161/s, 525.161/s/gpu LR: 9.353539e-09 Logit Scale: 100.000 Contrastive_loss: 0.21567 (0.26331) Loss: 0.21567 (0.26331)
2024-08-29,19:49:23 | INFO | Train Epoch: 15 [290100/145000.0 (200%)] Data (t): 0.110 Batch (t): 0.198, 526.630/s, 526.630/s/gpu LR: 9.348380e-09 Logit Scale: 100.000 Contrastive_loss: 0.16965 (0.26029) Loss: 0.16965 (0.26029)
2024-08-29,19:49:42 | INFO | Train Epoch: 15 [300100/145000.0 (207%)] Data (t): 0.110 Batch (t): 0.200, 485.579/s, 485.579/s/gpu LR: 9.343201e-09 Logit Scale: 100.000 Contrastive_loss: 0.31532 (0.26201) Loss: 0.31532 (0.26201)
2024-08-29,19:50:02 | INFO | Train Epoch: 15 [310100/145000.0 (214%)] Data (t): 0.110 Batch (t): 0.198, 524.525/s, 524.525/s/gpu LR: 9.338003e-09 Logit Scale: 100.000 Contrastive_loss: 0.29726 (0.26308) Loss: 0.29726 (0.26308)
2024-08-29,19:50:22 | INFO | Train Epoch: 15 [320100/145000.0 (221%)] Data (t): 0.111 Batch (t): 0.199, 505.049/s, 505.049/s/gpu LR: 9.332786e-09 Logit Scale: 100.000 Contrastive_loss: 0.35404 (0.26576) Loss: 0.35404 (0.26576)
2024-08-29,19:50:42 | INFO | Train Epoch: 15 [330100/145000.0 (228%)] Data (t): 0.111 Batch (t): 0.199, 451.157/s, 451.157/s/gpu LR: 9.327550e-09 Logit Scale: 100.000 Contrastive_loss: 0.21527 (0.26431) Loss: 0.21527 (0.26431)
2024-08-29,19:51:02 | INFO | Train Epoch: 15 [340100/145000.0 (235%)] Data (t): 0.110 Batch (t): 0.197, 481.675/s, 481.675/s/gpu LR: 9.322295e-09 Logit Scale: 100.000 Contrastive_loss: 0.33836 (0.26637) Loss: 0.33836 (0.26637)
2024-08-29,19:51:22 | INFO | Train Epoch: 15 [350100/145000.0 (241%)] Data (t): 0.111 Batch (t): 0.200, 524.117/s, 524.117/s/gpu LR: 9.317022e-09 Logit Scale: 100.000 Contrastive_loss: 0.38497 (0.26958) Loss: 0.38497 (0.26958)
2024-08-29,19:51:41 | INFO | Train Epoch: 15 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.195, 534.329/s, 534.329/s/gpu LR: 9.311729e-09 Logit Scale: 100.000 Contrastive_loss: 0.19678 (0.26766) Loss: 0.19678 (0.26766)
2024-08-29,19:52:01 | INFO | Train Epoch: 15 [370100/145000.0 (255%)] Data (t): 0.111 Batch (t): 0.200, 532.078/s, 532.078/s/gpu LR: 9.306417e-09 Logit Scale: 100.000 Contrastive_loss: 0.30397 (0.26859) Loss: 0.30397 (0.26859)
2024-08-29,19:52:21 | INFO | Train Epoch: 15 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.194, 527.213/s, 527.213/s/gpu LR: 9.301087e-09 Logit Scale: 100.000 Contrastive_loss: 0.29097 (0.26915) Loss: 0.29097 (0.26915)
2024-08-29,19:52:40 | INFO | Train Epoch: 15 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 529.754/s, 529.754/s/gpu LR: 9.295737e-09 Logit Scale: 100.000 Contrastive_loss: 0.18465 (0.26709) Loss: 0.18465 (0.26709)
2024-08-29,19:52:59 | INFO | Train Epoch: 15 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.194, 525.563/s, 525.563/s/gpu LR: 9.290369e-09 Logit Scale: 100.000 Contrastive_loss: 0.27498 (0.26728) Loss: 0.27498 (0.26728)
2024-08-29,19:53:19 | INFO | Train Epoch: 15 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.194, 522.304/s, 522.304/s/gpu LR: 9.284982e-09 Logit Scale: 100.000 Contrastive_loss: 0.11739 (0.26379) Loss: 0.11739 (0.26379)
2024-08-29,19:53:26 | INFO | Eval Epoch: 16 [200 / 1000]	Clip Loss: 0.503284	
2024-08-29,19:53:27 | INFO | Eval Epoch: 16 image_to_text_mean_rank: 2.5330	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6500	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.4370	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6680	text_to_image_R@5: 0.8950	text_to_image_R@10: 0.9590	clip_val_loss: 0.4580	epoch: 16.0000	num_samples: 1000.0000
2024-08-29,19:53:28 | INFO | Start epoch 16
2024-08-29,19:53:29 | INFO | Train Epoch: 16 [   100/145000.0 (0%)] Data (t): 0.029 Batch (t): 0.111, 896.930/s, 896.930/s/gpu LR: 9.421317e-09 Logit Scale: 100.000 Contrastive_loss: 0.21866 (0.21866) Loss: 0.21866 (0.21866)
2024-08-29,19:53:48 | INFO | Train Epoch: 16 [ 10100/145000.0 (7%)] Data (t): 0.108 Batch (t): 0.194, 493.450/s, 493.450/s/gpu LR: 9.416417e-09 Logit Scale: 100.000 Contrastive_loss: 0.40304 (0.31085) Loss: 0.40304 (0.31085)
2024-08-29,19:54:07 | INFO | Train Epoch: 16 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.195, 533.331/s, 533.331/s/gpu LR: 9.411498e-09 Logit Scale: 100.000 Contrastive_loss: 0.42752 (0.34974) Loss: 0.42752 (0.34974)
2024-08-29,19:54:27 | INFO | Train Epoch: 16 [ 30100/145000.0 (21%)] Data (t): 0.106 Batch (t): 0.194, 532.249/s, 532.249/s/gpu LR: 9.406559e-09 Logit Scale: 100.000 Contrastive_loss: 0.23811 (0.32183) Loss: 0.23811 (0.32183)
2024-08-29,19:54:47 | INFO | Train Epoch: 16 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.198, 530.839/s, 530.839/s/gpu LR: 9.401601e-09 Logit Scale: 100.000 Contrastive_loss: 0.16725 (0.29092) Loss: 0.16725 (0.29092)
2024-08-29,19:55:06 | INFO | Train Epoch: 16 [ 50100/145000.0 (35%)] Data (t): 0.107 Batch (t): 0.195, 523.583/s, 523.583/s/gpu LR: 9.396624e-09 Logit Scale: 100.000 Contrastive_loss: 0.20546 (0.27667) Loss: 0.20546 (0.27667)
2024-08-29,19:55:26 | INFO | Train Epoch: 16 [ 60100/145000.0 (41%)] Data (t): 0.106 Batch (t): 0.196, 545.381/s, 545.381/s/gpu LR: 9.391627e-09 Logit Scale: 100.000 Contrastive_loss: 0.14995 (0.25857) Loss: 0.14995 (0.25857)
2024-08-29,19:55:45 | INFO | Train Epoch: 16 [ 70100/145000.0 (48%)] Data (t): 0.107 Batch (t): 0.195, 533.554/s, 533.554/s/gpu LR: 9.386611e-09 Logit Scale: 100.000 Contrastive_loss: 0.29369 (0.26296) Loss: 0.29369 (0.26296)
2024-08-29,19:56:05 | INFO | Train Epoch: 16 [ 80100/145000.0 (55%)] Data (t): 0.105 Batch (t): 0.194, 533.587/s, 533.587/s/gpu LR: 9.381576e-09 Logit Scale: 100.000 Contrastive_loss: 0.18416 (0.25421) Loss: 0.18416 (0.25421)
2024-08-29,19:56:24 | INFO | Train Epoch: 16 [ 90100/145000.0 (62%)] Data (t): 0.107 Batch (t): 0.195, 499.453/s, 499.453/s/gpu LR: 9.376521e-09 Logit Scale: 100.000 Contrastive_loss: 0.26262 (0.25505) Loss: 0.26262 (0.25505)
2024-08-29,19:56:44 | INFO | Train Epoch: 16 [100100/145000.0 (69%)] Data (t): 0.106 Batch (t): 0.196, 492.304/s, 492.304/s/gpu LR: 9.371448e-09 Logit Scale: 100.000 Contrastive_loss: 0.18904 (0.24905) Loss: 0.18904 (0.24905)
2024-08-29,19:57:03 | INFO | Train Epoch: 16 [110100/145000.0 (76%)] Data (t): 0.104 Batch (t): 0.196, 533.321/s, 533.321/s/gpu LR: 9.366355e-09 Logit Scale: 100.000 Contrastive_loss: 0.22197 (0.24679) Loss: 0.22197 (0.24679)
2024-08-29,19:57:23 | INFO | Train Epoch: 16 [120100/145000.0 (83%)] Data (t): 0.106 Batch (t): 0.194, 496.380/s, 496.380/s/gpu LR: 9.361243e-09 Logit Scale: 100.000 Contrastive_loss: 0.34823 (0.25459) Loss: 0.34823 (0.25459)
2024-08-29,19:57:42 | INFO | Train Epoch: 16 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.194, 491.165/s, 491.165/s/gpu LR: 9.356112e-09 Logit Scale: 100.000 Contrastive_loss: 0.19355 (0.25023) Loss: 0.19355 (0.25023)
2024-08-29,19:58:02 | INFO | Train Epoch: 16 [140100/145000.0 (97%)] Data (t): 0.110 Batch (t): 0.196, 499.609/s, 499.609/s/gpu LR: 9.350962e-09 Logit Scale: 100.000 Contrastive_loss: 0.21357 (0.24779) Loss: 0.21357 (0.24779)
2024-08-29,19:58:11 | INFO | Train Epoch: 16 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.195, 503.352/s, 503.352/s/gpu LR: 9.348431e-09 Logit Scale: 100.000 Contrastive_loss: 0.27843 (0.24970) Loss: 0.27843 (0.24970)
2024-08-29,19:58:21 | INFO | Train Epoch: 16 [150100/145000.0 (104%)] Data (t): 0.111 Batch (t): 0.197, 531.800/s, 531.800/s/gpu LR: 9.345793e-09 Logit Scale: 100.000 Contrastive_loss: 0.26011 (0.25032) Loss: 0.26011 (0.25032)
2024-08-29,19:58:41 | INFO | Train Epoch: 16 [160100/145000.0 (110%)] Data (t): 0.106 Batch (t): 0.192, 521.985/s, 521.985/s/gpu LR: 9.340604e-09 Logit Scale: 100.000 Contrastive_loss: 0.31366 (0.25383) Loss: 0.31366 (0.25383)
2024-08-29,19:59:00 | INFO | Train Epoch: 16 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 521.560/s, 521.560/s/gpu LR: 9.335397e-09 Logit Scale: 100.000 Contrastive_loss: 0.22731 (0.25244) Loss: 0.22731 (0.25244)
2024-08-29,19:59:19 | INFO | Train Epoch: 16 [180100/145000.0 (124%)] Data (t): 0.107 Batch (t): 0.192, 492.163/s, 492.163/s/gpu LR: 9.330171e-09 Logit Scale: 100.000 Contrastive_loss: 0.23539 (0.25159) Loss: 0.23539 (0.25159)
2024-08-29,19:59:38 | INFO | Train Epoch: 16 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 532.884/s, 532.884/s/gpu LR: 9.324925e-09 Logit Scale: 100.000 Contrastive_loss: 0.30317 (0.25404) Loss: 0.30317 (0.25404)
2024-08-29,19:59:57 | INFO | Train Epoch: 16 [200100/145000.0 (138%)] Data (t): 0.107 Batch (t): 0.192, 524.091/s, 524.091/s/gpu LR: 9.319661e-09 Logit Scale: 100.000 Contrastive_loss: 0.19491 (0.25135) Loss: 0.19491 (0.25135)
2024-08-29,20:00:17 | INFO | Train Epoch: 16 [210100/145000.0 (145%)] Data (t): 0.107 Batch (t): 0.192, 524.976/s, 524.976/s/gpu LR: 9.314378e-09 Logit Scale: 100.000 Contrastive_loss: 0.30076 (0.25350) Loss: 0.30076 (0.25350)
2024-08-29,20:00:36 | INFO | Train Epoch: 16 [220100/145000.0 (152%)] Data (t): 0.107 Batch (t): 0.192, 495.908/s, 495.908/s/gpu LR: 9.309075e-09 Logit Scale: 100.000 Contrastive_loss: 0.39709 (0.25949) Loss: 0.39709 (0.25949)
2024-08-29,20:00:55 | INFO | Train Epoch: 16 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 493.549/s, 493.549/s/gpu LR: 9.303754e-09 Logit Scale: 100.000 Contrastive_loss: 0.34052 (0.26273) Loss: 0.34052 (0.26273)
2024-08-29,20:01:14 | INFO | Train Epoch: 16 [240100/145000.0 (166%)] Data (t): 0.107 Batch (t): 0.192, 521.453/s, 521.453/s/gpu LR: 9.298414e-09 Logit Scale: 100.000 Contrastive_loss: 0.21834 (0.26102) Loss: 0.21834 (0.26102)
2024-08-29,20:01:33 | INFO | Train Epoch: 16 [250100/145000.0 (172%)] Data (t): 0.107 Batch (t): 0.192, 502.459/s, 502.459/s/gpu LR: 9.293056e-09 Logit Scale: 100.000 Contrastive_loss: 0.25528 (0.26081) Loss: 0.25528 (0.26081)
2024-08-29,20:01:53 | INFO | Train Epoch: 16 [260100/145000.0 (179%)] Data (t): 0.107 Batch (t): 0.192, 499.052/s, 499.052/s/gpu LR: 9.287678e-09 Logit Scale: 100.000 Contrastive_loss: 0.27427 (0.26129) Loss: 0.27427 (0.26129)
2024-08-29,20:02:12 | INFO | Train Epoch: 16 [270100/145000.0 (186%)] Data (t): 0.107 Batch (t): 0.192, 520.348/s, 520.348/s/gpu LR: 9.282281e-09 Logit Scale: 100.000 Contrastive_loss: 0.29102 (0.26231) Loss: 0.29102 (0.26231)
2024-08-29,20:02:31 | INFO | Train Epoch: 16 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.192, 527.380/s, 527.380/s/gpu LR: 9.276866e-09 Logit Scale: 100.000 Contrastive_loss: 0.21309 (0.26067) Loss: 0.21309 (0.26067)
2024-08-29,20:02:50 | INFO | Train Epoch: 16 [290100/145000.0 (200%)] Data (t): 0.107 Batch (t): 0.192, 531.204/s, 531.204/s/gpu LR: 9.271432e-09 Logit Scale: 100.000 Contrastive_loss: 0.16830 (0.25769) Loss: 0.16830 (0.25769)
2024-08-29,20:03:09 | INFO | Train Epoch: 16 [300100/145000.0 (207%)] Data (t): 0.107 Batch (t): 0.192, 494.260/s, 494.260/s/gpu LR: 9.265979e-09 Logit Scale: 100.000 Contrastive_loss: 0.31310 (0.25942) Loss: 0.31310 (0.25942)
2024-08-29,20:03:28 | INFO | Train Epoch: 16 [310100/145000.0 (214%)] Data (t): 0.107 Batch (t): 0.192, 462.595/s, 462.595/s/gpu LR: 9.260508e-09 Logit Scale: 100.000 Contrastive_loss: 0.29222 (0.26042) Loss: 0.29222 (0.26042)
2024-08-29,20:03:48 | INFO | Train Epoch: 16 [320100/145000.0 (221%)] Data (t): 0.107 Batch (t): 0.192, 500.809/s, 500.809/s/gpu LR: 9.255018e-09 Logit Scale: 100.000 Contrastive_loss: 0.35059 (0.26307) Loss: 0.35059 (0.26307)
2024-08-29,20:04:07 | INFO | Train Epoch: 16 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.193, 529.453/s, 529.453/s/gpu LR: 9.249509e-09 Logit Scale: 100.000 Contrastive_loss: 0.21397 (0.26167) Loss: 0.21397 (0.26167)
2024-08-29,20:04:26 | INFO | Train Epoch: 16 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 493.725/s, 493.725/s/gpu LR: 9.243981e-09 Logit Scale: 100.000 Contrastive_loss: 0.33401 (0.26368) Loss: 0.33401 (0.26368)
2024-08-29,20:04:46 | INFO | Train Epoch: 16 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.193, 530.481/s, 530.481/s/gpu LR: 9.238435e-09 Logit Scale: 100.000 Contrastive_loss: 0.38101 (0.26685) Loss: 0.38101 (0.26685)
2024-08-29,20:05:05 | INFO | Train Epoch: 16 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 531.207/s, 531.207/s/gpu LR: 9.232871e-09 Logit Scale: 100.000 Contrastive_loss: 0.19484 (0.26495) Loss: 0.19484 (0.26495)
2024-08-29,20:05:24 | INFO | Train Epoch: 16 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 531.120/s, 531.120/s/gpu LR: 9.227287e-09 Logit Scale: 100.000 Contrastive_loss: 0.30196 (0.26590) Loss: 0.30196 (0.26590)
2024-08-29,20:05:43 | INFO | Train Epoch: 16 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 498.862/s, 498.862/s/gpu LR: 9.221686e-09 Logit Scale: 100.000 Contrastive_loss: 0.28817 (0.26646) Loss: 0.28817 (0.26646)
2024-08-29,20:06:03 | INFO | Train Epoch: 16 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.193, 489.908/s, 489.908/s/gpu LR: 9.216065e-09 Logit Scale: 100.000 Contrastive_loss: 0.18194 (0.26440) Loss: 0.18194 (0.26440)
2024-08-29,20:06:22 | INFO | Train Epoch: 16 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.193, 520.129/s, 520.129/s/gpu LR: 9.210427e-09 Logit Scale: 100.000 Contrastive_loss: 0.27285 (0.26460) Loss: 0.27285 (0.26460)
2024-08-29,20:06:41 | INFO | Train Epoch: 16 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 517.859/s, 517.859/s/gpu LR: 9.204769e-09 Logit Scale: 100.000 Contrastive_loss: 0.11646 (0.26115) Loss: 0.11646 (0.26115)
2024-08-29,20:06:49 | INFO | Eval Epoch: 17 [200 / 1000]	Clip Loss: 0.500889	
2024-08-29,20:06:50 | INFO | Eval Epoch: 17 image_to_text_mean_rank: 2.5180	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6550	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9600	text_to_image_mean_rank: 3.4430	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6690	text_to_image_R@5: 0.8960	text_to_image_R@10: 0.9590	clip_val_loss: 0.4561	epoch: 17.0000	num_samples: 1000.0000
2024-08-29,20:06:51 | INFO | Start epoch 17
2024-08-29,20:06:51 | INFO | Train Epoch: 17 [   100/145000.0 (0%)] Data (t): 0.022 Batch (t): 0.105, 950.531/s, 950.531/s/gpu LR: 9.348380e-09 Logit Scale: 100.000 Contrastive_loss: 0.21602 (0.21602) Loss: 0.21602 (0.21602)
2024-08-29,20:07:10 | INFO | Train Epoch: 17 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 519.032/s, 519.032/s/gpu LR: 9.343201e-09 Logit Scale: 100.000 Contrastive_loss: 0.39945 (0.30774) Loss: 0.39945 (0.30774)
2024-08-29,20:07:30 | INFO | Train Epoch: 17 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.194, 530.162/s, 530.162/s/gpu LR: 9.338003e-09 Logit Scale: 100.000 Contrastive_loss: 0.42427 (0.34658) Loss: 0.42427 (0.34658)
2024-08-29,20:07:49 | INFO | Train Epoch: 17 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 495.004/s, 495.004/s/gpu LR: 9.332786e-09 Logit Scale: 100.000 Contrastive_loss: 0.23522 (0.31874) Loss: 0.23522 (0.31874)
2024-08-29,20:08:08 | INFO | Train Epoch: 17 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 493.516/s, 493.516/s/gpu LR: 9.327550e-09 Logit Scale: 100.000 Contrastive_loss: 0.16567 (0.28813) Loss: 0.16567 (0.28813)
2024-08-29,20:08:28 | INFO | Train Epoch: 17 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.192, 528.811/s, 528.811/s/gpu LR: 9.322295e-09 Logit Scale: 100.000 Contrastive_loss: 0.20397 (0.27410) Loss: 0.20397 (0.27410)
2024-08-29,20:08:47 | INFO | Train Epoch: 17 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 487.124/s, 487.124/s/gpu LR: 9.317022e-09 Logit Scale: 100.000 Contrastive_loss: 0.14808 (0.25610) Loss: 0.14808 (0.25610)
2024-08-29,20:09:06 | INFO | Train Epoch: 17 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 527.862/s, 527.862/s/gpu LR: 9.311729e-09 Logit Scale: 100.000 Contrastive_loss: 0.29070 (0.26042) Loss: 0.29070 (0.26042)
2024-08-29,20:09:26 | INFO | Train Epoch: 17 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 497.379/s, 497.379/s/gpu LR: 9.306417e-09 Logit Scale: 100.000 Contrastive_loss: 0.18149 (0.25165) Loss: 0.18149 (0.25165)
2024-08-29,20:09:45 | INFO | Train Epoch: 17 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 539.421/s, 539.421/s/gpu LR: 9.301087e-09 Logit Scale: 100.000 Contrastive_loss: 0.26055 (0.25254) Loss: 0.26055 (0.25254)
2024-08-29,20:10:04 | INFO | Train Epoch: 17 [100100/145000.0 (69%)] Data (t): 0.110 Batch (t): 0.196, 527.097/s, 527.097/s/gpu LR: 9.295737e-09 Logit Scale: 100.000 Contrastive_loss: 0.18750 (0.24663) Loss: 0.18750 (0.24663)
2024-08-29,20:10:24 | INFO | Train Epoch: 17 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.196, 494.138/s, 494.138/s/gpu LR: 9.290369e-09 Logit Scale: 100.000 Contrastive_loss: 0.21955 (0.24437) Loss: 0.21955 (0.24437)
2024-08-29,20:10:43 | INFO | Train Epoch: 17 [120100/145000.0 (83%)] Data (t): 0.106 Batch (t): 0.191, 535.127/s, 535.127/s/gpu LR: 9.284982e-09 Logit Scale: 100.000 Contrastive_loss: 0.34437 (0.25207) Loss: 0.34437 (0.25207)
2024-08-29,20:11:02 | INFO | Train Epoch: 17 [130100/145000.0 (90%)] Data (t): 0.106 Batch (t): 0.192, 532.663/s, 532.663/s/gpu LR: 9.279576e-09 Logit Scale: 100.000 Contrastive_loss: 0.19133 (0.24773) Loss: 0.19133 (0.24773)
2024-08-29,20:11:22 | INFO | Train Epoch: 17 [140100/145000.0 (97%)] Data (t): 0.107 Batch (t): 0.192, 498.831/s, 498.831/s/gpu LR: 9.274151e-09 Logit Scale: 100.000 Contrastive_loss: 0.21152 (0.24531) Loss: 0.21152 (0.24531)
2024-08-29,20:11:31 | INFO | Train Epoch: 17 [145000/145000.0 (100%)] Data (t): 0.106 Batch (t): 0.191, 521.768/s, 521.768/s/gpu LR: 9.271487e-09 Logit Scale: 100.000 Contrastive_loss: 0.27544 (0.24720) Loss: 0.27544 (0.24720)
2024-08-29,20:11:41 | INFO | Train Epoch: 17 [150100/145000.0 (104%)] Data (t): 0.107 Batch (t): 0.191, 534.706/s, 534.706/s/gpu LR: 9.268708e-09 Logit Scale: 100.000 Contrastive_loss: 0.25675 (0.24776) Loss: 0.25675 (0.24776)
2024-08-29,20:12:00 | INFO | Train Epoch: 17 [160100/145000.0 (110%)] Data (t): 0.107 Batch (t): 0.191, 498.910/s, 498.910/s/gpu LR: 9.263246e-09 Logit Scale: 100.000 Contrastive_loss: 0.30915 (0.25117) Loss: 0.30915 (0.25117)
2024-08-29,20:12:19 | INFO | Train Epoch: 17 [170100/145000.0 (117%)] Data (t): 0.106 Batch (t): 0.191, 524.838/s, 524.838/s/gpu LR: 9.257765e-09 Logit Scale: 100.000 Contrastive_loss: 0.22463 (0.24977) Loss: 0.22463 (0.24977)
2024-08-29,20:12:38 | INFO | Train Epoch: 17 [180100/145000.0 (124%)] Data (t): 0.107 Batch (t): 0.191, 523.065/s, 523.065/s/gpu LR: 9.252266e-09 Logit Scale: 100.000 Contrastive_loss: 0.23313 (0.24894) Loss: 0.23313 (0.24894)
2024-08-29,20:12:57 | INFO | Train Epoch: 17 [190100/145000.0 (131%)] Data (t): 0.107 Batch (t): 0.192, 495.473/s, 495.473/s/gpu LR: 9.246747e-09 Logit Scale: 100.000 Contrastive_loss: 0.30034 (0.25139) Loss: 0.30034 (0.25139)
2024-08-29,20:13:16 | INFO | Train Epoch: 17 [200100/145000.0 (138%)] Data (t): 0.106 Batch (t): 0.191, 523.082/s, 523.082/s/gpu LR: 9.241211e-09 Logit Scale: 100.000 Contrastive_loss: 0.19370 (0.24877) Loss: 0.19370 (0.24877)
2024-08-29,20:13:35 | INFO | Train Epoch: 17 [210100/145000.0 (145%)] Data (t): 0.107 Batch (t): 0.192, 524.550/s, 524.550/s/gpu LR: 9.235655e-09 Logit Scale: 100.000 Contrastive_loss: 0.29845 (0.25093) Loss: 0.29845 (0.25093)
2024-08-29,20:13:54 | INFO | Train Epoch: 17 [220100/145000.0 (152%)] Data (t): 0.106 Batch (t): 0.191, 501.238/s, 501.238/s/gpu LR: 9.230081e-09 Logit Scale: 100.000 Contrastive_loss: 0.39308 (0.25685) Loss: 0.39308 (0.25685)
2024-08-29,20:14:14 | INFO | Train Epoch: 17 [230100/145000.0 (159%)] Data (t): 0.107 Batch (t): 0.191, 522.431/s, 522.431/s/gpu LR: 9.224489e-09 Logit Scale: 100.000 Contrastive_loss: 0.33923 (0.26014) Loss: 0.33923 (0.26014)
2024-08-29,20:14:33 | INFO | Train Epoch: 17 [240100/145000.0 (166%)] Data (t): 0.106 Batch (t): 0.191, 494.809/s, 494.809/s/gpu LR: 9.218878e-09 Logit Scale: 100.000 Contrastive_loss: 0.21607 (0.25845) Loss: 0.21607 (0.25845)
2024-08-29,20:14:52 | INFO | Train Epoch: 17 [250100/145000.0 (172%)] Data (t): 0.106 Batch (t): 0.191, 523.722/s, 523.722/s/gpu LR: 9.213248e-09 Logit Scale: 100.000 Contrastive_loss: 0.25333 (0.25826) Loss: 0.25333 (0.25826)
2024-08-29,20:15:11 | INFO | Train Epoch: 17 [260100/145000.0 (179%)] Data (t): 0.106 Batch (t): 0.191, 526.733/s, 526.733/s/gpu LR: 9.207600e-09 Logit Scale: 100.000 Contrastive_loss: 0.27149 (0.25873) Loss: 0.27149 (0.25873)
2024-08-29,20:15:30 | INFO | Train Epoch: 17 [270100/145000.0 (186%)] Data (t): 0.106 Batch (t): 0.192, 527.024/s, 527.024/s/gpu LR: 9.201934e-09 Logit Scale: 100.000 Contrastive_loss: 0.28964 (0.25980) Loss: 0.28964 (0.25980)
2024-08-29,20:15:49 | INFO | Train Epoch: 17 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 498.843/s, 498.843/s/gpu LR: 9.196249e-09 Logit Scale: 100.000 Contrastive_loss: 0.21092 (0.25817) Loss: 0.21092 (0.25817)
2024-08-29,20:16:09 | INFO | Train Epoch: 17 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.192, 536.811/s, 536.811/s/gpu LR: 9.190545e-09 Logit Scale: 100.000 Contrastive_loss: 0.16709 (0.25523) Loss: 0.16709 (0.25523)
2024-08-29,20:16:28 | INFO | Train Epoch: 17 [300100/145000.0 (207%)] Data (t): 0.107 Batch (t): 0.192, 531.951/s, 531.951/s/gpu LR: 9.184824e-09 Logit Scale: 100.000 Contrastive_loss: 0.31092 (0.25697) Loss: 0.31092 (0.25697)
2024-08-29,20:16:47 | INFO | Train Epoch: 17 [310100/145000.0 (214%)] Data (t): 0.107 Batch (t): 0.192, 508.517/s, 508.517/s/gpu LR: 9.179084e-09 Logit Scale: 100.000 Contrastive_loss: 0.28783 (0.25791) Loss: 0.28783 (0.25791)
2024-08-29,20:17:06 | INFO | Train Epoch: 17 [320100/145000.0 (221%)] Data (t): 0.107 Batch (t): 0.193, 526.481/s, 526.481/s/gpu LR: 9.173325e-09 Logit Scale: 100.000 Contrastive_loss: 0.34740 (0.26054) Loss: 0.34740 (0.26054)
2024-08-29,20:17:25 | INFO | Train Epoch: 17 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 525.928/s, 525.928/s/gpu LR: 9.167549e-09 Logit Scale: 100.000 Contrastive_loss: 0.21271 (0.25917) Loss: 0.21271 (0.25917)
2024-08-29,20:17:45 | INFO | Train Epoch: 17 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 535.420/s, 535.420/s/gpu LR: 9.161754e-09 Logit Scale: 100.000 Contrastive_loss: 0.33001 (0.26114) Loss: 0.33001 (0.26114)
2024-08-29,20:18:04 | INFO | Train Epoch: 17 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 500.943/s, 500.943/s/gpu LR: 9.155940e-09 Logit Scale: 100.000 Contrastive_loss: 0.37732 (0.26428) Loss: 0.37732 (0.26428)
2024-08-29,20:18:23 | INFO | Train Epoch: 17 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 519.159/s, 519.159/s/gpu LR: 9.150109e-09 Logit Scale: 100.000 Contrastive_loss: 0.19299 (0.26240) Loss: 0.19299 (0.26240)
2024-08-29,20:18:43 | INFO | Train Epoch: 17 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 534.091/s, 534.091/s/gpu LR: 9.144259e-09 Logit Scale: 100.000 Contrastive_loss: 0.29999 (0.26337) Loss: 0.29999 (0.26337)
2024-08-29,20:19:02 | INFO | Train Epoch: 17 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.196, 524.928/s, 524.928/s/gpu LR: 9.138391e-09 Logit Scale: 100.000 Contrastive_loss: 0.28554 (0.26392) Loss: 0.28554 (0.26392)
2024-08-29,20:19:22 | INFO | Train Epoch: 17 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.196, 525.839/s, 525.839/s/gpu LR: 9.132505e-09 Logit Scale: 100.000 Contrastive_loss: 0.17930 (0.26186) Loss: 0.17930 (0.26186)
2024-08-29,20:19:41 | INFO | Train Epoch: 17 [400100/145000.0 (276%)] Data (t): 0.107 Batch (t): 0.193, 529.516/s, 529.516/s/gpu LR: 9.126601e-09 Logit Scale: 100.000 Contrastive_loss: 0.27085 (0.26207) Loss: 0.27085 (0.26207)
2024-08-29,20:20:00 | INFO | Train Epoch: 17 [410100/145000.0 (283%)] Data (t): 0.107 Batch (t): 0.192, 503.546/s, 503.546/s/gpu LR: 9.120679e-09 Logit Scale: 100.000 Contrastive_loss: 0.11534 (0.25866) Loss: 0.11534 (0.25866)
2024-08-29,20:20:08 | INFO | Eval Epoch: 18 [200 / 1000]	Clip Loss: 0.498674	
2024-08-29,20:20:08 | INFO | Eval Epoch: 18 image_to_text_mean_rank: 2.5130	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6560	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.4340	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6720	text_to_image_R@5: 0.8970	text_to_image_R@10: 0.9590	clip_val_loss: 0.4544	epoch: 18.0000	num_samples: 1000.0000
2024-08-29,20:20:10 | INFO | Start epoch 18
2024-08-29,20:20:10 | INFO | Train Epoch: 18 [   100/145000.0 (0%)] Data (t): 0.034 Batch (t): 0.126, 794.168/s, 794.168/s/gpu LR: 9.271432e-09 Logit Scale: 100.000 Contrastive_loss: 0.21347 (0.21347) Loss: 0.21347 (0.21347)
2024-08-29,20:20:29 | INFO | Train Epoch: 18 [ 10100/145000.0 (7%)] Data (t): 0.108 Batch (t): 0.194, 526.321/s, 526.321/s/gpu LR: 9.265979e-09 Logit Scale: 100.000 Contrastive_loss: 0.39620 (0.30484) Loss: 0.39620 (0.30484)
2024-08-29,20:20:49 | INFO | Train Epoch: 18 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.195, 492.296/s, 492.296/s/gpu LR: 9.260508e-09 Logit Scale: 100.000 Contrastive_loss: 0.42120 (0.34362) Loss: 0.42120 (0.34362)
2024-08-29,20:21:08 | INFO | Train Epoch: 18 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.197, 532.036/s, 532.036/s/gpu LR: 9.255018e-09 Logit Scale: 100.000 Contrastive_loss: 0.23276 (0.31591) Loss: 0.23276 (0.31591)
2024-08-29,20:21:28 | INFO | Train Epoch: 18 [ 40100/145000.0 (28%)] Data (t): 0.106 Batch (t): 0.191, 528.046/s, 528.046/s/gpu LR: 9.249509e-09 Logit Scale: 100.000 Contrastive_loss: 0.16405 (0.28554) Loss: 0.16405 (0.28554)
2024-08-29,20:21:47 | INFO | Train Epoch: 18 [ 50100/145000.0 (35%)] Data (t): 0.106 Batch (t): 0.191, 533.776/s, 533.776/s/gpu LR: 9.243981e-09 Logit Scale: 100.000 Contrastive_loss: 0.20226 (0.27166) Loss: 0.20226 (0.27166)
2024-08-29,20:22:06 | INFO | Train Epoch: 18 [ 60100/145000.0 (41%)] Data (t): 0.107 Batch (t): 0.192, 489.700/s, 489.700/s/gpu LR: 9.238435e-09 Logit Scale: 100.000 Contrastive_loss: 0.14646 (0.25377) Loss: 0.14646 (0.25377)
2024-08-29,20:22:25 | INFO | Train Epoch: 18 [ 70100/145000.0 (48%)] Data (t): 0.107 Batch (t): 0.191, 536.840/s, 536.840/s/gpu LR: 9.232871e-09 Logit Scale: 100.000 Contrastive_loss: 0.28782 (0.25803) Loss: 0.28782 (0.25803)
2024-08-29,20:22:44 | INFO | Train Epoch: 18 [ 80100/145000.0 (55%)] Data (t): 0.106 Batch (t): 0.191, 506.577/s, 506.577/s/gpu LR: 9.227287e-09 Logit Scale: 100.000 Contrastive_loss: 0.17906 (0.24925) Loss: 0.17906 (0.24925)
2024-08-29,20:23:03 | INFO | Train Epoch: 18 [ 90100/145000.0 (62%)] Data (t): 0.106 Batch (t): 0.192, 524.177/s, 524.177/s/gpu LR: 9.221686e-09 Logit Scale: 100.000 Contrastive_loss: 0.25846 (0.25017) Loss: 0.25846 (0.25017)
2024-08-29,20:23:22 | INFO | Train Epoch: 18 [100100/145000.0 (69%)] Data (t): 0.107 Batch (t): 0.192, 514.501/s, 514.501/s/gpu LR: 9.216065e-09 Logit Scale: 100.000 Contrastive_loss: 0.18578 (0.24432) Loss: 0.18578 (0.24432)
2024-08-29,20:23:41 | INFO | Train Epoch: 18 [110100/145000.0 (76%)] Data (t): 0.107 Batch (t): 0.192, 530.800/s, 530.800/s/gpu LR: 9.210427e-09 Logit Scale: 100.000 Contrastive_loss: 0.21712 (0.24205) Loss: 0.21712 (0.24205)
2024-08-29,20:24:01 | INFO | Train Epoch: 18 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.192, 535.919/s, 535.919/s/gpu LR: 9.204769e-09 Logit Scale: 100.000 Contrastive_loss: 0.34042 (0.24962) Loss: 0.34042 (0.24962)
2024-08-29,20:24:20 | INFO | Train Epoch: 18 [130100/145000.0 (90%)] Data (t): 0.107 Batch (t): 0.192, 504.621/s, 504.621/s/gpu LR: 9.199094e-09 Logit Scale: 100.000 Contrastive_loss: 0.18932 (0.24531) Loss: 0.18932 (0.24531)
2024-08-29,20:24:39 | INFO | Train Epoch: 18 [140100/145000.0 (97%)] Data (t): 0.107 Batch (t): 0.191, 559.471/s, 559.471/s/gpu LR: 9.193399e-09 Logit Scale: 100.000 Contrastive_loss: 0.20964 (0.24293) Loss: 0.20964 (0.24293)
2024-08-29,20:24:48 | INFO | Train Epoch: 18 [145000/145000.0 (100%)] Data (t): 0.107 Batch (t): 0.192, 506.076/s, 506.076/s/gpu LR: 9.190603e-09 Logit Scale: 100.000 Contrastive_loss: 0.27286 (0.24480) Loss: 0.27286 (0.24480)
2024-08-29,20:24:58 | INFO | Train Epoch: 18 [150100/145000.0 (104%)] Data (t): 0.106 Batch (t): 0.192, 506.244/s, 506.244/s/gpu LR: 9.187687e-09 Logit Scale: 100.000 Contrastive_loss: 0.25369 (0.24533) Loss: 0.25369 (0.24533)
2024-08-29,20:25:17 | INFO | Train Epoch: 18 [160100/145000.0 (110%)] Data (t): 0.107 Batch (t): 0.191, 532.561/s, 532.561/s/gpu LR: 9.181956e-09 Logit Scale: 100.000 Contrastive_loss: 0.30490 (0.24864) Loss: 0.30490 (0.24864)
2024-08-29,20:25:37 | INFO | Train Epoch: 18 [170100/145000.0 (117%)] Data (t): 0.107 Batch (t): 0.192, 492.923/s, 492.923/s/gpu LR: 9.176207e-09 Logit Scale: 100.000 Contrastive_loss: 0.22218 (0.24724) Loss: 0.22218 (0.24724)
2024-08-29,20:25:56 | INFO | Train Epoch: 18 [180100/145000.0 (124%)] Data (t): 0.106 Batch (t): 0.191, 532.233/s, 532.233/s/gpu LR: 9.170439e-09 Logit Scale: 100.000 Contrastive_loss: 0.23112 (0.24644) Loss: 0.23112 (0.24644)
2024-08-29,20:26:15 | INFO | Train Epoch: 18 [190100/145000.0 (131%)] Data (t): 0.106 Batch (t): 0.191, 500.187/s, 500.187/s/gpu LR: 9.164653e-09 Logit Scale: 100.000 Contrastive_loss: 0.29760 (0.24887) Loss: 0.29760 (0.24887)
2024-08-29,20:26:34 | INFO | Train Epoch: 18 [200100/145000.0 (138%)] Data (t): 0.107 Batch (t): 0.192, 497.903/s, 497.903/s/gpu LR: 9.158849e-09 Logit Scale: 100.000 Contrastive_loss: 0.19251 (0.24631) Loss: 0.19251 (0.24631)
2024-08-29,20:26:53 | INFO | Train Epoch: 18 [210100/145000.0 (145%)] Data (t): 0.107 Batch (t): 0.191, 532.165/s, 532.165/s/gpu LR: 9.153027e-09 Logit Scale: 100.000 Contrastive_loss: 0.29618 (0.24848) Loss: 0.29618 (0.24848)
2024-08-29,20:27:12 | INFO | Train Epoch: 18 [220100/145000.0 (152%)] Data (t): 0.106 Batch (t): 0.191, 530.380/s, 530.380/s/gpu LR: 9.147186e-09 Logit Scale: 100.000 Contrastive_loss: 0.38914 (0.25434) Loss: 0.38914 (0.25434)
2024-08-29,20:27:31 | INFO | Train Epoch: 18 [230100/145000.0 (159%)] Data (t): 0.107 Batch (t): 0.191, 521.961/s, 521.961/s/gpu LR: 9.141327e-09 Logit Scale: 100.000 Contrastive_loss: 0.33783 (0.25768) Loss: 0.33783 (0.25768)
2024-08-29,20:27:50 | INFO | Train Epoch: 18 [240100/145000.0 (166%)] Data (t): 0.106 Batch (t): 0.191, 489.581/s, 489.581/s/gpu LR: 9.135451e-09 Logit Scale: 100.000 Contrastive_loss: 0.21380 (0.25599) Loss: 0.21380 (0.25599)
2024-08-29,20:28:10 | INFO | Train Epoch: 18 [250100/145000.0 (172%)] Data (t): 0.106 Batch (t): 0.191, 526.513/s, 526.513/s/gpu LR: 9.129555e-09 Logit Scale: 100.000 Contrastive_loss: 0.25142 (0.25582) Loss: 0.25142 (0.25582)
2024-08-29,20:28:29 | INFO | Train Epoch: 18 [260100/145000.0 (179%)] Data (t): 0.106 Batch (t): 0.191, 535.210/s, 535.210/s/gpu LR: 9.123642e-09 Logit Scale: 100.000 Contrastive_loss: 0.26856 (0.25628) Loss: 0.26856 (0.25628)
2024-08-29,20:28:48 | INFO | Train Epoch: 18 [270100/145000.0 (186%)] Data (t): 0.107 Batch (t): 0.192, 531.423/s, 531.423/s/gpu LR: 9.117711e-09 Logit Scale: 100.000 Contrastive_loss: 0.28819 (0.25738) Loss: 0.28819 (0.25738)
2024-08-29,20:29:07 | INFO | Train Epoch: 18 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 531.798/s, 531.798/s/gpu LR: 9.111762e-09 Logit Scale: 100.000 Contrastive_loss: 0.20877 (0.25576) Loss: 0.20877 (0.25576)
2024-08-29,20:29:26 | INFO | Train Epoch: 18 [290100/145000.0 (200%)] Data (t): 0.107 Batch (t): 0.192, 542.384/s, 542.384/s/gpu LR: 9.105794e-09 Logit Scale: 100.000 Contrastive_loss: 0.16597 (0.25286) Loss: 0.16597 (0.25286)
2024-08-29,20:29:46 | INFO | Train Epoch: 18 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 534.594/s, 534.594/s/gpu LR: 9.099809e-09 Logit Scale: 100.000 Contrastive_loss: 0.30894 (0.25461) Loss: 0.30894 (0.25461)
2024-08-29,20:30:05 | INFO | Train Epoch: 18 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 509.509/s, 509.509/s/gpu LR: 9.093805e-09 Logit Scale: 100.000 Contrastive_loss: 0.28415 (0.25551) Loss: 0.28415 (0.25551)
2024-08-29,20:30:24 | INFO | Train Epoch: 18 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 525.930/s, 525.930/s/gpu LR: 9.087784e-09 Logit Scale: 100.000 Contrastive_loss: 0.34409 (0.25811) Loss: 0.34409 (0.25811)
2024-08-29,20:30:43 | INFO | Train Epoch: 18 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 528.967/s, 528.967/s/gpu LR: 9.081745e-09 Logit Scale: 100.000 Contrastive_loss: 0.21161 (0.25679) Loss: 0.21161 (0.25679)
2024-08-29,20:31:03 | INFO | Train Epoch: 18 [340100/145000.0 (235%)] Data (t): 0.107 Batch (t): 0.192, 532.002/s, 532.002/s/gpu LR: 9.075688e-09 Logit Scale: 100.000 Contrastive_loss: 0.32603 (0.25871) Loss: 0.32603 (0.25871)
2024-08-29,20:31:22 | INFO | Train Epoch: 18 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 490.651/s, 490.651/s/gpu LR: 9.069613e-09 Logit Scale: 100.000 Contrastive_loss: 0.37356 (0.26181) Loss: 0.37356 (0.26181)
2024-08-29,20:31:41 | INFO | Train Epoch: 18 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.192, 520.757/s, 520.757/s/gpu LR: 9.063520e-09 Logit Scale: 100.000 Contrastive_loss: 0.19115 (0.25995) Loss: 0.19115 (0.25995)
2024-08-29,20:32:00 | INFO | Train Epoch: 18 [370100/145000.0 (255%)] Data (t): 0.107 Batch (t): 0.192, 523.738/s, 523.738/s/gpu LR: 9.057409e-09 Logit Scale: 100.000 Contrastive_loss: 0.29816 (0.26093) Loss: 0.29816 (0.26093)
2024-08-29,20:32:20 | INFO | Train Epoch: 18 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 525.456/s, 525.456/s/gpu LR: 9.051281e-09 Logit Scale: 100.000 Contrastive_loss: 0.28302 (0.26149) Loss: 0.28302 (0.26149)
2024-08-29,20:32:39 | INFO | Train Epoch: 18 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 496.296/s, 496.296/s/gpu LR: 9.045134e-09 Logit Scale: 100.000 Contrastive_loss: 0.17692 (0.25942) Loss: 0.17692 (0.25942)
2024-08-29,20:32:58 | INFO | Train Epoch: 18 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 499.408/s, 499.408/s/gpu LR: 9.038970e-09 Logit Scale: 100.000 Contrastive_loss: 0.26881 (0.25965) Loss: 0.26881 (0.25965)
2024-08-29,20:33:18 | INFO | Train Epoch: 18 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 529.727/s, 529.727/s/gpu LR: 9.032788e-09 Logit Scale: 100.000 Contrastive_loss: 0.11439 (0.25627) Loss: 0.11439 (0.25627)
2024-08-29,20:33:25 | INFO | Eval Epoch: 19 [200 / 1000]	Clip Loss: 0.496931	
2024-08-29,20:33:26 | INFO | Eval Epoch: 19 image_to_text_mean_rank: 2.5020	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6560	image_to_text_R@5: 0.9100	image_to_text_R@10: 0.9620	text_to_image_mean_rank: 3.4460	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6720	text_to_image_R@5: 0.8970	text_to_image_R@10: 0.9590	clip_val_loss: 0.4530	epoch: 19.0000	num_samples: 1000.0000
2024-08-29,20:33:27 | INFO | Start epoch 19
2024-08-29,20:33:27 | INFO | Train Epoch: 19 [   100/145000.0 (0%)] Data (t): 0.020 Batch (t): 0.108, 925.903/s, 925.903/s/gpu LR: 9.190545e-09 Logit Scale: 100.000 Contrastive_loss: 0.21102 (0.21102) Loss: 0.21102 (0.21102)
2024-08-29,20:33:47 | INFO | Train Epoch: 19 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 503.274/s, 503.274/s/gpu LR: 9.184824e-09 Logit Scale: 100.000 Contrastive_loss: 0.39303 (0.30202) Loss: 0.39303 (0.30202)
2024-08-29,20:34:06 | INFO | Train Epoch: 19 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 524.173/s, 524.173/s/gpu LR: 9.179084e-09 Logit Scale: 100.000 Contrastive_loss: 0.41811 (0.34072) Loss: 0.41811 (0.34072)
2024-08-29,20:34:25 | INFO | Train Epoch: 19 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.192, 526.489/s, 526.489/s/gpu LR: 9.173325e-09 Logit Scale: 100.000 Contrastive_loss: 0.23039 (0.31314) Loss: 0.23039 (0.31314)
2024-08-29,20:34:44 | INFO | Train Epoch: 19 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 528.765/s, 528.765/s/gpu LR: 9.167549e-09 Logit Scale: 100.000 Contrastive_loss: 0.16249 (0.28301) Loss: 0.16249 (0.28301)
2024-08-29,20:35:04 | INFO | Train Epoch: 19 [ 50100/145000.0 (35%)] Data (t): 0.107 Batch (t): 0.192, 564.643/s, 564.643/s/gpu LR: 9.161754e-09 Logit Scale: 100.000 Contrastive_loss: 0.20078 (0.26930) Loss: 0.20078 (0.26930)
2024-08-29,20:35:23 | INFO | Train Epoch: 19 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 541.024/s, 541.024/s/gpu LR: 9.155940e-09 Logit Scale: 100.000 Contrastive_loss: 0.14483 (0.25152) Loss: 0.14483 (0.25152)
2024-08-29,20:35:42 | INFO | Train Epoch: 19 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 498.172/s, 498.172/s/gpu LR: 9.150109e-09 Logit Scale: 100.000 Contrastive_loss: 0.28503 (0.25571) Loss: 0.28503 (0.25571)
2024-08-29,20:36:02 | INFO | Train Epoch: 19 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 495.543/s, 495.543/s/gpu LR: 9.144259e-09 Logit Scale: 100.000 Contrastive_loss: 0.17668 (0.24693) Loss: 0.17668 (0.24693)
2024-08-29,20:36:21 | INFO | Train Epoch: 19 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 533.162/s, 533.162/s/gpu LR: 9.138391e-09 Logit Scale: 100.000 Contrastive_loss: 0.25643 (0.24788) Loss: 0.25643 (0.24788)
2024-08-29,20:36:40 | INFO | Train Epoch: 19 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.192, 518.833/s, 518.833/s/gpu LR: 9.132505e-09 Logit Scale: 100.000 Contrastive_loss: 0.18431 (0.24210) Loss: 0.18431 (0.24210)
2024-08-29,20:36:59 | INFO | Train Epoch: 19 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 507.362/s, 507.362/s/gpu LR: 9.126601e-09 Logit Scale: 100.000 Contrastive_loss: 0.21500 (0.23984) Loss: 0.21500 (0.23984)
2024-08-29,20:37:19 | INFO | Train Epoch: 19 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 521.985/s, 521.985/s/gpu LR: 9.120679e-09 Logit Scale: 100.000 Contrastive_loss: 0.33677 (0.24730) Loss: 0.33677 (0.24730)
2024-08-29,20:37:38 | INFO | Train Epoch: 19 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 494.467/s, 494.467/s/gpu LR: 9.114739e-09 Logit Scale: 100.000 Contrastive_loss: 0.18728 (0.24301) Loss: 0.18728 (0.24301)
2024-08-29,20:37:57 | INFO | Train Epoch: 19 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 502.162/s, 502.162/s/gpu LR: 9.108780e-09 Logit Scale: 100.000 Contrastive_loss: 0.20790 (0.24067) Loss: 0.20790 (0.24067)
2024-08-29,20:38:07 | INFO | Train Epoch: 19 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.192, 537.455/s, 537.455/s/gpu LR: 9.105854e-09 Logit Scale: 100.000 Contrastive_loss: 0.27031 (0.24252) Loss: 0.27031 (0.24252)
2024-08-29,20:38:17 | INFO | Train Epoch: 19 [150100/145000.0 (104%)] Data (t): 0.107 Batch (t): 0.192, 530.217/s, 530.217/s/gpu LR: 9.102804e-09 Logit Scale: 100.000 Contrastive_loss: 0.25070 (0.24300) Loss: 0.25070 (0.24300)
2024-08-29,20:38:36 | INFO | Train Epoch: 19 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 503.713/s, 503.713/s/gpu LR: 9.096809e-09 Logit Scale: 100.000 Contrastive_loss: 0.30100 (0.24623) Loss: 0.30100 (0.24623)
2024-08-29,20:38:55 | INFO | Train Epoch: 19 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 531.466/s, 531.466/s/gpu LR: 9.090797e-09 Logit Scale: 100.000 Contrastive_loss: 0.21972 (0.24483) Loss: 0.21972 (0.24483)
2024-08-29,20:39:14 | INFO | Train Epoch: 19 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 491.173/s, 491.173/s/gpu LR: 9.084767e-09 Logit Scale: 100.000 Contrastive_loss: 0.22910 (0.24404) Loss: 0.22910 (0.24404)
2024-08-29,20:39:34 | INFO | Train Epoch: 19 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 494.093/s, 494.093/s/gpu LR: 9.078719e-09 Logit Scale: 100.000 Contrastive_loss: 0.29490 (0.24647) Loss: 0.29490 (0.24647)
2024-08-29,20:39:53 | INFO | Train Epoch: 19 [200100/145000.0 (138%)] Data (t): 0.107 Batch (t): 0.192, 488.565/s, 488.565/s/gpu LR: 9.072652e-09 Logit Scale: 100.000 Contrastive_loss: 0.19139 (0.24396) Loss: 0.19139 (0.24396)
2024-08-29,20:40:12 | INFO | Train Epoch: 19 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 501.889/s, 501.889/s/gpu LR: 9.066568e-09 Logit Scale: 100.000 Contrastive_loss: 0.29397 (0.24614) Loss: 0.29397 (0.24614)
2024-08-29,20:40:31 | INFO | Train Epoch: 19 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 531.438/s, 531.438/s/gpu LR: 9.060467e-09 Logit Scale: 100.000 Contrastive_loss: 0.38531 (0.25194) Loss: 0.38531 (0.25194)
2024-08-29,20:40:51 | INFO | Train Epoch: 19 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 491.509/s, 491.509/s/gpu LR: 9.054347e-09 Logit Scale: 100.000 Contrastive_loss: 0.33656 (0.25532) Loss: 0.33656 (0.25532)
2024-08-29,20:41:10 | INFO | Train Epoch: 19 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 492.755/s, 492.755/s/gpu LR: 9.048210e-09 Logit Scale: 100.000 Contrastive_loss: 0.21162 (0.25364) Loss: 0.21162 (0.25364)
2024-08-29,20:41:29 | INFO | Train Epoch: 19 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 497.636/s, 497.636/s/gpu LR: 9.042054e-09 Logit Scale: 100.000 Contrastive_loss: 0.24975 (0.25350) Loss: 0.24975 (0.25350)
2024-08-29,20:41:48 | INFO | Train Epoch: 19 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 504.398/s, 504.398/s/gpu LR: 9.035882e-09 Logit Scale: 100.000 Contrastive_loss: 0.26595 (0.25394) Loss: 0.26595 (0.25394)
2024-08-29,20:42:08 | INFO | Train Epoch: 19 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 536.024/s, 536.024/s/gpu LR: 9.029691e-09 Logit Scale: 100.000 Contrastive_loss: 0.28689 (0.25508) Loss: 0.28689 (0.25508)
2024-08-29,20:42:27 | INFO | Train Epoch: 19 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 526.097/s, 526.097/s/gpu LR: 9.023483e-09 Logit Scale: 100.000 Contrastive_loss: 0.20658 (0.25346) Loss: 0.20658 (0.25346)
2024-08-29,20:42:46 | INFO | Train Epoch: 19 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 502.928/s, 502.928/s/gpu LR: 9.017257e-09 Logit Scale: 100.000 Contrastive_loss: 0.16487 (0.25060) Loss: 0.16487 (0.25060)
2024-08-29,20:43:06 | INFO | Train Epoch: 19 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 529.120/s, 529.120/s/gpu LR: 9.011013e-09 Logit Scale: 100.000 Contrastive_loss: 0.30701 (0.25236) Loss: 0.30701 (0.25236)
2024-08-29,20:43:25 | INFO | Train Epoch: 19 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 501.981/s, 501.981/s/gpu LR: 9.004752e-09 Logit Scale: 100.000 Contrastive_loss: 0.28077 (0.25323) Loss: 0.28077 (0.25323)
2024-08-29,20:43:44 | INFO | Train Epoch: 19 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 563.230/s, 563.230/s/gpu LR: 8.998473e-09 Logit Scale: 100.000 Contrastive_loss: 0.34100 (0.25581) Loss: 0.34100 (0.25581)
2024-08-29,20:44:04 | INFO | Train Epoch: 19 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.193, 500.974/s, 500.974/s/gpu LR: 8.992177e-09 Logit Scale: 100.000 Contrastive_loss: 0.21049 (0.25451) Loss: 0.21049 (0.25451)
2024-08-29,20:44:23 | INFO | Train Epoch: 19 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.192, 487.465/s, 487.465/s/gpu LR: 8.985863e-09 Logit Scale: 100.000 Contrastive_loss: 0.32223 (0.25639) Loss: 0.32223 (0.25639)
2024-08-29,20:44:42 | INFO | Train Epoch: 19 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 526.470/s, 526.470/s/gpu LR: 8.979532e-09 Logit Scale: 100.000 Contrastive_loss: 0.37000 (0.25946) Loss: 0.37000 (0.25946)
2024-08-29,20:45:01 | INFO | Train Epoch: 19 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 534.506/s, 534.506/s/gpu LR: 8.973183e-09 Logit Scale: 100.000 Contrastive_loss: 0.18960 (0.25763) Loss: 0.18960 (0.25763)
2024-08-29,20:45:21 | INFO | Train Epoch: 19 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 497.088/s, 497.088/s/gpu LR: 8.966817e-09 Logit Scale: 100.000 Contrastive_loss: 0.29665 (0.25863) Loss: 0.29665 (0.25863)
2024-08-29,20:45:40 | INFO | Train Epoch: 19 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 532.473/s, 532.473/s/gpu LR: 8.960434e-09 Logit Scale: 100.000 Contrastive_loss: 0.28050 (0.25917) Loss: 0.28050 (0.25917)
2024-08-29,20:45:59 | INFO | Train Epoch: 19 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.193, 518.392/s, 518.392/s/gpu LR: 8.954033e-09 Logit Scale: 100.000 Contrastive_loss: 0.17448 (0.25711) Loss: 0.17448 (0.25711)
2024-08-29,20:46:18 | INFO | Train Epoch: 19 [400100/145000.0 (276%)] Data (t): 0.107 Batch (t): 0.192, 531.085/s, 531.085/s/gpu LR: 8.947614e-09 Logit Scale: 100.000 Contrastive_loss: 0.26697 (0.25734) Loss: 0.26697 (0.25734)
2024-08-29,20:46:38 | INFO | Train Epoch: 19 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 531.897/s, 531.897/s/gpu LR: 8.941179e-09 Logit Scale: 100.000 Contrastive_loss: 0.11337 (0.25399) Loss: 0.11337 (0.25399)
2024-08-29,20:46:45 | INFO | Eval Epoch: 20 [200 / 1000]	Clip Loss: 0.495043	
2024-08-29,20:46:46 | INFO | Eval Epoch: 20 image_to_text_mean_rank: 2.4970	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6570	image_to_text_R@5: 0.9100	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.4470	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6740	text_to_image_R@5: 0.8970	text_to_image_R@10: 0.9590	clip_val_loss: 0.4515	epoch: 20.0000	num_samples: 1000.0000
2024-08-29,20:46:47 | INFO | Start epoch 20
2024-08-29,20:46:48 | INFO | Train Epoch: 20 [   100/145000.0 (0%)] Data (t): 0.035 Batch (t): 0.117, 854.373/s, 854.373/s/gpu LR: 9.105794e-09 Logit Scale: 100.000 Contrastive_loss: 0.20891 (0.20891) Loss: 0.20891 (0.20891)
2024-08-29,20:47:07 | INFO | Train Epoch: 20 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 525.557/s, 525.557/s/gpu LR: 9.099809e-09 Logit Scale: 100.000 Contrastive_loss: 0.39009 (0.29950) Loss: 0.39009 (0.29950)
2024-08-29,20:47:26 | INFO | Train Epoch: 20 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 499.006/s, 499.006/s/gpu LR: 9.093805e-09 Logit Scale: 100.000 Contrastive_loss: 0.41518 (0.33806) Loss: 0.41518 (0.33806)
2024-08-29,20:47:46 | INFO | Train Epoch: 20 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 505.563/s, 505.563/s/gpu LR: 9.087784e-09 Logit Scale: 100.000 Contrastive_loss: 0.22827 (0.31061) Loss: 0.22827 (0.31061)
2024-08-29,20:48:05 | INFO | Train Epoch: 20 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 500.269/s, 500.269/s/gpu LR: 9.081745e-09 Logit Scale: 100.000 Contrastive_loss: 0.16104 (0.28070) Loss: 0.16104 (0.28070)
2024-08-29,20:48:24 | INFO | Train Epoch: 20 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 496.671/s, 496.671/s/gpu LR: 9.075688e-09 Logit Scale: 100.000 Contrastive_loss: 0.19904 (0.26709) Loss: 0.19904 (0.26709)
2024-08-29,20:48:43 | INFO | Train Epoch: 20 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 501.731/s, 501.731/s/gpu LR: 9.069613e-09 Logit Scale: 100.000 Contrastive_loss: 0.14330 (0.24940) Loss: 0.14330 (0.24940)
2024-08-29,20:49:03 | INFO | Train Epoch: 20 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.194, 531.771/s, 531.771/s/gpu LR: 9.063520e-09 Logit Scale: 100.000 Contrastive_loss: 0.28217 (0.25350) Loss: 0.28217 (0.25350)
2024-08-29,20:49:22 | INFO | Train Epoch: 20 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 534.853/s, 534.853/s/gpu LR: 9.057409e-09 Logit Scale: 100.000 Contrastive_loss: 0.17426 (0.24470) Loss: 0.17426 (0.24470)
2024-08-29,20:49:41 | INFO | Train Epoch: 20 [ 90100/145000.0 (62%)] Data (t): 0.107 Batch (t): 0.193, 500.864/s, 500.864/s/gpu LR: 9.051281e-09 Logit Scale: 100.000 Contrastive_loss: 0.25436 (0.24566) Loss: 0.25436 (0.24566)
2024-08-29,20:50:01 | INFO | Train Epoch: 20 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 527.908/s, 527.908/s/gpu LR: 9.045134e-09 Logit Scale: 100.000 Contrastive_loss: 0.18281 (0.23995) Loss: 0.18281 (0.23995)
2024-08-29,20:50:20 | INFO | Train Epoch: 20 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 517.677/s, 517.677/s/gpu LR: 9.038970e-09 Logit Scale: 100.000 Contrastive_loss: 0.21280 (0.23769) Loss: 0.21280 (0.23769)
2024-08-29,20:50:39 | INFO | Train Epoch: 20 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 501.276/s, 501.276/s/gpu LR: 9.032788e-09 Logit Scale: 100.000 Contrastive_loss: 0.33310 (0.24503) Loss: 0.33310 (0.24503)
2024-08-29,20:50:59 | INFO | Train Epoch: 20 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 526.562/s, 526.562/s/gpu LR: 9.026589e-09 Logit Scale: 100.000 Contrastive_loss: 0.18533 (0.24076) Loss: 0.18533 (0.24076)
2024-08-29,20:51:18 | INFO | Train Epoch: 20 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 528.403/s, 528.403/s/gpu LR: 9.020372e-09 Logit Scale: 100.000 Contrastive_loss: 0.20625 (0.23846) Loss: 0.20625 (0.23846)
2024-08-29,20:51:27 | INFO | Train Epoch: 20 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 536.569/s, 536.569/s/gpu LR: 9.017319e-09 Logit Scale: 100.000 Contrastive_loss: 0.26808 (0.24031) Loss: 0.26808 (0.24031)
2024-08-29,20:51:37 | INFO | Train Epoch: 20 [150100/145000.0 (104%)] Data (t): 0.107 Batch (t): 0.193, 507.503/s, 507.503/s/gpu LR: 9.014137e-09 Logit Scale: 100.000 Contrastive_loss: 0.24781 (0.24075) Loss: 0.24781 (0.24075)
2024-08-29,20:51:56 | INFO | Train Epoch: 20 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 537.886/s, 537.886/s/gpu LR: 9.007885e-09 Logit Scale: 100.000 Contrastive_loss: 0.29709 (0.24388) Loss: 0.29709 (0.24388)
2024-08-29,20:52:16 | INFO | Train Epoch: 20 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 531.937/s, 531.937/s/gpu LR: 9.001615e-09 Logit Scale: 100.000 Contrastive_loss: 0.21741 (0.24249) Loss: 0.21741 (0.24249)
2024-08-29,20:52:35 | INFO | Train Epoch: 20 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 534.802/s, 534.802/s/gpu LR: 8.995327e-09 Logit Scale: 100.000 Contrastive_loss: 0.22728 (0.24173) Loss: 0.22728 (0.24173)
2024-08-29,20:52:54 | INFO | Train Epoch: 20 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 508.312/s, 508.312/s/gpu LR: 8.989022e-09 Logit Scale: 100.000 Contrastive_loss: 0.29233 (0.24414) Loss: 0.29233 (0.24414)
2024-08-29,20:53:14 | INFO | Train Epoch: 20 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.194, 496.080/s, 496.080/s/gpu LR: 8.982700e-09 Logit Scale: 100.000 Contrastive_loss: 0.19039 (0.24170) Loss: 0.19039 (0.24170)
2024-08-29,20:53:33 | INFO | Train Epoch: 20 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 518.245/s, 518.245/s/gpu LR: 8.976360e-09 Logit Scale: 100.000 Contrastive_loss: 0.29202 (0.24388) Loss: 0.29202 (0.24388)
2024-08-29,20:53:52 | INFO | Train Epoch: 20 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.194, 518.661/s, 518.661/s/gpu LR: 8.970002e-09 Logit Scale: 100.000 Contrastive_loss: 0.38176 (0.24963) Loss: 0.38176 (0.24963)
2024-08-29,20:54:12 | INFO | Train Epoch: 20 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 497.744/s, 497.744/s/gpu LR: 8.963628e-09 Logit Scale: 100.000 Contrastive_loss: 0.33536 (0.25306) Loss: 0.33536 (0.25306)
2024-08-29,20:54:31 | INFO | Train Epoch: 20 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 510.825/s, 510.825/s/gpu LR: 8.957235e-09 Logit Scale: 100.000 Contrastive_loss: 0.20941 (0.25138) Loss: 0.20941 (0.25138)
2024-08-29,20:54:50 | INFO | Train Epoch: 20 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 534.533/s, 534.533/s/gpu LR: 8.950826e-09 Logit Scale: 100.000 Contrastive_loss: 0.24804 (0.25126) Loss: 0.24804 (0.25126)
2024-08-29,20:55:09 | INFO | Train Epoch: 20 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 523.330/s, 523.330/s/gpu LR: 8.944399e-09 Logit Scale: 100.000 Contrastive_loss: 0.26340 (0.25169) Loss: 0.26340 (0.25169)
2024-08-29,20:55:29 | INFO | Train Epoch: 20 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 526.180/s, 526.180/s/gpu LR: 8.937955e-09 Logit Scale: 100.000 Contrastive_loss: 0.28552 (0.25286) Loss: 0.28552 (0.25286)
2024-08-29,20:55:48 | INFO | Train Epoch: 20 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 520.923/s, 520.923/s/gpu LR: 8.931493e-09 Logit Scale: 100.000 Contrastive_loss: 0.20457 (0.25125) Loss: 0.20457 (0.25125)
2024-08-29,20:56:07 | INFO | Train Epoch: 20 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 500.829/s, 500.829/s/gpu LR: 8.925014e-09 Logit Scale: 100.000 Contrastive_loss: 0.16375 (0.24842) Loss: 0.16375 (0.24842)
2024-08-29,20:56:27 | INFO | Train Epoch: 20 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 503.855/s, 503.855/s/gpu LR: 8.918518e-09 Logit Scale: 100.000 Contrastive_loss: 0.30501 (0.25019) Loss: 0.30501 (0.25019)
2024-08-29,20:56:46 | INFO | Train Epoch: 20 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 532.667/s, 532.667/s/gpu LR: 8.912005e-09 Logit Scale: 100.000 Contrastive_loss: 0.27779 (0.25103) Loss: 0.27779 (0.25103)
2024-08-29,20:57:05 | INFO | Train Epoch: 20 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 527.119/s, 527.119/s/gpu LR: 8.905475e-09 Logit Scale: 100.000 Contrastive_loss: 0.33798 (0.25359) Loss: 0.33798 (0.25359)
2024-08-29,20:57:24 | INFO | Train Epoch: 20 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 534.608/s, 534.608/s/gpu LR: 8.898928e-09 Logit Scale: 100.000 Contrastive_loss: 0.20928 (0.25232) Loss: 0.20928 (0.25232)
2024-08-29,20:57:44 | INFO | Train Epoch: 20 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.192, 527.160/s, 527.160/s/gpu LR: 8.892363e-09 Logit Scale: 100.000 Contrastive_loss: 0.31867 (0.25416) Loss: 0.31867 (0.25416)
2024-08-29,20:58:03 | INFO | Train Epoch: 20 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 499.637/s, 499.637/s/gpu LR: 8.885781e-09 Logit Scale: 100.000 Contrastive_loss: 0.36639 (0.25720) Loss: 0.36639 (0.25720)
2024-08-29,20:58:22 | INFO | Train Epoch: 20 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 523.264/s, 523.264/s/gpu LR: 8.879183e-09 Logit Scale: 100.000 Contrastive_loss: 0.18798 (0.25537) Loss: 0.18798 (0.25537)
2024-08-29,20:58:42 | INFO | Train Epoch: 20 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 533.428/s, 533.428/s/gpu LR: 8.872567e-09 Logit Scale: 100.000 Contrastive_loss: 0.29503 (0.25639) Loss: 0.29503 (0.25639)
2024-08-29,20:59:01 | INFO | Train Epoch: 20 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 501.538/s, 501.538/s/gpu LR: 8.865934e-09 Logit Scale: 100.000 Contrastive_loss: 0.27824 (0.25694) Loss: 0.27824 (0.25694)
2024-08-29,20:59:20 | INFO | Train Epoch: 20 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 502.080/s, 502.080/s/gpu LR: 8.859285e-09 Logit Scale: 100.000 Contrastive_loss: 0.17236 (0.25487) Loss: 0.17236 (0.25487)
2024-08-29,20:59:39 | INFO | Train Epoch: 20 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.192, 527.763/s, 527.763/s/gpu LR: 8.852618e-09 Logit Scale: 100.000 Contrastive_loss: 0.26488 (0.25511) Loss: 0.26488 (0.25511)
2024-08-29,20:59:59 | INFO | Train Epoch: 20 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 533.902/s, 533.902/s/gpu LR: 8.845935e-09 Logit Scale: 100.000 Contrastive_loss: 0.11235 (0.25179) Loss: 0.11235 (0.25179)
2024-08-29,21:00:06 | INFO | Eval Epoch: 21 [200 / 1000]	Clip Loss: 0.493675	
2024-08-29,21:00:07 | INFO | Eval Epoch: 21 image_to_text_mean_rank: 2.4870	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6570	image_to_text_R@5: 0.9100	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.4650	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6720	text_to_image_R@5: 0.8980	text_to_image_R@10: 0.9580	clip_val_loss: 0.4503	epoch: 21.0000	num_samples: 1000.0000
2024-08-29,21:00:08 | INFO | Start epoch 21
2024-08-29,21:00:08 | INFO | Train Epoch: 21 [   100/145000.0 (0%)] Data (t): 0.024 Batch (t): 0.111, 902.756/s, 902.756/s/gpu LR: 9.017257e-09 Logit Scale: 100.000 Contrastive_loss: 0.20675 (0.20675) Loss: 0.20675 (0.20675)
2024-08-29,21:00:28 | INFO | Train Epoch: 21 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 529.888/s, 529.888/s/gpu LR: 9.011013e-09 Logit Scale: 100.000 Contrastive_loss: 0.38725 (0.29700) Loss: 0.38725 (0.29700)
2024-08-29,21:00:47 | INFO | Train Epoch: 21 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 538.592/s, 538.592/s/gpu LR: 9.004752e-09 Logit Scale: 100.000 Contrastive_loss: 0.41237 (0.33546) Loss: 0.41237 (0.33546)
2024-08-29,21:01:06 | INFO | Train Epoch: 21 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 493.689/s, 493.689/s/gpu LR: 8.998473e-09 Logit Scale: 100.000 Contrastive_loss: 0.22591 (0.30807) Loss: 0.22591 (0.30807)
2024-08-29,21:01:26 | INFO | Train Epoch: 21 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 499.609/s, 499.609/s/gpu LR: 8.992177e-09 Logit Scale: 100.000 Contrastive_loss: 0.15961 (0.27838) Loss: 0.15961 (0.27838)
2024-08-29,21:01:45 | INFO | Train Epoch: 21 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.192, 520.622/s, 520.622/s/gpu LR: 8.985863e-09 Logit Scale: 100.000 Contrastive_loss: 0.19752 (0.26490) Loss: 0.19752 (0.26490)
2024-08-29,21:02:04 | INFO | Train Epoch: 21 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 530.808/s, 530.808/s/gpu LR: 8.979532e-09 Logit Scale: 100.000 Contrastive_loss: 0.14194 (0.24734) Loss: 0.14194 (0.24734)
2024-08-29,21:02:23 | INFO | Train Epoch: 21 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.192, 486.247/s, 486.247/s/gpu LR: 8.973183e-09 Logit Scale: 100.000 Contrastive_loss: 0.27937 (0.25134) Loss: 0.27937 (0.25134)
2024-08-29,21:02:43 | INFO | Train Epoch: 21 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.192, 523.019/s, 523.019/s/gpu LR: 8.966817e-09 Logit Scale: 100.000 Contrastive_loss: 0.17224 (0.24255) Loss: 0.17224 (0.24255)
2024-08-29,21:03:02 | INFO | Train Epoch: 21 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.194, 497.487/s, 497.487/s/gpu LR: 8.960434e-09 Logit Scale: 100.000 Contrastive_loss: 0.25245 (0.24354) Loss: 0.25245 (0.24354)
2024-08-29,21:03:21 | INFO | Train Epoch: 21 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 536.063/s, 536.063/s/gpu LR: 8.954033e-09 Logit Scale: 100.000 Contrastive_loss: 0.18138 (0.23789) Loss: 0.18138 (0.23789)
2024-08-29,21:03:41 | INFO | Train Epoch: 21 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.192, 488.279/s, 488.279/s/gpu LR: 8.947614e-09 Logit Scale: 100.000 Contrastive_loss: 0.21080 (0.23563) Loss: 0.21080 (0.23563)
2024-08-29,21:04:00 | INFO | Train Epoch: 21 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.192, 526.852/s, 526.852/s/gpu LR: 8.941179e-09 Logit Scale: 100.000 Contrastive_loss: 0.32985 (0.24288) Loss: 0.32985 (0.24288)
2024-08-29,21:04:19 | INFO | Train Epoch: 21 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 526.073/s, 526.073/s/gpu LR: 8.934726e-09 Logit Scale: 100.000 Contrastive_loss: 0.18344 (0.23864) Loss: 0.18344 (0.23864)
2024-08-29,21:04:38 | INFO | Train Epoch: 21 [140100/145000.0 (97%)] Data (t): 0.107 Batch (t): 0.192, 503.662/s, 503.662/s/gpu LR: 8.928256e-09 Logit Scale: 100.000 Contrastive_loss: 0.20474 (0.23638) Loss: 0.20474 (0.23638)
2024-08-29,21:04:48 | INFO | Train Epoch: 21 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.192, 531.103/s, 531.103/s/gpu LR: 8.925079e-09 Logit Scale: 100.000 Contrastive_loss: 0.26591 (0.23822) Loss: 0.26591 (0.23822)
2024-08-29,21:04:57 | INFO | Train Epoch: 21 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 529.059/s, 529.059/s/gpu LR: 8.921769e-09 Logit Scale: 100.000 Contrastive_loss: 0.24511 (0.23863) Loss: 0.24511 (0.23863)
2024-08-29,21:05:17 | INFO | Train Epoch: 21 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 526.741/s, 526.741/s/gpu LR: 8.915264e-09 Logit Scale: 100.000 Contrastive_loss: 0.29333 (0.24167) Loss: 0.29333 (0.24167)
2024-08-29,21:05:36 | INFO | Train Epoch: 21 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 525.216/s, 525.216/s/gpu LR: 8.908742e-09 Logit Scale: 100.000 Contrastive_loss: 0.21497 (0.24026) Loss: 0.21497 (0.24026)
2024-08-29,21:05:55 | INFO | Train Epoch: 21 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 523.365/s, 523.365/s/gpu LR: 8.902203e-09 Logit Scale: 100.000 Contrastive_loss: 0.22558 (0.23953) Loss: 0.22558 (0.23953)
2024-08-29,21:06:15 | INFO | Train Epoch: 21 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 529.793/s, 529.793/s/gpu LR: 8.895647e-09 Logit Scale: 100.000 Contrastive_loss: 0.28986 (0.24192) Loss: 0.28986 (0.24192)
2024-08-29,21:06:34 | INFO | Train Epoch: 21 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.194, 524.537/s, 524.537/s/gpu LR: 8.889074e-09 Logit Scale: 100.000 Contrastive_loss: 0.18937 (0.23953) Loss: 0.18937 (0.23953)
2024-08-29,21:06:53 | INFO | Train Epoch: 21 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 523.165/s, 523.165/s/gpu LR: 8.882484e-09 Logit Scale: 100.000 Contrastive_loss: 0.28992 (0.24172) Loss: 0.28992 (0.24172)
2024-08-29,21:07:13 | INFO | Train Epoch: 21 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 494.808/s, 494.808/s/gpu LR: 8.875877e-09 Logit Scale: 100.000 Contrastive_loss: 0.37796 (0.24740) Loss: 0.37796 (0.24740)
2024-08-29,21:07:32 | INFO | Train Epoch: 21 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 499.994/s, 499.994/s/gpu LR: 8.869253e-09 Logit Scale: 100.000 Contrastive_loss: 0.33404 (0.25087) Loss: 0.33404 (0.25087)
2024-08-29,21:07:51 | INFO | Train Epoch: 21 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.192, 503.520/s, 503.520/s/gpu LR: 8.862612e-09 Logit Scale: 100.000 Contrastive_loss: 0.20731 (0.24919) Loss: 0.20731 (0.24919)
2024-08-29,21:08:10 | INFO | Train Epoch: 21 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 494.204/s, 494.204/s/gpu LR: 8.855954e-09 Logit Scale: 100.000 Contrastive_loss: 0.24623 (0.24908) Loss: 0.24623 (0.24908)
2024-08-29,21:08:30 | INFO | Train Epoch: 21 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.192, 537.170/s, 537.170/s/gpu LR: 8.849279e-09 Logit Scale: 100.000 Contrastive_loss: 0.26097 (0.24951) Loss: 0.26097 (0.24951)
2024-08-29,21:08:49 | INFO | Train Epoch: 21 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 492.840/s, 492.840/s/gpu LR: 8.842587e-09 Logit Scale: 100.000 Contrastive_loss: 0.28415 (0.25070) Loss: 0.28415 (0.25070)
2024-08-29,21:09:08 | INFO | Train Epoch: 21 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 490.441/s, 490.441/s/gpu LR: 8.835878e-09 Logit Scale: 100.000 Contrastive_loss: 0.20267 (0.24910) Loss: 0.20267 (0.24910)
2024-08-29,21:09:27 | INFO | Train Epoch: 21 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.192, 493.977/s, 493.977/s/gpu LR: 8.829152e-09 Logit Scale: 100.000 Contrastive_loss: 0.16270 (0.24631) Loss: 0.16270 (0.24631)
2024-08-29,21:09:46 | INFO | Train Epoch: 21 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 503.797/s, 503.797/s/gpu LR: 8.822410e-09 Logit Scale: 100.000 Contrastive_loss: 0.30325 (0.24809) Loss: 0.30325 (0.24809)
2024-08-29,21:10:06 | INFO | Train Epoch: 21 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 494.786/s, 494.786/s/gpu LR: 8.815651e-09 Logit Scale: 100.000 Contrastive_loss: 0.27496 (0.24891) Loss: 0.27496 (0.24891)
2024-08-29,21:10:25 | INFO | Train Epoch: 21 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 495.903/s, 495.903/s/gpu LR: 8.808875e-09 Logit Scale: 100.000 Contrastive_loss: 0.33513 (0.25144) Loss: 0.33513 (0.25144)
2024-08-29,21:10:44 | INFO | Train Epoch: 21 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 534.314/s, 534.314/s/gpu LR: 8.802083e-09 Logit Scale: 100.000 Contrastive_loss: 0.20825 (0.25021) Loss: 0.20825 (0.25021)
2024-08-29,21:11:04 | INFO | Train Epoch: 21 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 494.894/s, 494.894/s/gpu LR: 8.795273e-09 Logit Scale: 100.000 Contrastive_loss: 0.31527 (0.25202) Loss: 0.31527 (0.25202)
2024-08-29,21:11:23 | INFO | Train Epoch: 21 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 534.855/s, 534.855/s/gpu LR: 8.788447e-09 Logit Scale: 100.000 Contrastive_loss: 0.36259 (0.25500) Loss: 0.36259 (0.25500)
2024-08-29,21:11:42 | INFO | Train Epoch: 21 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.192, 523.580/s, 523.580/s/gpu LR: 8.781605e-09 Logit Scale: 100.000 Contrastive_loss: 0.18650 (0.25320) Loss: 0.18650 (0.25320)
2024-08-29,21:12:01 | INFO | Train Epoch: 21 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 528.430/s, 528.430/s/gpu LR: 8.774746e-09 Logit Scale: 100.000 Contrastive_loss: 0.29334 (0.25423) Loss: 0.29334 (0.25423)
2024-08-29,21:12:21 | INFO | Train Epoch: 21 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 503.491/s, 503.491/s/gpu LR: 8.767870e-09 Logit Scale: 100.000 Contrastive_loss: 0.27599 (0.25477) Loss: 0.27599 (0.25477)
2024-08-29,21:12:40 | INFO | Train Epoch: 21 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 494.074/s, 494.074/s/gpu LR: 8.760978e-09 Logit Scale: 100.000 Contrastive_loss: 0.17024 (0.25271) Loss: 0.17024 (0.25271)
2024-08-29,21:12:59 | INFO | Train Epoch: 21 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 504.618/s, 504.618/s/gpu LR: 8.754069e-09 Logit Scale: 100.000 Contrastive_loss: 0.26296 (0.25296) Loss: 0.26296 (0.25296)
2024-08-29,21:13:19 | INFO | Train Epoch: 21 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 527.187/s, 527.187/s/gpu LR: 8.747144e-09 Logit Scale: 100.000 Contrastive_loss: 0.11139 (0.24966) Loss: 0.11139 (0.24966)
2024-08-29,21:13:26 | INFO | Eval Epoch: 22 [200 / 1000]	Clip Loss: 0.492387	
2024-08-29,21:13:27 | INFO | Eval Epoch: 22 image_to_text_mean_rank: 2.4790	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6570	image_to_text_R@5: 0.9100	image_to_text_R@10: 0.9620	text_to_image_mean_rank: 3.4750	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6720	text_to_image_R@5: 0.8990	text_to_image_R@10: 0.9580	clip_val_loss: 0.4491	epoch: 22.0000	num_samples: 1000.0000
2024-08-29,21:13:28 | INFO | Start epoch 22
2024-08-29,21:13:28 | INFO | Train Epoch: 22 [   100/145000.0 (0%)] Data (t): 0.023 Batch (t): 0.109, 915.873/s, 915.873/s/gpu LR: 8.925014e-09 Logit Scale: 100.000 Contrastive_loss: 0.20466 (0.20466) Loss: 0.20466 (0.20466)
2024-08-29,21:13:48 | INFO | Train Epoch: 22 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 490.733/s, 490.733/s/gpu LR: 8.918518e-09 Logit Scale: 100.000 Contrastive_loss: 0.38467 (0.29467) Loss: 0.38467 (0.29467)
2024-08-29,21:14:07 | INFO | Train Epoch: 22 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 498.774/s, 498.774/s/gpu LR: 8.912005e-09 Logit Scale: 100.000 Contrastive_loss: 0.40956 (0.33296) Loss: 0.40956 (0.33296)
2024-08-29,21:14:26 | INFO | Train Epoch: 22 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 499.252/s, 499.252/s/gpu LR: 8.905475e-09 Logit Scale: 100.000 Contrastive_loss: 0.22374 (0.30566) Loss: 0.22374 (0.30566)
2024-08-29,21:14:46 | INFO | Train Epoch: 22 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 500.665/s, 500.665/s/gpu LR: 8.898928e-09 Logit Scale: 100.000 Contrastive_loss: 0.15825 (0.27618) Loss: 0.15825 (0.27618)
2024-08-29,21:15:05 | INFO | Train Epoch: 22 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 502.915/s, 502.915/s/gpu LR: 8.892363e-09 Logit Scale: 100.000 Contrastive_loss: 0.19585 (0.26279) Loss: 0.19585 (0.26279)
2024-08-29,21:15:24 | INFO | Train Epoch: 22 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 500.970/s, 500.970/s/gpu LR: 8.885781e-09 Logit Scale: 100.000 Contrastive_loss: 0.14068 (0.24534) Loss: 0.14068 (0.24534)
2024-08-29,21:15:43 | INFO | Train Epoch: 22 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 503.591/s, 503.591/s/gpu LR: 8.879183e-09 Logit Scale: 100.000 Contrastive_loss: 0.27688 (0.24929) Loss: 0.27688 (0.24929)
2024-08-29,21:16:03 | INFO | Train Epoch: 22 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 492.989/s, 492.989/s/gpu LR: 8.872567e-09 Logit Scale: 100.000 Contrastive_loss: 0.17010 (0.24049) Loss: 0.17010 (0.24049)
2024-08-29,21:16:22 | INFO | Train Epoch: 22 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 530.021/s, 530.021/s/gpu LR: 8.865934e-09 Logit Scale: 100.000 Contrastive_loss: 0.25060 (0.24150) Loss: 0.25060 (0.24150)
2024-08-29,21:16:41 | INFO | Train Epoch: 22 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 494.769/s, 494.769/s/gpu LR: 8.859285e-09 Logit Scale: 100.000 Contrastive_loss: 0.18003 (0.23591) Loss: 0.18003 (0.23591)
2024-08-29,21:17:01 | INFO | Train Epoch: 22 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.192, 491.868/s, 491.868/s/gpu LR: 8.852618e-09 Logit Scale: 100.000 Contrastive_loss: 0.20887 (0.23366) Loss: 0.20887 (0.23366)
2024-08-29,21:17:20 | INFO | Train Epoch: 22 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 533.331/s, 533.331/s/gpu LR: 8.845935e-09 Logit Scale: 100.000 Contrastive_loss: 0.32658 (0.24081) Loss: 0.32658 (0.24081)
2024-08-29,21:17:39 | INFO | Train Epoch: 22 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 542.559/s, 542.559/s/gpu LR: 8.839235e-09 Logit Scale: 100.000 Contrastive_loss: 0.18162 (0.23658) Loss: 0.18162 (0.23658)
2024-08-29,21:17:59 | INFO | Train Epoch: 22 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.194, 498.318/s, 498.318/s/gpu LR: 8.832517e-09 Logit Scale: 100.000 Contrastive_loss: 0.20316 (0.23435) Loss: 0.20316 (0.23435)
2024-08-29,21:18:08 | INFO | Train Epoch: 22 [145000/145000.0 (100%)] Data (t): 0.107 Batch (t): 0.192, 523.746/s, 523.746/s/gpu LR: 8.829220e-09 Logit Scale: 100.000 Contrastive_loss: 0.26373 (0.23619) Loss: 0.26373 (0.23619)
2024-08-29,21:18:18 | INFO | Train Epoch: 22 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 528.727/s, 528.727/s/gpu LR: 8.825783e-09 Logit Scale: 100.000 Contrastive_loss: 0.24250 (0.23656) Loss: 0.24250 (0.23656)
2024-08-29,21:18:37 | INFO | Train Epoch: 22 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 532.673/s, 532.673/s/gpu LR: 8.819033e-09 Logit Scale: 100.000 Contrastive_loss: 0.28982 (0.23952) Loss: 0.28982 (0.23952)
2024-08-29,21:18:56 | INFO | Train Epoch: 22 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 534.713/s, 534.713/s/gpu LR: 8.812265e-09 Logit Scale: 100.000 Contrastive_loss: 0.21279 (0.23811) Loss: 0.21279 (0.23811)
2024-08-29,21:19:16 | INFO | Train Epoch: 22 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 533.804/s, 533.804/s/gpu LR: 8.805481e-09 Logit Scale: 100.000 Contrastive_loss: 0.22403 (0.23741) Loss: 0.22403 (0.23741)
2024-08-29,21:19:35 | INFO | Train Epoch: 22 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 516.712/s, 516.712/s/gpu LR: 8.798680e-09 Logit Scale: 100.000 Contrastive_loss: 0.28757 (0.23980) Loss: 0.28757 (0.23980)
2024-08-29,21:19:54 | INFO | Train Epoch: 22 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 493.431/s, 493.431/s/gpu LR: 8.791862e-09 Logit Scale: 100.000 Contrastive_loss: 0.18830 (0.23745) Loss: 0.18830 (0.23745)
2024-08-29,21:20:14 | INFO | Train Epoch: 22 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 499.901/s, 499.901/s/gpu LR: 8.785028e-09 Logit Scale: 100.000 Contrastive_loss: 0.28785 (0.23965) Loss: 0.28785 (0.23965)
2024-08-29,21:20:33 | INFO | Train Epoch: 22 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 533.239/s, 533.239/s/gpu LR: 8.778177e-09 Logit Scale: 100.000 Contrastive_loss: 0.37449 (0.24526) Loss: 0.37449 (0.24526)
2024-08-29,21:20:52 | INFO | Train Epoch: 22 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 496.527/s, 496.527/s/gpu LR: 8.771310e-09 Logit Scale: 100.000 Contrastive_loss: 0.33273 (0.24876) Loss: 0.33273 (0.24876)
2024-08-29,21:21:12 | INFO | Train Epoch: 22 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 536.216/s, 536.216/s/gpu LR: 8.764426e-09 Logit Scale: 100.000 Contrastive_loss: 0.20532 (0.24709) Loss: 0.20532 (0.24709)
2024-08-29,21:21:31 | INFO | Train Epoch: 22 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 530.824/s, 530.824/s/gpu LR: 8.757526e-09 Logit Scale: 100.000 Contrastive_loss: 0.24497 (0.24701) Loss: 0.24497 (0.24701)
2024-08-29,21:21:50 | INFO | Train Epoch: 22 [260100/145000.0 (179%)] Data (t): 0.107 Batch (t): 0.192, 508.879/s, 508.879/s/gpu LR: 8.750609e-09 Logit Scale: 100.000 Contrastive_loss: 0.25867 (0.24743) Loss: 0.25867 (0.24743)
2024-08-29,21:22:09 | INFO | Train Epoch: 22 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 491.150/s, 491.150/s/gpu LR: 8.743675e-09 Logit Scale: 100.000 Contrastive_loss: 0.28282 (0.24865) Loss: 0.28282 (0.24865)
2024-08-29,21:22:28 | INFO | Train Epoch: 22 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 537.638/s, 537.638/s/gpu LR: 8.736725e-09 Logit Scale: 100.000 Contrastive_loss: 0.20089 (0.24706) Loss: 0.20089 (0.24706)
2024-08-29,21:22:48 | INFO | Train Epoch: 22 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.192, 493.845/s, 493.845/s/gpu LR: 8.729759e-09 Logit Scale: 100.000 Contrastive_loss: 0.16177 (0.24431) Loss: 0.16177 (0.24431)
2024-08-29,21:23:07 | INFO | Train Epoch: 22 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 517.833/s, 517.833/s/gpu LR: 8.722777e-09 Logit Scale: 100.000 Contrastive_loss: 0.30130 (0.24609) Loss: 0.30130 (0.24609)
2024-08-29,21:23:26 | INFO | Train Epoch: 22 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 533.474/s, 533.474/s/gpu LR: 8.715778e-09 Logit Scale: 100.000 Contrastive_loss: 0.27255 (0.24689) Loss: 0.27255 (0.24689)
2024-08-29,21:23:46 | INFO | Train Epoch: 22 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.192, 531.573/s, 531.573/s/gpu LR: 8.708763e-09 Logit Scale: 100.000 Contrastive_loss: 0.33247 (0.24941) Loss: 0.33247 (0.24941)
2024-08-29,21:24:05 | INFO | Train Epoch: 22 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.193, 494.232/s, 494.232/s/gpu LR: 8.701731e-09 Logit Scale: 100.000 Contrastive_loss: 0.20720 (0.24820) Loss: 0.20720 (0.24820)
2024-08-29,21:24:24 | INFO | Train Epoch: 22 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 526.283/s, 526.283/s/gpu LR: 8.694684e-09 Logit Scale: 100.000 Contrastive_loss: 0.31207 (0.24997) Loss: 0.31207 (0.24997)
2024-08-29,21:24:43 | INFO | Train Epoch: 22 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.193, 535.305/s, 535.305/s/gpu LR: 8.687620e-09 Logit Scale: 100.000 Contrastive_loss: 0.35903 (0.25292) Loss: 0.35903 (0.25292)
2024-08-29,21:25:03 | INFO | Train Epoch: 22 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 532.977/s, 532.977/s/gpu LR: 8.680540e-09 Logit Scale: 100.000 Contrastive_loss: 0.18511 (0.25114) Loss: 0.18511 (0.25114)
2024-08-29,21:25:22 | INFO | Train Epoch: 22 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 499.893/s, 499.893/s/gpu LR: 8.673443e-09 Logit Scale: 100.000 Contrastive_loss: 0.29191 (0.25218) Loss: 0.29191 (0.25218)
2024-08-29,21:25:41 | INFO | Train Epoch: 22 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 503.453/s, 503.453/s/gpu LR: 8.666331e-09 Logit Scale: 100.000 Contrastive_loss: 0.27389 (0.25273) Loss: 0.27389 (0.25273)
2024-08-29,21:26:01 | INFO | Train Epoch: 22 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 520.603/s, 520.603/s/gpu LR: 8.659203e-09 Logit Scale: 100.000 Contrastive_loss: 0.16814 (0.25066) Loss: 0.16814 (0.25066)
2024-08-29,21:26:20 | INFO | Train Epoch: 22 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.192, 507.585/s, 507.585/s/gpu LR: 8.652058e-09 Logit Scale: 100.000 Contrastive_loss: 0.26108 (0.25091) Loss: 0.26108 (0.25091)
2024-08-29,21:26:39 | INFO | Train Epoch: 22 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.192, 530.306/s, 530.306/s/gpu LR: 8.644898e-09 Logit Scale: 100.000 Contrastive_loss: 0.11038 (0.24764) Loss: 0.11038 (0.24764)
2024-08-29,21:26:46 | INFO | Eval Epoch: 23 [200 / 1000]	Clip Loss: 0.491113	
2024-08-29,21:26:47 | INFO | Eval Epoch: 23 image_to_text_mean_rank: 2.4740	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6580	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.4890	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6730	text_to_image_R@5: 0.8980	text_to_image_R@10: 0.9580	clip_val_loss: 0.4481	epoch: 23.0000	num_samples: 1000.0000
2024-08-29,21:26:48 | INFO | Start epoch 23
2024-08-29,21:26:49 | INFO | Train Epoch: 23 [   100/145000.0 (0%)] Data (t): 0.025 Batch (t): 0.111, 901.968/s, 901.968/s/gpu LR: 8.829152e-09 Logit Scale: 100.000 Contrastive_loss: 0.20283 (0.20283) Loss: 0.20283 (0.20283)
2024-08-29,21:27:08 | INFO | Train Epoch: 23 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 499.541/s, 499.541/s/gpu LR: 8.822410e-09 Logit Scale: 100.000 Contrastive_loss: 0.38223 (0.29253) Loss: 0.38223 (0.29253)
2024-08-29,21:27:27 | INFO | Train Epoch: 23 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 525.659/s, 525.659/s/gpu LR: 8.815651e-09 Logit Scale: 100.000 Contrastive_loss: 0.40670 (0.33058) Loss: 0.40670 (0.33058)
2024-08-29,21:27:47 | INFO | Train Epoch: 23 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.194, 489.538/s, 489.538/s/gpu LR: 8.808875e-09 Logit Scale: 100.000 Contrastive_loss: 0.22187 (0.30341) Loss: 0.22187 (0.30341)
2024-08-29,21:28:06 | INFO | Train Epoch: 23 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 522.959/s, 522.959/s/gpu LR: 8.802083e-09 Logit Scale: 100.000 Contrastive_loss: 0.15702 (0.27413) Loss: 0.15702 (0.27413)
2024-08-29,21:28:25 | INFO | Train Epoch: 23 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 536.810/s, 536.810/s/gpu LR: 8.795273e-09 Logit Scale: 100.000 Contrastive_loss: 0.19430 (0.26082) Loss: 0.19430 (0.26082)
2024-08-29,21:28:45 | INFO | Train Epoch: 23 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 504.234/s, 504.234/s/gpu LR: 8.788447e-09 Logit Scale: 100.000 Contrastive_loss: 0.13934 (0.24347) Loss: 0.13934 (0.24347)
2024-08-29,21:29:04 | INFO | Train Epoch: 23 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 502.753/s, 502.753/s/gpu LR: 8.781605e-09 Logit Scale: 100.000 Contrastive_loss: 0.27424 (0.24732) Loss: 0.27424 (0.24732)
2024-08-29,21:29:23 | INFO | Train Epoch: 23 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 490.291/s, 490.291/s/gpu LR: 8.774746e-09 Logit Scale: 100.000 Contrastive_loss: 0.16803 (0.23851) Loss: 0.16803 (0.23851)
2024-08-29,21:29:42 | INFO | Train Epoch: 23 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 523.341/s, 523.341/s/gpu LR: 8.767870e-09 Logit Scale: 100.000 Contrastive_loss: 0.24871 (0.23953) Loss: 0.24871 (0.23953)
2024-08-29,21:30:02 | INFO | Train Epoch: 23 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 522.427/s, 522.427/s/gpu LR: 8.760978e-09 Logit Scale: 100.000 Contrastive_loss: 0.17850 (0.23398) Loss: 0.17850 (0.23398)
2024-08-29,21:30:21 | INFO | Train Epoch: 23 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 494.650/s, 494.650/s/gpu LR: 8.754069e-09 Logit Scale: 100.000 Contrastive_loss: 0.20690 (0.23172) Loss: 0.20690 (0.23172)
2024-08-29,21:30:40 | INFO | Train Epoch: 23 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.192, 539.126/s, 539.126/s/gpu LR: 8.747144e-09 Logit Scale: 100.000 Contrastive_loss: 0.32364 (0.23879) Loss: 0.32364 (0.23879)
2024-08-29,21:31:00 | INFO | Train Epoch: 23 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 493.532/s, 493.532/s/gpu LR: 8.740202e-09 Logit Scale: 100.000 Contrastive_loss: 0.18006 (0.23460) Loss: 0.18006 (0.23460)
2024-08-29,21:31:19 | INFO | Train Epoch: 23 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.192, 542.696/s, 542.696/s/gpu LR: 8.733244e-09 Logit Scale: 100.000 Contrastive_loss: 0.20183 (0.23241) Loss: 0.20183 (0.23241)
2024-08-29,21:31:28 | INFO | Train Epoch: 23 [145000/145000.0 (100%)] Data (t): 0.110 Batch (t): 0.194, 509.971/s, 509.971/s/gpu LR: 8.729829e-09 Logit Scale: 100.000 Contrastive_loss: 0.26180 (0.23425) Loss: 0.26180 (0.23425)
2024-08-29,21:31:38 | INFO | Train Epoch: 23 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 524.025/s, 524.025/s/gpu LR: 8.726270e-09 Logit Scale: 100.000 Contrastive_loss: 0.23997 (0.23459) Loss: 0.23997 (0.23459)
2024-08-29,21:31:57 | INFO | Train Epoch: 23 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 525.435/s, 525.435/s/gpu LR: 8.719279e-09 Logit Scale: 100.000 Contrastive_loss: 0.28654 (0.23747) Loss: 0.28654 (0.23747)
2024-08-29,21:32:17 | INFO | Train Epoch: 23 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 531.978/s, 531.978/s/gpu LR: 8.712272e-09 Logit Scale: 100.000 Contrastive_loss: 0.21074 (0.23607) Loss: 0.21074 (0.23607)
2024-08-29,21:32:36 | INFO | Train Epoch: 23 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 529.506/s, 529.506/s/gpu LR: 8.705249e-09 Logit Scale: 100.000 Contrastive_loss: 0.22253 (0.23539) Loss: 0.22253 (0.23539)
2024-08-29,21:32:55 | INFO | Train Epoch: 23 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 503.155/s, 503.155/s/gpu LR: 8.698209e-09 Logit Scale: 100.000 Contrastive_loss: 0.28515 (0.23776) Loss: 0.28515 (0.23776)
2024-08-29,21:33:15 | INFO | Train Epoch: 23 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 543.485/s, 543.485/s/gpu LR: 8.691154e-09 Logit Scale: 100.000 Contrastive_loss: 0.18716 (0.23546) Loss: 0.18716 (0.23546)
2024-08-29,21:33:34 | INFO | Train Epoch: 23 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 498.444/s, 498.444/s/gpu LR: 8.684082e-09 Logit Scale: 100.000 Contrastive_loss: 0.28613 (0.23766) Loss: 0.28613 (0.23766)
2024-08-29,21:33:53 | INFO | Train Epoch: 23 [220100/145000.0 (152%)] Data (t): 0.107 Batch (t): 0.192, 534.112/s, 534.112/s/gpu LR: 8.676993e-09 Logit Scale: 100.000 Contrastive_loss: 0.37112 (0.24322) Loss: 0.37112 (0.24322)
2024-08-29,21:34:12 | INFO | Train Epoch: 23 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 503.874/s, 503.874/s/gpu LR: 8.669889e-09 Logit Scale: 100.000 Contrastive_loss: 0.33132 (0.24675) Loss: 0.33132 (0.24675)
2024-08-29,21:34:32 | INFO | Train Epoch: 23 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 531.276/s, 531.276/s/gpu LR: 8.662769e-09 Logit Scale: 100.000 Contrastive_loss: 0.20338 (0.24508) Loss: 0.20338 (0.24508)
2024-08-29,21:34:51 | INFO | Train Epoch: 23 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 537.013/s, 537.013/s/gpu LR: 8.655632e-09 Logit Scale: 100.000 Contrastive_loss: 0.24326 (0.24501) Loss: 0.24326 (0.24501)
2024-08-29,21:35:10 | INFO | Train Epoch: 23 [260100/145000.0 (179%)] Data (t): 0.107 Batch (t): 0.193, 494.131/s, 494.131/s/gpu LR: 8.648480e-09 Logit Scale: 100.000 Contrastive_loss: 0.25646 (0.24542) Loss: 0.25646 (0.24542)
2024-08-29,21:35:29 | INFO | Train Epoch: 23 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 505.949/s, 505.949/s/gpu LR: 8.641311e-09 Logit Scale: 100.000 Contrastive_loss: 0.28164 (0.24667) Loss: 0.28164 (0.24667)
2024-08-29,21:35:49 | INFO | Train Epoch: 23 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.194, 506.226/s, 506.226/s/gpu LR: 8.634127e-09 Logit Scale: 100.000 Contrastive_loss: 0.19907 (0.24508) Loss: 0.19907 (0.24508)
2024-08-29,21:36:08 | INFO | Train Epoch: 23 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 497.524/s, 497.524/s/gpu LR: 8.626927e-09 Logit Scale: 100.000 Contrastive_loss: 0.16092 (0.24237) Loss: 0.16092 (0.24237)
2024-08-29,21:36:27 | INFO | Train Epoch: 23 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 505.639/s, 505.639/s/gpu LR: 8.619710e-09 Logit Scale: 100.000 Contrastive_loss: 0.29957 (0.24415) Loss: 0.29957 (0.24415)
2024-08-29,21:36:47 | INFO | Train Epoch: 23 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 507.731/s, 507.731/s/gpu LR: 8.612478e-09 Logit Scale: 100.000 Contrastive_loss: 0.27049 (0.24495) Loss: 0.27049 (0.24495)
2024-08-29,21:37:06 | INFO | Train Epoch: 23 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 531.376/s, 531.376/s/gpu LR: 8.605230e-09 Logit Scale: 100.000 Contrastive_loss: 0.32989 (0.24745) Loss: 0.32989 (0.24745)
2024-08-29,21:37:25 | INFO | Train Epoch: 23 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 497.307/s, 497.307/s/gpu LR: 8.597966e-09 Logit Scale: 100.000 Contrastive_loss: 0.20621 (0.24627) Loss: 0.20621 (0.24627)
2024-08-29,21:37:45 | INFO | Train Epoch: 23 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 535.055/s, 535.055/s/gpu LR: 8.590687e-09 Logit Scale: 100.000 Contrastive_loss: 0.30884 (0.24801) Loss: 0.30884 (0.24801)
2024-08-29,21:38:04 | INFO | Train Epoch: 23 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 495.051/s, 495.051/s/gpu LR: 8.583391e-09 Logit Scale: 100.000 Contrastive_loss: 0.35541 (0.25091) Loss: 0.35541 (0.25091)
2024-08-29,21:38:23 | INFO | Train Epoch: 23 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 529.720/s, 529.720/s/gpu LR: 8.576080e-09 Logit Scale: 100.000 Contrastive_loss: 0.18367 (0.24914) Loss: 0.18367 (0.24914)
2024-08-29,21:38:42 | INFO | Train Epoch: 23 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 494.707/s, 494.707/s/gpu LR: 8.568753e-09 Logit Scale: 100.000 Contrastive_loss: 0.29042 (0.25020) Loss: 0.29042 (0.25020)
2024-08-29,21:39:02 | INFO | Train Epoch: 23 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 488.836/s, 488.836/s/gpu LR: 8.561411e-09 Logit Scale: 100.000 Contrastive_loss: 0.27183 (0.25074) Loss: 0.27183 (0.25074)
2024-08-29,21:39:21 | INFO | Train Epoch: 23 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 530.670/s, 530.670/s/gpu LR: 8.554053e-09 Logit Scale: 100.000 Contrastive_loss: 0.16610 (0.24868) Loss: 0.16610 (0.24868)
2024-08-29,21:39:40 | INFO | Train Epoch: 23 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.194, 494.316/s, 494.316/s/gpu LR: 8.546679e-09 Logit Scale: 100.000 Contrastive_loss: 0.25935 (0.24893) Loss: 0.25935 (0.24893)
2024-08-29,21:40:00 | INFO | Train Epoch: 23 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 495.782/s, 495.782/s/gpu LR: 8.539290e-09 Logit Scale: 100.000 Contrastive_loss: 0.10943 (0.24569) Loss: 0.10943 (0.24569)
2024-08-29,21:40:07 | INFO | Eval Epoch: 24 [200 / 1000]	Clip Loss: 0.490141	
2024-08-29,21:40:08 | INFO | Eval Epoch: 24 image_to_text_mean_rank: 2.4710	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6590	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.5020	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6760	text_to_image_R@5: 0.8980	text_to_image_R@10: 0.9580	clip_val_loss: 0.4472	epoch: 24.0000	num_samples: 1000.0000
2024-08-29,21:40:09 | INFO | Start epoch 24
2024-08-29,21:40:09 | INFO | Train Epoch: 24 [   100/145000.0 (0%)] Data (t): 0.034 Batch (t): 0.121, 825.722/s, 825.722/s/gpu LR: 8.729759e-09 Logit Scale: 100.000 Contrastive_loss: 0.20107 (0.20107) Loss: 0.20107 (0.20107)
2024-08-29,21:40:29 | INFO | Train Epoch: 24 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 486.917/s, 486.917/s/gpu LR: 8.722777e-09 Logit Scale: 100.000 Contrastive_loss: 0.37981 (0.29044) Loss: 0.37981 (0.29044)
2024-08-29,21:40:48 | INFO | Train Epoch: 24 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 502.157/s, 502.157/s/gpu LR: 8.715778e-09 Logit Scale: 100.000 Contrastive_loss: 0.40390 (0.32826) Loss: 0.40390 (0.32826)
2024-08-29,21:41:07 | INFO | Train Epoch: 24 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 497.843/s, 497.843/s/gpu LR: 8.708763e-09 Logit Scale: 100.000 Contrastive_loss: 0.21995 (0.30118) Loss: 0.21995 (0.30118)
2024-08-29,21:41:27 | INFO | Train Epoch: 24 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.192, 526.135/s, 526.135/s/gpu LR: 8.701731e-09 Logit Scale: 100.000 Contrastive_loss: 0.15571 (0.27209) Loss: 0.15571 (0.27209)
2024-08-29,21:41:46 | INFO | Train Epoch: 24 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.194, 532.314/s, 532.314/s/gpu LR: 8.694684e-09 Logit Scale: 100.000 Contrastive_loss: 0.19282 (0.25888) Loss: 0.19282 (0.25888)
2024-08-29,21:42:05 | INFO | Train Epoch: 24 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.194, 541.677/s, 541.677/s/gpu LR: 8.687620e-09 Logit Scale: 100.000 Contrastive_loss: 0.13818 (0.24163) Loss: 0.13818 (0.24163)
2024-08-29,21:42:25 | INFO | Train Epoch: 24 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.194, 494.845/s, 494.845/s/gpu LR: 8.680540e-09 Logit Scale: 100.000 Contrastive_loss: 0.27185 (0.24541) Loss: 0.27185 (0.24541)
2024-08-29,21:42:44 | INFO | Train Epoch: 24 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 503.535/s, 503.535/s/gpu LR: 8.673443e-09 Logit Scale: 100.000 Contrastive_loss: 0.16602 (0.23659) Loss: 0.16602 (0.23659)
2024-08-29,21:43:03 | INFO | Train Epoch: 24 [ 90100/145000.0 (62%)] Data (t): 0.107 Batch (t): 0.192, 523.656/s, 523.656/s/gpu LR: 8.666331e-09 Logit Scale: 100.000 Contrastive_loss: 0.24708 (0.23764) Loss: 0.24708 (0.23764)
2024-08-29,21:43:23 | INFO | Train Epoch: 24 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 492.972/s, 492.972/s/gpu LR: 8.659203e-09 Logit Scale: 100.000 Contrastive_loss: 0.17724 (0.23215) Loss: 0.17724 (0.23215)
2024-08-29,21:43:42 | INFO | Train Epoch: 24 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 497.750/s, 497.750/s/gpu LR: 8.652058e-09 Logit Scale: 100.000 Contrastive_loss: 0.20504 (0.22989) Loss: 0.20504 (0.22989)
2024-08-29,21:44:01 | INFO | Train Epoch: 24 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 523.769/s, 523.769/s/gpu LR: 8.644898e-09 Logit Scale: 100.000 Contrastive_loss: 0.32077 (0.23688) Loss: 0.32077 (0.23688)
2024-08-29,21:44:20 | INFO | Train Epoch: 24 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 503.438/s, 503.438/s/gpu LR: 8.637721e-09 Logit Scale: 100.000 Contrastive_loss: 0.17837 (0.23270) Loss: 0.17837 (0.23270)
2024-08-29,21:44:40 | INFO | Train Epoch: 24 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 490.816/s, 490.816/s/gpu LR: 8.630529e-09 Logit Scale: 100.000 Contrastive_loss: 0.20052 (0.23055) Loss: 0.20052 (0.23055)
2024-08-29,21:44:49 | INFO | Train Epoch: 24 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 521.335/s, 521.335/s/gpu LR: 8.626999e-09 Logit Scale: 100.000 Contrastive_loss: 0.25982 (0.23238) Loss: 0.25982 (0.23238)
2024-08-29,21:44:59 | INFO | Train Epoch: 24 [150100/145000.0 (104%)] Data (t): 0.106 Batch (t): 0.192, 504.479/s, 504.479/s/gpu LR: 8.623320e-09 Logit Scale: 100.000 Contrastive_loss: 0.23762 (0.23269) Loss: 0.23762 (0.23269)
2024-08-29,21:45:18 | INFO | Train Epoch: 24 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 506.469/s, 506.469/s/gpu LR: 8.616096e-09 Logit Scale: 100.000 Contrastive_loss: 0.28332 (0.23550) Loss: 0.28332 (0.23550)
2024-08-29,21:45:38 | INFO | Train Epoch: 24 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 499.157/s, 499.157/s/gpu LR: 8.608856e-09 Logit Scale: 100.000 Contrastive_loss: 0.20866 (0.23409) Loss: 0.20866 (0.23409)
2024-08-29,21:45:57 | INFO | Train Epoch: 24 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.194, 532.244/s, 532.244/s/gpu LR: 8.601600e-09 Logit Scale: 100.000 Contrastive_loss: 0.22124 (0.23345) Loss: 0.22124 (0.23345)
2024-08-29,21:46:16 | INFO | Train Epoch: 24 [190100/145000.0 (131%)] Data (t): 0.110 Batch (t): 0.193, 499.263/s, 499.263/s/gpu LR: 8.594328e-09 Logit Scale: 100.000 Contrastive_loss: 0.28299 (0.23581) Loss: 0.28299 (0.23581)
2024-08-29,21:46:35 | INFO | Train Epoch: 24 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 494.881/s, 494.881/s/gpu LR: 8.587041e-09 Logit Scale: 100.000 Contrastive_loss: 0.18619 (0.23355) Loss: 0.18619 (0.23355)
2024-08-29,21:46:55 | INFO | Train Epoch: 24 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.192, 534.700/s, 534.700/s/gpu LR: 8.579738e-09 Logit Scale: 100.000 Contrastive_loss: 0.28421 (0.23576) Loss: 0.28421 (0.23576)
2024-08-29,21:47:14 | INFO | Train Epoch: 24 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.192, 504.112/s, 504.112/s/gpu LR: 8.572419e-09 Logit Scale: 100.000 Contrastive_loss: 0.36790 (0.24126) Loss: 0.36790 (0.24126)
2024-08-29,21:47:33 | INFO | Train Epoch: 24 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.194, 497.990/s, 497.990/s/gpu LR: 8.565084e-09 Logit Scale: 100.000 Contrastive_loss: 0.33011 (0.24482) Loss: 0.33011 (0.24482)
2024-08-29,21:47:53 | INFO | Train Epoch: 24 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 482.791/s, 482.791/s/gpu LR: 8.557734e-09 Logit Scale: 100.000 Contrastive_loss: 0.20158 (0.24315) Loss: 0.20158 (0.24315)
2024-08-29,21:48:12 | INFO | Train Epoch: 24 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.192, 535.726/s, 535.726/s/gpu LR: 8.550368e-09 Logit Scale: 100.000 Contrastive_loss: 0.24183 (0.24310) Loss: 0.24183 (0.24310)
2024-08-29,21:48:31 | INFO | Train Epoch: 24 [260100/145000.0 (179%)] Data (t): 0.110 Batch (t): 0.194, 535.809/s, 535.809/s/gpu LR: 8.542986e-09 Logit Scale: 100.000 Contrastive_loss: 0.25421 (0.24350) Loss: 0.25421 (0.24350)
2024-08-29,21:48:51 | INFO | Train Epoch: 24 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 494.275/s, 494.275/s/gpu LR: 8.535589e-09 Logit Scale: 100.000 Contrastive_loss: 0.28052 (0.24478) Loss: 0.28052 (0.24478)
2024-08-29,21:49:10 | INFO | Train Epoch: 24 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 490.991/s, 490.991/s/gpu LR: 8.528177e-09 Logit Scale: 100.000 Contrastive_loss: 0.19741 (0.24320) Loss: 0.19741 (0.24320)
2024-08-29,21:49:29 | INFO | Train Epoch: 24 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.192, 526.223/s, 526.223/s/gpu LR: 8.520749e-09 Logit Scale: 100.000 Contrastive_loss: 0.16002 (0.24051) Loss: 0.16002 (0.24051)
2024-08-29,21:49:48 | INFO | Train Epoch: 24 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 532.323/s, 532.323/s/gpu LR: 8.513306e-09 Logit Scale: 100.000 Contrastive_loss: 0.29779 (0.24230) Loss: 0.29779 (0.24230)
2024-08-29,21:50:08 | INFO | Train Epoch: 24 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 530.677/s, 530.677/s/gpu LR: 8.505847e-09 Logit Scale: 100.000 Contrastive_loss: 0.26848 (0.24310) Loss: 0.26848 (0.24310)
2024-08-29,21:50:27 | INFO | Train Epoch: 24 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 495.528/s, 495.528/s/gpu LR: 8.498373e-09 Logit Scale: 100.000 Contrastive_loss: 0.32739 (0.24558) Loss: 0.32739 (0.24558)
2024-08-29,21:50:46 | INFO | Train Epoch: 24 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 494.454/s, 494.454/s/gpu LR: 8.490883e-09 Logit Scale: 100.000 Contrastive_loss: 0.20524 (0.24442) Loss: 0.20524 (0.24442)
2024-08-29,21:51:06 | INFO | Train Epoch: 24 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 498.490/s, 498.490/s/gpu LR: 8.483378e-09 Logit Scale: 100.000 Contrastive_loss: 0.30554 (0.24612) Loss: 0.30554 (0.24612)
2024-08-29,21:51:25 | INFO | Train Epoch: 24 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 528.245/s, 528.245/s/gpu LR: 8.475858e-09 Logit Scale: 100.000 Contrastive_loss: 0.35171 (0.24898) Loss: 0.35171 (0.24898)
2024-08-29,21:51:44 | INFO | Train Epoch: 24 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 539.336/s, 539.336/s/gpu LR: 8.468323e-09 Logit Scale: 100.000 Contrastive_loss: 0.18233 (0.24722) Loss: 0.18233 (0.24722)
2024-08-29,21:52:03 | INFO | Train Epoch: 24 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 540.095/s, 540.095/s/gpu LR: 8.460772e-09 Logit Scale: 100.000 Contrastive_loss: 0.28894 (0.24829) Loss: 0.28894 (0.24829)
2024-08-29,21:52:23 | INFO | Train Epoch: 24 [380100/145000.0 (262%)] Data (t): 0.107 Batch (t): 0.193, 530.514/s, 530.514/s/gpu LR: 8.453206e-09 Logit Scale: 100.000 Contrastive_loss: 0.26982 (0.24883) Loss: 0.26982 (0.24883)
2024-08-29,21:52:42 | INFO | Train Epoch: 24 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 497.932/s, 497.932/s/gpu LR: 8.445625e-09 Logit Scale: 100.000 Contrastive_loss: 0.16421 (0.24677) Loss: 0.16421 (0.24677)
2024-08-29,21:53:01 | INFO | Train Epoch: 24 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 534.641/s, 534.641/s/gpu LR: 8.438029e-09 Logit Scale: 100.000 Contrastive_loss: 0.25746 (0.24702) Loss: 0.25746 (0.24702)
2024-08-29,21:53:20 | INFO | Train Epoch: 24 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 521.528/s, 521.528/s/gpu LR: 8.430418e-09 Logit Scale: 100.000 Contrastive_loss: 0.10849 (0.24380) Loss: 0.10849 (0.24380)
2024-08-29,21:53:28 | INFO | Eval Epoch: 25 [200 / 1000]	Clip Loss: 0.489192	
2024-08-29,21:53:29 | INFO | Eval Epoch: 25 image_to_text_mean_rank: 2.4710	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6590	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.5120	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6780	text_to_image_R@5: 0.8980	text_to_image_R@10: 0.9580	clip_val_loss: 0.4464	epoch: 25.0000	num_samples: 1000.0000
2024-08-29,21:53:30 | INFO | Start epoch 25
2024-08-29,21:53:30 | INFO | Train Epoch: 25 [   100/145000.0 (0%)] Data (t): 0.025 Batch (t): 0.111, 897.851/s, 897.851/s/gpu LR: 8.626927e-09 Logit Scale: 100.000 Contrastive_loss: 0.19938 (0.19938) Loss: 0.19938 (0.19938)
2024-08-29,21:53:50 | INFO | Train Epoch: 25 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 503.742/s, 503.742/s/gpu LR: 8.619710e-09 Logit Scale: 100.000 Contrastive_loss: 0.37752 (0.28845) Loss: 0.37752 (0.28845)
2024-08-29,21:54:09 | INFO | Train Epoch: 25 [ 20100/145000.0 (14%)] Data (t): 0.110 Batch (t): 0.194, 529.179/s, 529.179/s/gpu LR: 8.612478e-09 Logit Scale: 100.000 Contrastive_loss: 0.40130 (0.32607) Loss: 0.40130 (0.32607)
2024-08-29,21:54:28 | INFO | Train Epoch: 25 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 501.248/s, 501.248/s/gpu LR: 8.605230e-09 Logit Scale: 100.000 Contrastive_loss: 0.21797 (0.29904) Loss: 0.21797 (0.29904)
2024-08-29,21:54:47 | INFO | Train Epoch: 25 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 487.913/s, 487.913/s/gpu LR: 8.597966e-09 Logit Scale: 100.000 Contrastive_loss: 0.15451 (0.27014) Loss: 0.15451 (0.27014)
2024-08-29,21:55:07 | INFO | Train Epoch: 25 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 525.598/s, 525.598/s/gpu LR: 8.590687e-09 Logit Scale: 100.000 Contrastive_loss: 0.19123 (0.25698) Loss: 0.19123 (0.25698)
2024-08-29,21:55:26 | INFO | Train Epoch: 25 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.194, 500.832/s, 500.832/s/gpu LR: 8.583391e-09 Logit Scale: 100.000 Contrastive_loss: 0.13696 (0.23984) Loss: 0.13696 (0.23984)
2024-08-29,21:55:45 | INFO | Train Epoch: 25 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 522.473/s, 522.473/s/gpu LR: 8.576080e-09 Logit Scale: 100.000 Contrastive_loss: 0.26919 (0.24351) Loss: 0.26919 (0.24351)
2024-08-29,21:56:05 | INFO | Train Epoch: 25 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.192, 534.129/s, 534.129/s/gpu LR: 8.568753e-09 Logit Scale: 100.000 Contrastive_loss: 0.16413 (0.23469) Loss: 0.16413 (0.23469)
2024-08-29,21:56:24 | INFO | Train Epoch: 25 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.192, 532.603/s, 532.603/s/gpu LR: 8.561411e-09 Logit Scale: 100.000 Contrastive_loss: 0.24545 (0.23576) Loss: 0.24545 (0.23576)
2024-08-29,21:56:43 | INFO | Train Epoch: 25 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 494.112/s, 494.112/s/gpu LR: 8.554053e-09 Logit Scale: 100.000 Contrastive_loss: 0.17597 (0.23033) Loss: 0.17597 (0.23033)
2024-08-29,21:57:02 | INFO | Train Epoch: 25 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.193, 533.725/s, 533.725/s/gpu LR: 8.546679e-09 Logit Scale: 100.000 Contrastive_loss: 0.20336 (0.22808) Loss: 0.20336 (0.22808)
2024-08-29,21:57:22 | INFO | Train Epoch: 25 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 538.040/s, 538.040/s/gpu LR: 8.539290e-09 Logit Scale: 100.000 Contrastive_loss: 0.31801 (0.23500) Loss: 0.31801 (0.23500)
2024-08-29,21:57:41 | INFO | Train Epoch: 25 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 528.907/s, 528.907/s/gpu LR: 8.531885e-09 Logit Scale: 100.000 Contrastive_loss: 0.17689 (0.23085) Loss: 0.17689 (0.23085)
2024-08-29,21:58:00 | INFO | Train Epoch: 25 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 498.638/s, 498.638/s/gpu LR: 8.524465e-09 Logit Scale: 100.000 Contrastive_loss: 0.19926 (0.22874) Loss: 0.19926 (0.22874)
2024-08-29,21:58:10 | INFO | Train Epoch: 25 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 528.468/s, 528.468/s/gpu LR: 8.520823e-09 Logit Scale: 100.000 Contrastive_loss: 0.25807 (0.23057) Loss: 0.25807 (0.23057)
2024-08-29,21:58:20 | INFO | Train Epoch: 25 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 528.068/s, 528.068/s/gpu LR: 8.517029e-09 Logit Scale: 100.000 Contrastive_loss: 0.23529 (0.23085) Loss: 0.23529 (0.23085)
2024-08-29,21:58:39 | INFO | Train Epoch: 25 [160100/145000.0 (110%)] Data (t): 0.107 Batch (t): 0.192, 533.768/s, 533.768/s/gpu LR: 8.509578e-09 Logit Scale: 100.000 Contrastive_loss: 0.28036 (0.23360) Loss: 0.28036 (0.23360)
2024-08-29,21:58:58 | INFO | Train Epoch: 25 [170100/145000.0 (117%)] Data (t): 0.107 Batch (t): 0.192, 536.462/s, 536.462/s/gpu LR: 8.502112e-09 Logit Scale: 100.000 Contrastive_loss: 0.20676 (0.23219) Loss: 0.20676 (0.23219)
2024-08-29,21:59:17 | INFO | Train Epoch: 25 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 504.185/s, 504.185/s/gpu LR: 8.494630e-09 Logit Scale: 100.000 Contrastive_loss: 0.21997 (0.23158) Loss: 0.21997 (0.23158)
2024-08-29,21:59:37 | INFO | Train Epoch: 25 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 530.026/s, 530.026/s/gpu LR: 8.487133e-09 Logit Scale: 100.000 Contrastive_loss: 0.28085 (0.23392) Loss: 0.28085 (0.23392)
2024-08-29,21:59:56 | INFO | Train Epoch: 25 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 497.423/s, 497.423/s/gpu LR: 8.479620e-09 Logit Scale: 100.000 Contrastive_loss: 0.18520 (0.23171) Loss: 0.18520 (0.23171)
2024-08-29,22:00:15 | INFO | Train Epoch: 25 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 500.067/s, 500.067/s/gpu LR: 8.472092e-09 Logit Scale: 100.000 Contrastive_loss: 0.28236 (0.23391) Loss: 0.28236 (0.23391)
2024-08-29,22:00:34 | INFO | Train Epoch: 25 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 517.696/s, 517.696/s/gpu LR: 8.464549e-09 Logit Scale: 100.000 Contrastive_loss: 0.36466 (0.23936) Loss: 0.36466 (0.23936)
2024-08-29,22:00:54 | INFO | Train Epoch: 25 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 492.099/s, 492.099/s/gpu LR: 8.456991e-09 Logit Scale: 100.000 Contrastive_loss: 0.32898 (0.24295) Loss: 0.32898 (0.24295)
2024-08-29,22:01:13 | INFO | Train Epoch: 25 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 497.025/s, 497.025/s/gpu LR: 8.449418e-09 Logit Scale: 100.000 Contrastive_loss: 0.19980 (0.24129) Loss: 0.19980 (0.24129)
2024-08-29,22:01:32 | INFO | Train Epoch: 25 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 533.587/s, 533.587/s/gpu LR: 8.441829e-09 Logit Scale: 100.000 Contrastive_loss: 0.24047 (0.24126) Loss: 0.24047 (0.24126)
2024-08-29,22:01:52 | INFO | Train Epoch: 25 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 524.749/s, 524.749/s/gpu LR: 8.434226e-09 Logit Scale: 100.000 Contrastive_loss: 0.25227 (0.24165) Loss: 0.25227 (0.24165)
2024-08-29,22:02:11 | INFO | Train Epoch: 25 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 525.458/s, 525.458/s/gpu LR: 8.426607e-09 Logit Scale: 100.000 Contrastive_loss: 0.27916 (0.24294) Loss: 0.27916 (0.24294)
2024-08-29,22:02:30 | INFO | Train Epoch: 25 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.192, 525.861/s, 525.861/s/gpu LR: 8.418973e-09 Logit Scale: 100.000 Contrastive_loss: 0.19586 (0.24137) Loss: 0.19586 (0.24137)
2024-08-29,22:02:49 | INFO | Train Epoch: 25 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 501.506/s, 501.506/s/gpu LR: 8.411325e-09 Logit Scale: 100.000 Contrastive_loss: 0.15924 (0.23872) Loss: 0.15924 (0.23872)
2024-08-29,22:03:09 | INFO | Train Epoch: 25 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 508.447/s, 508.447/s/gpu LR: 8.403661e-09 Logit Scale: 100.000 Contrastive_loss: 0.29621 (0.24052) Loss: 0.29621 (0.24052)
2024-08-29,22:03:28 | INFO | Train Epoch: 25 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 507.408/s, 507.408/s/gpu LR: 8.395982e-09 Logit Scale: 100.000 Contrastive_loss: 0.26667 (0.24131) Loss: 0.26667 (0.24131)
2024-08-29,22:03:47 | INFO | Train Epoch: 25 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 526.046/s, 526.046/s/gpu LR: 8.388289e-09 Logit Scale: 100.000 Contrastive_loss: 0.32471 (0.24377) Loss: 0.32471 (0.24377)
2024-08-29,22:04:07 | INFO | Train Epoch: 25 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 508.333/s, 508.333/s/gpu LR: 8.380581e-09 Logit Scale: 100.000 Contrastive_loss: 0.20426 (0.24264) Loss: 0.20426 (0.24264)
2024-08-29,22:04:26 | INFO | Train Epoch: 25 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.192, 523.911/s, 523.911/s/gpu LR: 8.372857e-09 Logit Scale: 100.000 Contrastive_loss: 0.30272 (0.24431) Loss: 0.30272 (0.24431)
2024-08-29,22:04:45 | INFO | Train Epoch: 25 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 494.650/s, 494.650/s/gpu LR: 8.365119e-09 Logit Scale: 100.000 Contrastive_loss: 0.34823 (0.24711) Loss: 0.34823 (0.24711)
2024-08-29,22:05:04 | INFO | Train Epoch: 25 [360100/145000.0 (248%)] Data (t): 0.107 Batch (t): 0.192, 504.956/s, 504.956/s/gpu LR: 8.357367e-09 Logit Scale: 100.000 Contrastive_loss: 0.18099 (0.24537) Loss: 0.18099 (0.24537)
2024-08-29,22:05:23 | INFO | Train Epoch: 25 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 533.681/s, 533.681/s/gpu LR: 8.349599e-09 Logit Scale: 100.000 Contrastive_loss: 0.28754 (0.24646) Loss: 0.28754 (0.24646)
2024-08-29,22:05:43 | INFO | Train Epoch: 25 [380100/145000.0 (262%)] Data (t): 0.107 Batch (t): 0.192, 537.494/s, 537.494/s/gpu LR: 8.341817e-09 Logit Scale: 100.000 Contrastive_loss: 0.26795 (0.24699) Loss: 0.26795 (0.24699)
2024-08-29,22:06:02 | INFO | Train Epoch: 25 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 493.167/s, 493.167/s/gpu LR: 8.334020e-09 Logit Scale: 100.000 Contrastive_loss: 0.16246 (0.24493) Loss: 0.16246 (0.24493)
2024-08-29,22:06:21 | INFO | Train Epoch: 25 [400100/145000.0 (276%)] Data (t): 0.107 Batch (t): 0.192, 533.964/s, 533.964/s/gpu LR: 8.326209e-09 Logit Scale: 100.000 Contrastive_loss: 0.25587 (0.24519) Loss: 0.25587 (0.24519)
2024-08-29,22:06:40 | INFO | Train Epoch: 25 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.192, 503.796/s, 503.796/s/gpu LR: 8.318383e-09 Logit Scale: 100.000 Contrastive_loss: 0.10752 (0.24199) Loss: 0.10752 (0.24199)
2024-08-29,22:06:48 | INFO | Eval Epoch: 26 [200 / 1000]	Clip Loss: 0.488156	
2024-08-29,22:06:49 | INFO | Eval Epoch: 26 image_to_text_mean_rank: 2.4700	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6580	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9620	text_to_image_mean_rank: 3.5210	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6770	text_to_image_R@5: 0.8980	text_to_image_R@10: 0.9600	clip_val_loss: 0.4456	epoch: 26.0000	num_samples: 1000.0000
2024-08-29,22:06:50 | INFO | Start epoch 26
2024-08-29,22:06:50 | INFO | Train Epoch: 26 [   100/145000.0 (0%)] Data (t): 0.023 Batch (t): 0.110, 907.157/s, 907.157/s/gpu LR: 8.520749e-09 Logit Scale: 100.000 Contrastive_loss: 0.19753 (0.19753) Loss: 0.19753 (0.19753)
2024-08-29,22:07:10 | INFO | Train Epoch: 26 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 521.534/s, 521.534/s/gpu LR: 8.513306e-09 Logit Scale: 100.000 Contrastive_loss: 0.37516 (0.28635) Loss: 0.37516 (0.28635)
2024-08-29,22:07:29 | INFO | Train Epoch: 26 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 486.188/s, 486.188/s/gpu LR: 8.505847e-09 Logit Scale: 100.000 Contrastive_loss: 0.39871 (0.32380) Loss: 0.39871 (0.32380)
2024-08-29,22:07:48 | INFO | Train Epoch: 26 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 535.184/s, 535.184/s/gpu LR: 8.498373e-09 Logit Scale: 100.000 Contrastive_loss: 0.21612 (0.29688) Loss: 0.21612 (0.29688)
2024-08-29,22:08:08 | INFO | Train Epoch: 26 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 530.406/s, 530.406/s/gpu LR: 8.490883e-09 Logit Scale: 100.000 Contrastive_loss: 0.15334 (0.26817) Loss: 0.15334 (0.26817)
2024-08-29,22:08:27 | INFO | Train Epoch: 26 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 528.147/s, 528.147/s/gpu LR: 8.483378e-09 Logit Scale: 100.000 Contrastive_loss: 0.18971 (0.25510) Loss: 0.18971 (0.25510)
2024-08-29,22:08:46 | INFO | Train Epoch: 26 [ 60100/145000.0 (41%)] Data (t): 0.107 Batch (t): 0.192, 525.923/s, 525.923/s/gpu LR: 8.475858e-09 Logit Scale: 100.000 Contrastive_loss: 0.13585 (0.23806) Loss: 0.13585 (0.23806)
2024-08-29,22:09:05 | INFO | Train Epoch: 26 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 504.862/s, 504.862/s/gpu LR: 8.468323e-09 Logit Scale: 100.000 Contrastive_loss: 0.26665 (0.24164) Loss: 0.26665 (0.24164)
2024-08-29,22:09:25 | INFO | Train Epoch: 26 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 539.873/s, 539.873/s/gpu LR: 8.460772e-09 Logit Scale: 100.000 Contrastive_loss: 0.16229 (0.23282) Loss: 0.16229 (0.23282)
2024-08-29,22:09:44 | INFO | Train Epoch: 26 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 522.985/s, 522.985/s/gpu LR: 8.453206e-09 Logit Scale: 100.000 Contrastive_loss: 0.24384 (0.23392) Loss: 0.24384 (0.23392)
2024-08-29,22:10:03 | INFO | Train Epoch: 26 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 493.235/s, 493.235/s/gpu LR: 8.445625e-09 Logit Scale: 100.000 Contrastive_loss: 0.17464 (0.22853) Loss: 0.17464 (0.22853)
2024-08-29,22:10:23 | INFO | Train Epoch: 26 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.193, 500.694/s, 500.694/s/gpu LR: 8.438029e-09 Logit Scale: 100.000 Contrastive_loss: 0.20148 (0.22628) Loss: 0.20148 (0.22628)
2024-08-29,22:10:42 | INFO | Train Epoch: 26 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.192, 540.122/s, 540.122/s/gpu LR: 8.430418e-09 Logit Scale: 100.000 Contrastive_loss: 0.31525 (0.23312) Loss: 0.31525 (0.23312)
2024-08-29,22:11:01 | INFO | Train Epoch: 26 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 525.626/s, 525.626/s/gpu LR: 8.422792e-09 Logit Scale: 100.000 Contrastive_loss: 0.17531 (0.22899) Loss: 0.17531 (0.22899)
2024-08-29,22:11:20 | INFO | Train Epoch: 26 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 522.429/s, 522.429/s/gpu LR: 8.415151e-09 Logit Scale: 100.000 Contrastive_loss: 0.19804 (0.22693) Loss: 0.19804 (0.22693)
2024-08-29,22:11:30 | INFO | Train Epoch: 26 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 522.664/s, 522.664/s/gpu LR: 8.411401e-09 Logit Scale: 100.000 Contrastive_loss: 0.25639 (0.22877) Loss: 0.25639 (0.22877)
2024-08-29,22:11:40 | INFO | Train Epoch: 26 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 507.870/s, 507.870/s/gpu LR: 8.407495e-09 Logit Scale: 100.000 Contrastive_loss: 0.23315 (0.22903) Loss: 0.23315 (0.22903)
2024-08-29,22:11:59 | INFO | Train Epoch: 26 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 497.640/s, 497.640/s/gpu LR: 8.399824e-09 Logit Scale: 100.000 Contrastive_loss: 0.27724 (0.23171) Loss: 0.27724 (0.23171)
2024-08-29,22:12:18 | INFO | Train Epoch: 26 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 496.529/s, 496.529/s/gpu LR: 8.392138e-09 Logit Scale: 100.000 Contrastive_loss: 0.20485 (0.23029) Loss: 0.20485 (0.23029)
2024-08-29,22:12:38 | INFO | Train Epoch: 26 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 506.715/s, 506.715/s/gpu LR: 8.384437e-09 Logit Scale: 100.000 Contrastive_loss: 0.21856 (0.22971) Loss: 0.21856 (0.22971)
2024-08-29,22:12:57 | INFO | Train Epoch: 26 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 527.586/s, 527.586/s/gpu LR: 8.376721e-09 Logit Scale: 100.000 Contrastive_loss: 0.27883 (0.23205) Loss: 0.27883 (0.23205)
2024-08-29,22:13:16 | INFO | Train Epoch: 26 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 509.485/s, 509.485/s/gpu LR: 8.368990e-09 Logit Scale: 100.000 Contrastive_loss: 0.18432 (0.22988) Loss: 0.18432 (0.22988)
2024-08-29,22:13:35 | INFO | Train Epoch: 26 [210100/145000.0 (145%)] Data (t): 0.107 Batch (t): 0.192, 529.392/s, 529.392/s/gpu LR: 8.361245e-09 Logit Scale: 100.000 Contrastive_loss: 0.28064 (0.23208) Loss: 0.28064 (0.23208)
2024-08-29,22:13:55 | INFO | Train Epoch: 26 [220100/145000.0 (152%)] Data (t): 0.107 Batch (t): 0.192, 526.733/s, 526.733/s/gpu LR: 8.353485e-09 Logit Scale: 100.000 Contrastive_loss: 0.36173 (0.23749) Loss: 0.36173 (0.23749)
2024-08-29,22:14:14 | INFO | Train Epoch: 26 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 498.269/s, 498.269/s/gpu LR: 8.345710e-09 Logit Scale: 100.000 Contrastive_loss: 0.32752 (0.24109) Loss: 0.32752 (0.24109)
2024-08-29,22:14:33 | INFO | Train Epoch: 26 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 501.015/s, 501.015/s/gpu LR: 8.337921e-09 Logit Scale: 100.000 Contrastive_loss: 0.19789 (0.23943) Loss: 0.19789 (0.23943)
2024-08-29,22:14:52 | INFO | Train Epoch: 26 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 542.384/s, 542.384/s/gpu LR: 8.330117e-09 Logit Scale: 100.000 Contrastive_loss: 0.23916 (0.23942) Loss: 0.23916 (0.23942)
2024-08-29,22:15:12 | INFO | Train Epoch: 26 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 502.751/s, 502.751/s/gpu LR: 8.322298e-09 Logit Scale: 100.000 Contrastive_loss: 0.25031 (0.23980) Loss: 0.25031 (0.23980)
2024-08-29,22:15:31 | INFO | Train Epoch: 26 [270100/145000.0 (186%)] Data (t): 0.107 Batch (t): 0.192, 536.393/s, 536.393/s/gpu LR: 8.314465e-09 Logit Scale: 100.000 Contrastive_loss: 0.27801 (0.24112) Loss: 0.27801 (0.24112)
2024-08-29,22:15:50 | INFO | Train Epoch: 26 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.192, 510.983/s, 510.983/s/gpu LR: 8.306617e-09 Logit Scale: 100.000 Contrastive_loss: 0.19434 (0.23956) Loss: 0.19434 (0.23956)
2024-08-29,22:16:09 | INFO | Train Epoch: 26 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.192, 532.971/s, 532.971/s/gpu LR: 8.298754e-09 Logit Scale: 100.000 Contrastive_loss: 0.15842 (0.23694) Loss: 0.15842 (0.23694)
2024-08-29,22:16:29 | INFO | Train Epoch: 26 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 504.883/s, 504.883/s/gpu LR: 8.290878e-09 Logit Scale: 100.000 Contrastive_loss: 0.29442 (0.23874) Loss: 0.29442 (0.23874)
2024-08-29,22:16:48 | INFO | Train Epoch: 26 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 500.095/s, 500.095/s/gpu LR: 8.282986e-09 Logit Scale: 100.000 Contrastive_loss: 0.26512 (0.23954) Loss: 0.26512 (0.23954)
2024-08-29,22:17:07 | INFO | Train Epoch: 26 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 527.393/s, 527.393/s/gpu LR: 8.275081e-09 Logit Scale: 100.000 Contrastive_loss: 0.32225 (0.24197) Loss: 0.32225 (0.24197)
2024-08-29,22:17:26 | INFO | Train Epoch: 26 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 504.362/s, 504.362/s/gpu LR: 8.267161e-09 Logit Scale: 100.000 Contrastive_loss: 0.20329 (0.24087) Loss: 0.20329 (0.24087)
2024-08-29,22:17:46 | INFO | Train Epoch: 26 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.192, 526.864/s, 526.864/s/gpu LR: 8.259226e-09 Logit Scale: 100.000 Contrastive_loss: 0.29994 (0.24251) Loss: 0.29994 (0.24251)
2024-08-29,22:18:05 | INFO | Train Epoch: 26 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.193, 535.059/s, 535.059/s/gpu LR: 8.251278e-09 Logit Scale: 100.000 Contrastive_loss: 0.34501 (0.24528) Loss: 0.34501 (0.24528)
2024-08-29,22:18:24 | INFO | Train Epoch: 26 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 529.603/s, 529.603/s/gpu LR: 8.243315e-09 Logit Scale: 100.000 Contrastive_loss: 0.17980 (0.24356) Loss: 0.17980 (0.24356)
2024-08-29,22:18:44 | INFO | Train Epoch: 26 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 507.056/s, 507.056/s/gpu LR: 8.235338e-09 Logit Scale: 100.000 Contrastive_loss: 0.28612 (0.24465) Loss: 0.28612 (0.24465)
2024-08-29,22:19:03 | INFO | Train Epoch: 26 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 528.569/s, 528.569/s/gpu LR: 8.227346e-09 Logit Scale: 100.000 Contrastive_loss: 0.26604 (0.24518) Loss: 0.26604 (0.24518)
2024-08-29,22:19:22 | INFO | Train Epoch: 26 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 497.109/s, 497.109/s/gpu LR: 8.219341e-09 Logit Scale: 100.000 Contrastive_loss: 0.16065 (0.24312) Loss: 0.16065 (0.24312)
2024-08-29,22:19:41 | INFO | Train Epoch: 26 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 538.826/s, 538.826/s/gpu LR: 8.211321e-09 Logit Scale: 100.000 Contrastive_loss: 0.25420 (0.24338) Loss: 0.25420 (0.24338)
2024-08-29,22:20:01 | INFO | Train Epoch: 26 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 523.195/s, 523.195/s/gpu LR: 8.203288e-09 Logit Scale: 100.000 Contrastive_loss: 0.10658 (0.24020) Loss: 0.10658 (0.24020)
2024-08-29,22:20:08 | INFO | Eval Epoch: 27 [200 / 1000]	Clip Loss: 0.487320	
2024-08-29,22:20:09 | INFO | Eval Epoch: 27 image_to_text_mean_rank: 2.4660	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6600	image_to_text_R@5: 0.9100	image_to_text_R@10: 0.9620	text_to_image_mean_rank: 3.5330	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6780	text_to_image_R@5: 0.8980	text_to_image_R@10: 0.9610	clip_val_loss: 0.4448	epoch: 27.0000	num_samples: 1000.0000
2024-08-29,22:20:10 | INFO | Start epoch 27
2024-08-29,22:20:11 | INFO | Train Epoch: 27 [   100/145000.0 (0%)] Data (t): 0.024 Batch (t): 0.104, 965.675/s, 965.675/s/gpu LR: 8.411325e-09 Logit Scale: 100.000 Contrastive_loss: 0.19600 (0.19600) Loss: 0.19600 (0.19600)
2024-08-29,22:20:30 | INFO | Train Epoch: 27 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 505.279/s, 505.279/s/gpu LR: 8.403661e-09 Logit Scale: 100.000 Contrastive_loss: 0.37310 (0.28455) Loss: 0.37310 (0.28455)
2024-08-29,22:20:49 | INFO | Train Epoch: 27 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 499.439/s, 499.439/s/gpu LR: 8.395982e-09 Logit Scale: 100.000 Contrastive_loss: 0.39615 (0.32175) Loss: 0.39615 (0.32175)
2024-08-29,22:21:09 | INFO | Train Epoch: 27 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 534.235/s, 534.235/s/gpu LR: 8.388289e-09 Logit Scale: 100.000 Contrastive_loss: 0.21446 (0.29493) Loss: 0.21446 (0.29493)
2024-08-29,22:21:28 | INFO | Train Epoch: 27 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 494.371/s, 494.371/s/gpu LR: 8.380581e-09 Logit Scale: 100.000 Contrastive_loss: 0.15221 (0.26638) Loss: 0.15221 (0.26638)
2024-08-29,22:21:47 | INFO | Train Epoch: 27 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.194, 532.198/s, 532.198/s/gpu LR: 8.372857e-09 Logit Scale: 100.000 Contrastive_loss: 0.18825 (0.25336) Loss: 0.18825 (0.25336)
2024-08-29,22:22:07 | INFO | Train Epoch: 27 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 509.129/s, 509.129/s/gpu LR: 8.365119e-09 Logit Scale: 100.000 Contrastive_loss: 0.13475 (0.23642) Loss: 0.13475 (0.23642)
2024-08-29,22:22:26 | INFO | Train Epoch: 27 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.194, 496.048/s, 496.048/s/gpu LR: 8.357367e-09 Logit Scale: 100.000 Contrastive_loss: 0.26433 (0.23991) Loss: 0.26433 (0.23991)
2024-08-29,22:22:45 | INFO | Train Epoch: 27 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 495.772/s, 495.772/s/gpu LR: 8.349599e-09 Logit Scale: 100.000 Contrastive_loss: 0.16047 (0.23108) Loss: 0.16047 (0.23108)
2024-08-29,22:23:05 | INFO | Train Epoch: 27 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 490.762/s, 490.762/s/gpu LR: 8.341817e-09 Logit Scale: 100.000 Contrastive_loss: 0.24212 (0.23218) Loss: 0.24212 (0.23218)
2024-08-29,22:23:24 | INFO | Train Epoch: 27 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 500.542/s, 500.542/s/gpu LR: 8.334020e-09 Logit Scale: 100.000 Contrastive_loss: 0.17336 (0.22684) Loss: 0.17336 (0.22684)
2024-08-29,22:23:43 | INFO | Train Epoch: 27 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 502.071/s, 502.071/s/gpu LR: 8.326209e-09 Logit Scale: 100.000 Contrastive_loss: 0.19969 (0.22457) Loss: 0.19969 (0.22457)
2024-08-29,22:24:02 | INFO | Train Epoch: 27 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 537.775/s, 537.775/s/gpu LR: 8.318383e-09 Logit Scale: 100.000 Contrastive_loss: 0.31279 (0.23136) Loss: 0.31279 (0.23136)
2024-08-29,22:24:22 | INFO | Train Epoch: 27 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 507.527/s, 507.527/s/gpu LR: 8.310542e-09 Logit Scale: 100.000 Contrastive_loss: 0.17399 (0.22726) Loss: 0.17399 (0.22726)
2024-08-29,22:24:41 | INFO | Train Epoch: 27 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 498.331/s, 498.331/s/gpu LR: 8.302687e-09 Logit Scale: 100.000 Contrastive_loss: 0.19676 (0.22523) Loss: 0.19676 (0.22523)
2024-08-29,22:24:50 | INFO | Train Epoch: 27 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 535.432/s, 535.432/s/gpu LR: 8.298833e-09 Logit Scale: 100.000 Contrastive_loss: 0.25470 (0.22707) Loss: 0.25470 (0.22707)
2024-08-29,22:25:00 | INFO | Train Epoch: 27 [150100/145000.0 (104%)] Data (t): 0.110 Batch (t): 0.193, 528.680/s, 528.680/s/gpu LR: 8.294818e-09 Logit Scale: 100.000 Contrastive_loss: 0.23113 (0.22731) Loss: 0.23113 (0.22731)
2024-08-29,22:25:20 | INFO | Train Epoch: 27 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 511.148/s, 511.148/s/gpu LR: 8.286934e-09 Logit Scale: 100.000 Contrastive_loss: 0.27464 (0.22994) Loss: 0.27464 (0.22994)
2024-08-29,22:25:39 | INFO | Train Epoch: 27 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 530.784/s, 530.784/s/gpu LR: 8.279035e-09 Logit Scale: 100.000 Contrastive_loss: 0.20297 (0.22852) Loss: 0.20297 (0.22852)
2024-08-29,22:25:58 | INFO | Train Epoch: 27 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 531.080/s, 531.080/s/gpu LR: 8.271122e-09 Logit Scale: 100.000 Contrastive_loss: 0.21739 (0.22796) Loss: 0.21739 (0.22796)
2024-08-29,22:26:17 | INFO | Train Epoch: 27 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 531.718/s, 531.718/s/gpu LR: 8.263195e-09 Logit Scale: 100.000 Contrastive_loss: 0.27682 (0.23029) Loss: 0.27682 (0.23029)
2024-08-29,22:26:37 | INFO | Train Epoch: 27 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 495.364/s, 495.364/s/gpu LR: 8.255254e-09 Logit Scale: 100.000 Contrastive_loss: 0.18330 (0.22815) Loss: 0.18330 (0.22815)
2024-08-29,22:26:56 | INFO | Train Epoch: 27 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 494.820/s, 494.820/s/gpu LR: 8.247298e-09 Logit Scale: 100.000 Contrastive_loss: 0.27891 (0.23036) Loss: 0.27891 (0.23036)
2024-08-29,22:27:15 | INFO | Train Epoch: 27 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 527.795/s, 527.795/s/gpu LR: 8.239328e-09 Logit Scale: 100.000 Contrastive_loss: 0.35880 (0.23571) Loss: 0.35880 (0.23571)
2024-08-29,22:27:35 | INFO | Train Epoch: 27 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 524.145/s, 524.145/s/gpu LR: 8.231344e-09 Logit Scale: 100.000 Contrastive_loss: 0.32635 (0.23934) Loss: 0.32635 (0.23934)
2024-08-29,22:27:54 | INFO | Train Epoch: 27 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 525.982/s, 525.982/s/gpu LR: 8.223345e-09 Logit Scale: 100.000 Contrastive_loss: 0.19612 (0.23768) Loss: 0.19612 (0.23768)
2024-08-29,22:28:13 | INFO | Train Epoch: 27 [250100/145000.0 (172%)] Data (t): 0.107 Batch (t): 0.192, 532.507/s, 532.507/s/gpu LR: 8.215333e-09 Logit Scale: 100.000 Contrastive_loss: 0.23781 (0.23768) Loss: 0.23781 (0.23768)
2024-08-29,22:28:32 | INFO | Train Epoch: 27 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 534.575/s, 534.575/s/gpu LR: 8.207306e-09 Logit Scale: 100.000 Contrastive_loss: 0.24845 (0.23807) Loss: 0.24845 (0.23807)
2024-08-29,22:28:52 | INFO | Train Epoch: 27 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 493.752/s, 493.752/s/gpu LR: 8.199265e-09 Logit Scale: 100.000 Contrastive_loss: 0.27693 (0.23941) Loss: 0.27693 (0.23941)
2024-08-29,22:29:11 | INFO | Train Epoch: 27 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 489.234/s, 489.234/s/gpu LR: 8.191211e-09 Logit Scale: 100.000 Contrastive_loss: 0.19281 (0.23785) Loss: 0.19281 (0.23785)
2024-08-29,22:29:30 | INFO | Train Epoch: 27 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.193, 493.162/s, 493.162/s/gpu LR: 8.183142e-09 Logit Scale: 100.000 Contrastive_loss: 0.15765 (0.23527) Loss: 0.15765 (0.23527)
2024-08-29,22:29:49 | INFO | Train Epoch: 27 [300100/145000.0 (207%)] Data (t): 0.107 Batch (t): 0.192, 528.523/s, 528.523/s/gpu LR: 8.175059e-09 Logit Scale: 100.000 Contrastive_loss: 0.29287 (0.23707) Loss: 0.29287 (0.23707)
2024-08-29,22:30:09 | INFO | Train Epoch: 27 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 492.549/s, 492.549/s/gpu LR: 8.166963e-09 Logit Scale: 100.000 Contrastive_loss: 0.26361 (0.23787) Loss: 0.26361 (0.23787)
2024-08-29,22:30:28 | INFO | Train Epoch: 27 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 531.708/s, 531.708/s/gpu LR: 8.158852e-09 Logit Scale: 100.000 Contrastive_loss: 0.31995 (0.24028) Loss: 0.31995 (0.24028)
2024-08-29,22:30:47 | INFO | Train Epoch: 27 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 519.333/s, 519.333/s/gpu LR: 8.150728e-09 Logit Scale: 100.000 Contrastive_loss: 0.20240 (0.23920) Loss: 0.20240 (0.23920)
2024-08-29,22:31:06 | INFO | Train Epoch: 27 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 530.971/s, 530.971/s/gpu LR: 8.142589e-09 Logit Scale: 100.000 Contrastive_loss: 0.29681 (0.24080) Loss: 0.29681 (0.24080)
2024-08-29,22:31:26 | INFO | Train Epoch: 27 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.192, 530.965/s, 530.965/s/gpu LR: 8.134437e-09 Logit Scale: 100.000 Contrastive_loss: 0.34185 (0.24353) Loss: 0.34185 (0.24353)
2024-08-29,22:31:45 | INFO | Train Epoch: 27 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 530.109/s, 530.109/s/gpu LR: 8.126272e-09 Logit Scale: 100.000 Contrastive_loss: 0.17855 (0.24182) Loss: 0.17855 (0.24182)
2024-08-29,22:32:04 | INFO | Train Epoch: 27 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 507.682/s, 507.682/s/gpu LR: 8.118092e-09 Logit Scale: 100.000 Contrastive_loss: 0.28465 (0.24292) Loss: 0.28465 (0.24292)
2024-08-29,22:32:24 | INFO | Train Epoch: 27 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 524.206/s, 524.206/s/gpu LR: 8.109899e-09 Logit Scale: 100.000 Contrastive_loss: 0.26419 (0.24345) Loss: 0.26419 (0.24345)
2024-08-29,22:32:43 | INFO | Train Epoch: 27 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.193, 538.043/s, 538.043/s/gpu LR: 8.101692e-09 Logit Scale: 100.000 Contrastive_loss: 0.15899 (0.24139) Loss: 0.15899 (0.24139)
2024-08-29,22:33:02 | INFO | Train Epoch: 27 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 521.189/s, 521.189/s/gpu LR: 8.093472e-09 Logit Scale: 100.000 Contrastive_loss: 0.25250 (0.24166) Loss: 0.25250 (0.24166)
2024-08-29,22:33:21 | INFO | Train Epoch: 27 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.192, 497.716/s, 497.716/s/gpu LR: 8.085238e-09 Logit Scale: 100.000 Contrastive_loss: 0.10566 (0.23849) Loss: 0.10566 (0.23849)
2024-08-29,22:33:29 | INFO | Eval Epoch: 28 [200 / 1000]	Clip Loss: 0.486711	
2024-08-29,22:33:30 | INFO | Eval Epoch: 28 image_to_text_mean_rank: 2.4660	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6610	image_to_text_R@5: 0.9100	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.5440	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6770	text_to_image_R@5: 0.8980	text_to_image_R@10: 0.9610	clip_val_loss: 0.4443	epoch: 28.0000	num_samples: 1000.0000
2024-08-29,22:33:31 | INFO | Start epoch 28
2024-08-29,22:33:31 | INFO | Train Epoch: 28 [   100/145000.0 (0%)] Data (t): 0.027 Batch (t): 0.110, 909.086/s, 909.086/s/gpu LR: 8.298754e-09 Logit Scale: 100.000 Contrastive_loss: 0.19457 (0.19457) Loss: 0.19457 (0.19457)
2024-08-29,22:33:51 | INFO | Train Epoch: 28 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 525.692/s, 525.692/s/gpu LR: 8.290878e-09 Logit Scale: 100.000 Contrastive_loss: 0.37094 (0.28275) Loss: 0.37094 (0.28275)
2024-08-29,22:34:10 | INFO | Train Epoch: 28 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 532.549/s, 532.549/s/gpu LR: 8.282986e-09 Logit Scale: 100.000 Contrastive_loss: 0.39376 (0.31976) Loss: 0.39376 (0.31976)
2024-08-29,22:34:29 | INFO | Train Epoch: 28 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 522.653/s, 522.653/s/gpu LR: 8.275081e-09 Logit Scale: 100.000 Contrastive_loss: 0.21270 (0.29299) Loss: 0.21270 (0.29299)
2024-08-29,22:34:49 | INFO | Train Epoch: 28 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 526.702/s, 526.702/s/gpu LR: 8.267161e-09 Logit Scale: 100.000 Contrastive_loss: 0.15114 (0.26462) Loss: 0.15114 (0.26462)
2024-08-29,22:35:08 | INFO | Train Epoch: 28 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.194, 496.903/s, 496.903/s/gpu LR: 8.259226e-09 Logit Scale: 100.000 Contrastive_loss: 0.18691 (0.25167) Loss: 0.18691 (0.25167)
2024-08-29,22:35:27 | INFO | Train Epoch: 28 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 523.172/s, 523.172/s/gpu LR: 8.251278e-09 Logit Scale: 100.000 Contrastive_loss: 0.13381 (0.23483) Loss: 0.13381 (0.23483)
2024-08-29,22:35:46 | INFO | Train Epoch: 28 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 495.282/s, 495.282/s/gpu LR: 8.243315e-09 Logit Scale: 100.000 Contrastive_loss: 0.26200 (0.23823) Loss: 0.26200 (0.23823)
2024-08-29,22:36:06 | INFO | Train Epoch: 28 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 498.408/s, 498.408/s/gpu LR: 8.235338e-09 Logit Scale: 100.000 Contrastive_loss: 0.15884 (0.22941) Loss: 0.15884 (0.22941)
2024-08-29,22:36:25 | INFO | Train Epoch: 28 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 503.467/s, 503.467/s/gpu LR: 8.227346e-09 Logit Scale: 100.000 Contrastive_loss: 0.24059 (0.23053) Loss: 0.24059 (0.23053)
2024-08-29,22:36:44 | INFO | Train Epoch: 28 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 496.935/s, 496.935/s/gpu LR: 8.219341e-09 Logit Scale: 100.000 Contrastive_loss: 0.17221 (0.22522) Loss: 0.17221 (0.22522)
2024-08-29,22:37:04 | INFO | Train Epoch: 28 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.194, 506.243/s, 506.243/s/gpu LR: 8.211321e-09 Logit Scale: 100.000 Contrastive_loss: 0.19815 (0.22297) Loss: 0.19815 (0.22297)
2024-08-29,22:37:23 | INFO | Train Epoch: 28 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 496.814/s, 496.814/s/gpu LR: 8.203288e-09 Logit Scale: 100.000 Contrastive_loss: 0.31010 (0.22967) Loss: 0.31010 (0.22967)
2024-08-29,22:37:42 | INFO | Train Epoch: 28 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 530.170/s, 530.170/s/gpu LR: 8.195240e-09 Logit Scale: 100.000 Contrastive_loss: 0.17259 (0.22559) Loss: 0.17259 (0.22559)
2024-08-29,22:38:02 | INFO | Train Epoch: 28 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 507.874/s, 507.874/s/gpu LR: 8.187178e-09 Logit Scale: 100.000 Contrastive_loss: 0.19572 (0.22360) Loss: 0.19572 (0.22360)
2024-08-29,22:38:11 | INFO | Train Epoch: 28 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 529.092/s, 529.092/s/gpu LR: 8.183223e-09 Logit Scale: 100.000 Contrastive_loss: 0.25319 (0.22545) Loss: 0.25319 (0.22545)
2024-08-29,22:38:21 | INFO | Train Epoch: 28 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 503.580/s, 503.580/s/gpu LR: 8.179102e-09 Logit Scale: 100.000 Contrastive_loss: 0.22907 (0.22566) Loss: 0.22907 (0.22566)
2024-08-29,22:38:40 | INFO | Train Epoch: 28 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 536.097/s, 536.097/s/gpu LR: 8.171013e-09 Logit Scale: 100.000 Contrastive_loss: 0.27180 (0.22823) Loss: 0.27180 (0.22823)
2024-08-29,22:38:59 | INFO | Train Epoch: 28 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 495.133/s, 495.133/s/gpu LR: 8.162909e-09 Logit Scale: 100.000 Contrastive_loss: 0.20133 (0.22681) Loss: 0.20133 (0.22681)
2024-08-29,22:39:19 | INFO | Train Epoch: 28 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.192, 535.757/s, 535.757/s/gpu LR: 8.154792e-09 Logit Scale: 100.000 Contrastive_loss: 0.21635 (0.22629) Loss: 0.21635 (0.22629)
2024-08-29,22:39:38 | INFO | Train Epoch: 28 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 538.156/s, 538.156/s/gpu LR: 8.146660e-09 Logit Scale: 100.000 Contrastive_loss: 0.27483 (0.22860) Loss: 0.27483 (0.22860)
2024-08-29,22:39:57 | INFO | Train Epoch: 28 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 508.259/s, 508.259/s/gpu LR: 8.138515e-09 Logit Scale: 100.000 Contrastive_loss: 0.18240 (0.22650) Loss: 0.18240 (0.22650)
2024-08-29,22:40:17 | INFO | Train Epoch: 28 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 531.245/s, 531.245/s/gpu LR: 8.130356e-09 Logit Scale: 100.000 Contrastive_loss: 0.27736 (0.22871) Loss: 0.27736 (0.22871)
2024-08-29,22:40:36 | INFO | Train Epoch: 28 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 506.191/s, 506.191/s/gpu LR: 8.122184e-09 Logit Scale: 100.000 Contrastive_loss: 0.35567 (0.23400) Loss: 0.35567 (0.23400)
2024-08-29,22:40:55 | INFO | Train Epoch: 28 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 498.006/s, 498.006/s/gpu LR: 8.113997e-09 Logit Scale: 100.000 Contrastive_loss: 0.32507 (0.23764) Loss: 0.32507 (0.23764)
2024-08-29,22:41:14 | INFO | Train Epoch: 28 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 529.959/s, 529.959/s/gpu LR: 8.105797e-09 Logit Scale: 100.000 Contrastive_loss: 0.19435 (0.23598) Loss: 0.19435 (0.23598)
2024-08-29,22:41:34 | INFO | Train Epoch: 28 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 503.032/s, 503.032/s/gpu LR: 8.097584e-09 Logit Scale: 100.000 Contrastive_loss: 0.23657 (0.23600) Loss: 0.23657 (0.23600)
2024-08-29,22:41:53 | INFO | Train Epoch: 28 [260100/145000.0 (179%)] Data (t): 0.107 Batch (t): 0.193, 525.736/s, 525.736/s/gpu LR: 8.089357e-09 Logit Scale: 100.000 Contrastive_loss: 0.24653 (0.23638) Loss: 0.24653 (0.23638)
2024-08-29,22:42:12 | INFO | Train Epoch: 28 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 525.355/s, 525.355/s/gpu LR: 8.081116e-09 Logit Scale: 100.000 Contrastive_loss: 0.27588 (0.23774) Loss: 0.27588 (0.23774)
2024-08-29,22:42:32 | INFO | Train Epoch: 28 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 536.055/s, 536.055/s/gpu LR: 8.072862e-09 Logit Scale: 100.000 Contrastive_loss: 0.19146 (0.23620) Loss: 0.19146 (0.23620)
2024-08-29,22:42:51 | INFO | Train Epoch: 28 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 501.828/s, 501.828/s/gpu LR: 8.064594e-09 Logit Scale: 100.000 Contrastive_loss: 0.15692 (0.23364) Loss: 0.15692 (0.23364)
2024-08-29,22:43:10 | INFO | Train Epoch: 28 [300100/145000.0 (207%)] Data (t): 0.107 Batch (t): 0.192, 529.845/s, 529.845/s/gpu LR: 8.056313e-09 Logit Scale: 100.000 Contrastive_loss: 0.29112 (0.23544) Loss: 0.29112 (0.23544)
2024-08-29,22:43:29 | INFO | Train Epoch: 28 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 528.690/s, 528.690/s/gpu LR: 8.048018e-09 Logit Scale: 100.000 Contrastive_loss: 0.26219 (0.23625) Loss: 0.26219 (0.23625)
2024-08-29,22:43:49 | INFO | Train Epoch: 28 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 524.796/s, 524.796/s/gpu LR: 8.039710e-09 Logit Scale: 100.000 Contrastive_loss: 0.31755 (0.23864) Loss: 0.31755 (0.23864)
2024-08-29,22:44:08 | INFO | Train Epoch: 28 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 527.479/s, 527.479/s/gpu LR: 8.031389e-09 Logit Scale: 100.000 Contrastive_loss: 0.20150 (0.23758) Loss: 0.20150 (0.23758)
2024-08-29,22:44:27 | INFO | Train Epoch: 28 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.193, 500.613/s, 500.613/s/gpu LR: 8.023055e-09 Logit Scale: 100.000 Contrastive_loss: 0.29410 (0.23915) Loss: 0.29410 (0.23915)
2024-08-29,22:44:46 | INFO | Train Epoch: 28 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 502.518/s, 502.518/s/gpu LR: 8.014707e-09 Logit Scale: 100.000 Contrastive_loss: 0.33880 (0.24184) Loss: 0.33880 (0.24184)
2024-08-29,22:45:06 | INFO | Train Epoch: 28 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 505.989/s, 505.989/s/gpu LR: 8.006346e-09 Logit Scale: 100.000 Contrastive_loss: 0.17738 (0.24014) Loss: 0.17738 (0.24014)
2024-08-29,22:45:25 | INFO | Train Epoch: 28 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 529.401/s, 529.401/s/gpu LR: 7.997971e-09 Logit Scale: 100.000 Contrastive_loss: 0.28335 (0.24125) Loss: 0.28335 (0.24125)
2024-08-29,22:45:44 | INFO | Train Epoch: 28 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 532.586/s, 532.586/s/gpu LR: 7.989584e-09 Logit Scale: 100.000 Contrastive_loss: 0.26254 (0.24178) Loss: 0.26254 (0.24178)
2024-08-29,22:46:03 | INFO | Train Epoch: 28 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 526.333/s, 526.333/s/gpu LR: 7.981184e-09 Logit Scale: 100.000 Contrastive_loss: 0.15747 (0.23973) Loss: 0.15747 (0.23973)
2024-08-29,22:46:23 | INFO | Train Epoch: 28 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.193, 492.298/s, 492.298/s/gpu LR: 7.972770e-09 Logit Scale: 100.000 Contrastive_loss: 0.25093 (0.23999) Loss: 0.25093 (0.23999)
2024-08-29,22:46:42 | INFO | Train Epoch: 28 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 497.002/s, 497.002/s/gpu LR: 7.964343e-09 Logit Scale: 100.000 Contrastive_loss: 0.10475 (0.23685) Loss: 0.10475 (0.23685)
2024-08-29,22:46:50 | INFO | Eval Epoch: 29 [200 / 1000]	Clip Loss: 0.485922	
2024-08-29,22:46:50 | INFO | Eval Epoch: 29 image_to_text_mean_rank: 2.4630	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6610	image_to_text_R@5: 0.9100	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.5630	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6800	text_to_image_R@5: 0.8980	text_to_image_R@10: 0.9620	clip_val_loss: 0.4436	epoch: 29.0000	num_samples: 1000.0000
2024-08-29,22:46:52 | INFO | Start epoch 29
2024-08-29,22:46:52 | INFO | Train Epoch: 29 [   100/145000.0 (0%)] Data (t): 0.022 Batch (t): 0.104, 963.991/s, 963.991/s/gpu LR: 8.183142e-09 Logit Scale: 100.000 Contrastive_loss: 0.19307 (0.19307) Loss: 0.19307 (0.19307)
2024-08-29,22:47:11 | INFO | Train Epoch: 29 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 495.166/s, 495.166/s/gpu LR: 8.175059e-09 Logit Scale: 100.000 Contrastive_loss: 0.36919 (0.28113) Loss: 0.36919 (0.28113)
2024-08-29,22:47:30 | INFO | Train Epoch: 29 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 507.124/s, 507.124/s/gpu LR: 8.166963e-09 Logit Scale: 100.000 Contrastive_loss: 0.39147 (0.31791) Loss: 0.39147 (0.31791)
2024-08-29,22:47:50 | INFO | Train Epoch: 29 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 528.488/s, 528.488/s/gpu LR: 8.158852e-09 Logit Scale: 100.000 Contrastive_loss: 0.21108 (0.29120) Loss: 0.21108 (0.29120)
2024-08-29,22:48:09 | INFO | Train Epoch: 29 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 494.287/s, 494.287/s/gpu LR: 8.150728e-09 Logit Scale: 100.000 Contrastive_loss: 0.14997 (0.26296) Loss: 0.14997 (0.26296)
2024-08-29,22:48:28 | INFO | Train Epoch: 29 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 498.540/s, 498.540/s/gpu LR: 8.142589e-09 Logit Scale: 100.000 Contrastive_loss: 0.18548 (0.25004) Loss: 0.18548 (0.25004)
2024-08-29,22:48:48 | INFO | Train Epoch: 29 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 508.174/s, 508.174/s/gpu LR: 8.134437e-09 Logit Scale: 100.000 Contrastive_loss: 0.13285 (0.23330) Loss: 0.13285 (0.23330)
2024-08-29,22:49:07 | INFO | Train Epoch: 29 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 535.441/s, 535.441/s/gpu LR: 8.126272e-09 Logit Scale: 100.000 Contrastive_loss: 0.25968 (0.23660) Loss: 0.25968 (0.23660)
2024-08-29,22:49:26 | INFO | Train Epoch: 29 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 500.737/s, 500.737/s/gpu LR: 8.118092e-09 Logit Scale: 100.000 Contrastive_loss: 0.15717 (0.22777) Loss: 0.15717 (0.22777)
2024-08-29,22:49:46 | INFO | Train Epoch: 29 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 503.496/s, 503.496/s/gpu LR: 8.109899e-09 Logit Scale: 100.000 Contrastive_loss: 0.23904 (0.22890) Loss: 0.23904 (0.22890)
2024-08-29,22:50:05 | INFO | Train Epoch: 29 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.192, 520.943/s, 520.943/s/gpu LR: 8.101692e-09 Logit Scale: 100.000 Contrastive_loss: 0.17106 (0.22364) Loss: 0.17106 (0.22364)
2024-08-29,22:50:24 | INFO | Train Epoch: 29 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.192, 525.049/s, 525.049/s/gpu LR: 8.093472e-09 Logit Scale: 100.000 Contrastive_loss: 0.19650 (0.22138) Loss: 0.19650 (0.22138)
2024-08-29,22:50:43 | INFO | Train Epoch: 29 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 532.774/s, 532.774/s/gpu LR: 8.085238e-09 Logit Scale: 100.000 Contrastive_loss: 0.30763 (0.22802) Loss: 0.30763 (0.22802)
2024-08-29,22:51:03 | INFO | Train Epoch: 29 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 539.780/s, 539.780/s/gpu LR: 8.076991e-09 Logit Scale: 100.000 Contrastive_loss: 0.17127 (0.22396) Loss: 0.17127 (0.22396)
2024-08-29,22:51:22 | INFO | Train Epoch: 29 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 494.053/s, 494.053/s/gpu LR: 8.068730e-09 Logit Scale: 100.000 Contrastive_loss: 0.19468 (0.22201) Loss: 0.19468 (0.22201)
2024-08-29,22:51:31 | INFO | Train Epoch: 29 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 503.026/s, 503.026/s/gpu LR: 8.064677e-09 Logit Scale: 100.000 Contrastive_loss: 0.25158 (0.22386) Loss: 0.25158 (0.22386)
2024-08-29,22:51:41 | INFO | Train Epoch: 29 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 508.741/s, 508.741/s/gpu LR: 8.060455e-09 Logit Scale: 100.000 Contrastive_loss: 0.22706 (0.22405) Loss: 0.22706 (0.22405)
2024-08-29,22:52:00 | INFO | Train Epoch: 29 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.192, 501.161/s, 501.161/s/gpu LR: 8.052167e-09 Logit Scale: 100.000 Contrastive_loss: 0.26927 (0.22656) Loss: 0.26927 (0.22656)
2024-08-29,22:52:20 | INFO | Train Epoch: 29 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 500.311/s, 500.311/s/gpu LR: 8.043866e-09 Logit Scale: 100.000 Contrastive_loss: 0.19956 (0.22514) Loss: 0.19956 (0.22514)
2024-08-29,22:52:39 | INFO | Train Epoch: 29 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 523.033/s, 523.033/s/gpu LR: 8.035551e-09 Logit Scale: 100.000 Contrastive_loss: 0.21525 (0.22464) Loss: 0.21525 (0.22464)
2024-08-29,22:52:58 | INFO | Train Epoch: 29 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 528.917/s, 528.917/s/gpu LR: 8.027223e-09 Logit Scale: 100.000 Contrastive_loss: 0.27303 (0.22695) Loss: 0.27303 (0.22695)
2024-08-29,22:53:17 | INFO | Train Epoch: 29 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 527.747/s, 527.747/s/gpu LR: 8.018882e-09 Logit Scale: 100.000 Contrastive_loss: 0.18152 (0.22488) Loss: 0.18152 (0.22488)
2024-08-29,22:53:37 | INFO | Train Epoch: 29 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 481.456/s, 481.456/s/gpu LR: 8.010528e-09 Logit Scale: 100.000 Contrastive_loss: 0.27580 (0.22710) Loss: 0.27580 (0.22710)
2024-08-29,22:53:56 | INFO | Train Epoch: 29 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 489.730/s, 489.730/s/gpu LR: 8.002160e-09 Logit Scale: 100.000 Contrastive_loss: 0.35290 (0.23234) Loss: 0.35290 (0.23234)
2024-08-29,22:54:15 | INFO | Train Epoch: 29 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 540.163/s, 540.163/s/gpu LR: 7.993779e-09 Logit Scale: 100.000 Contrastive_loss: 0.32379 (0.23600) Loss: 0.32379 (0.23600)
2024-08-29,22:54:35 | INFO | Train Epoch: 29 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 533.964/s, 533.964/s/gpu LR: 7.985385e-09 Logit Scale: 100.000 Contrastive_loss: 0.19265 (0.23433) Loss: 0.19265 (0.23433)
2024-08-29,22:54:54 | INFO | Train Epoch: 29 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 535.909/s, 535.909/s/gpu LR: 7.976978e-09 Logit Scale: 100.000 Contrastive_loss: 0.23554 (0.23437) Loss: 0.23554 (0.23437)
2024-08-29,22:55:13 | INFO | Train Epoch: 29 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.194, 496.629/s, 496.629/s/gpu LR: 7.968558e-09 Logit Scale: 100.000 Contrastive_loss: 0.24474 (0.23474) Loss: 0.24474 (0.23474)
2024-08-29,22:55:33 | INFO | Train Epoch: 29 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 499.975/s, 499.975/s/gpu LR: 7.960125e-09 Logit Scale: 100.000 Contrastive_loss: 0.27481 (0.23613) Loss: 0.27481 (0.23613)
2024-08-29,22:55:52 | INFO | Train Epoch: 29 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 505.094/s, 505.094/s/gpu LR: 7.951679e-09 Logit Scale: 100.000 Contrastive_loss: 0.19003 (0.23459) Loss: 0.19003 (0.23459)
2024-08-29,22:56:11 | INFO | Train Epoch: 29 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 529.625/s, 529.625/s/gpu LR: 7.943220e-09 Logit Scale: 100.000 Contrastive_loss: 0.15610 (0.23206) Loss: 0.15610 (0.23206)
2024-08-29,22:56:30 | INFO | Train Epoch: 29 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 526.094/s, 526.094/s/gpu LR: 7.934748e-09 Logit Scale: 100.000 Contrastive_loss: 0.28977 (0.23386) Loss: 0.28977 (0.23386)
2024-08-29,22:56:50 | INFO | Train Epoch: 29 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 529.305/s, 529.305/s/gpu LR: 7.926263e-09 Logit Scale: 100.000 Contrastive_loss: 0.26102 (0.23468) Loss: 0.26102 (0.23468)
2024-08-29,22:57:09 | INFO | Train Epoch: 29 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 521.431/s, 521.431/s/gpu LR: 7.917765e-09 Logit Scale: 100.000 Contrastive_loss: 0.31545 (0.23706) Loss: 0.31545 (0.23706)
2024-08-29,22:57:28 | INFO | Train Epoch: 29 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 526.673/s, 526.673/s/gpu LR: 7.909255e-09 Logit Scale: 100.000 Contrastive_loss: 0.20056 (0.23602) Loss: 0.20056 (0.23602)
2024-08-29,22:57:48 | INFO | Train Epoch: 29 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.192, 530.850/s, 530.850/s/gpu LR: 7.900732e-09 Logit Scale: 100.000 Contrastive_loss: 0.29153 (0.23756) Loss: 0.29153 (0.23756)
2024-08-29,22:58:07 | INFO | Train Epoch: 29 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.193, 492.511/s, 492.511/s/gpu LR: 7.892196e-09 Logit Scale: 100.000 Contrastive_loss: 0.33592 (0.24022) Loss: 0.33592 (0.24022)
2024-08-29,22:58:26 | INFO | Train Epoch: 29 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.192, 491.626/s, 491.626/s/gpu LR: 7.883647e-09 Logit Scale: 100.000 Contrastive_loss: 0.17623 (0.23853) Loss: 0.17623 (0.23853)
2024-08-29,22:58:45 | INFO | Train Epoch: 29 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 519.400/s, 519.400/s/gpu LR: 7.875086e-09 Logit Scale: 100.000 Contrastive_loss: 0.28203 (0.23965) Loss: 0.28203 (0.23965)
2024-08-29,22:59:05 | INFO | Train Epoch: 29 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 494.650/s, 494.650/s/gpu LR: 7.866512e-09 Logit Scale: 100.000 Contrastive_loss: 0.26093 (0.24018) Loss: 0.26093 (0.24018)
2024-08-29,22:59:24 | INFO | Train Epoch: 29 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 538.122/s, 538.122/s/gpu LR: 7.857925e-09 Logit Scale: 100.000 Contrastive_loss: 0.15589 (0.23812) Loss: 0.15589 (0.23812)
2024-08-29,22:59:43 | INFO | Train Epoch: 29 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 528.589/s, 528.589/s/gpu LR: 7.849326e-09 Logit Scale: 100.000 Contrastive_loss: 0.24932 (0.23839) Loss: 0.24932 (0.23839)
2024-08-29,23:00:03 | INFO | Train Epoch: 29 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 535.965/s, 535.965/s/gpu LR: 7.840715e-09 Logit Scale: 100.000 Contrastive_loss: 0.10387 (0.23526) Loss: 0.10387 (0.23526)
2024-08-29,23:00:10 | INFO | Eval Epoch: 30 [200 / 1000]	Clip Loss: 0.485396	
2024-08-29,23:00:11 | INFO | Eval Epoch: 30 image_to_text_mean_rank: 2.4620	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6610	image_to_text_R@5: 0.9110	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.5830	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6780	text_to_image_R@5: 0.9000	text_to_image_R@10: 0.9640	clip_val_loss: 0.4431	epoch: 30.0000	num_samples: 1000.0000
2024-08-29,23:00:12 | INFO | Start epoch 30
2024-08-29,23:00:12 | INFO | Train Epoch: 30 [   100/145000.0 (0%)] Data (t): 0.023 Batch (t): 0.111, 897.740/s, 897.740/s/gpu LR: 8.064594e-09 Logit Scale: 100.000 Contrastive_loss: 0.19171 (0.19171) Loss: 0.19171 (0.19171)
2024-08-29,23:00:32 | INFO | Train Epoch: 30 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 492.077/s, 492.077/s/gpu LR: 8.056313e-09 Logit Scale: 100.000 Contrastive_loss: 0.36723 (0.27947) Loss: 0.36723 (0.27947)
2024-08-29,23:00:51 | INFO | Train Epoch: 30 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.194, 534.828/s, 534.828/s/gpu LR: 8.048018e-09 Logit Scale: 100.000 Contrastive_loss: 0.38894 (0.31596) Loss: 0.38894 (0.31596)
2024-08-29,23:01:10 | INFO | Train Epoch: 30 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 509.954/s, 509.954/s/gpu LR: 8.039710e-09 Logit Scale: 100.000 Contrastive_loss: 0.20947 (0.28934) Loss: 0.20947 (0.28934)
2024-08-29,23:01:30 | INFO | Train Epoch: 30 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.192, 527.518/s, 527.518/s/gpu LR: 8.031389e-09 Logit Scale: 100.000 Contrastive_loss: 0.14905 (0.26128) Loss: 0.14905 (0.26128)
2024-08-29,23:01:49 | INFO | Train Epoch: 30 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 495.123/s, 495.123/s/gpu LR: 8.023055e-09 Logit Scale: 100.000 Contrastive_loss: 0.18403 (0.24840) Loss: 0.18403 (0.24840)
2024-08-29,23:02:08 | INFO | Train Epoch: 30 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.192, 536.241/s, 536.241/s/gpu LR: 8.014707e-09 Logit Scale: 100.000 Contrastive_loss: 0.13184 (0.23175) Loss: 0.13184 (0.23175)
2024-08-29,23:02:27 | INFO | Train Epoch: 30 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 525.953/s, 525.953/s/gpu LR: 8.006346e-09 Logit Scale: 100.000 Contrastive_loss: 0.25758 (0.23498) Loss: 0.25758 (0.23498)
2024-08-29,23:02:47 | INFO | Train Epoch: 30 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 529.687/s, 529.687/s/gpu LR: 7.997971e-09 Logit Scale: 100.000 Contrastive_loss: 0.15566 (0.22617) Loss: 0.15566 (0.22617)
2024-08-29,23:03:06 | INFO | Train Epoch: 30 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 535.485/s, 535.485/s/gpu LR: 7.989584e-09 Logit Scale: 100.000 Contrastive_loss: 0.23746 (0.22730) Loss: 0.23746 (0.22730)
2024-08-29,23:03:25 | INFO | Train Epoch: 30 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.192, 529.053/s, 529.053/s/gpu LR: 7.981184e-09 Logit Scale: 100.000 Contrastive_loss: 0.16990 (0.22208) Loss: 0.16990 (0.22208)
2024-08-29,23:03:44 | INFO | Train Epoch: 30 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 528.565/s, 528.565/s/gpu LR: 7.972770e-09 Logit Scale: 100.000 Contrastive_loss: 0.19495 (0.21982) Loss: 0.19495 (0.21982)
2024-08-29,23:04:04 | INFO | Train Epoch: 30 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 522.324/s, 522.324/s/gpu LR: 7.964343e-09 Logit Scale: 100.000 Contrastive_loss: 0.30549 (0.22641) Loss: 0.30549 (0.22641)
2024-08-29,23:04:23 | INFO | Train Epoch: 30 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 527.795/s, 527.795/s/gpu LR: 7.955904e-09 Logit Scale: 100.000 Contrastive_loss: 0.17007 (0.22238) Loss: 0.17007 (0.22238)
2024-08-29,23:04:42 | INFO | Train Epoch: 30 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.192, 560.051/s, 560.051/s/gpu LR: 7.947451e-09 Logit Scale: 100.000 Contrastive_loss: 0.19360 (0.22046) Loss: 0.19360 (0.22046)
2024-08-29,23:04:52 | INFO | Train Epoch: 30 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.192, 542.693/s, 542.693/s/gpu LR: 7.943305e-09 Logit Scale: 100.000 Contrastive_loss: 0.25006 (0.22231) Loss: 0.25006 (0.22231)
2024-08-29,23:05:02 | INFO | Train Epoch: 30 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.192, 531.005/s, 531.005/s/gpu LR: 7.938986e-09 Logit Scale: 100.000 Contrastive_loss: 0.22528 (0.22249) Loss: 0.22528 (0.22249)
2024-08-29,23:05:21 | INFO | Train Epoch: 30 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.192, 528.581/s, 528.581/s/gpu LR: 7.930507e-09 Logit Scale: 100.000 Contrastive_loss: 0.26685 (0.22495) Loss: 0.26685 (0.22495)
2024-08-29,23:05:40 | INFO | Train Epoch: 30 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 529.141/s, 529.141/s/gpu LR: 7.922016e-09 Logit Scale: 100.000 Contrastive_loss: 0.19788 (0.22353) Loss: 0.19788 (0.22353)
2024-08-29,23:05:59 | INFO | Train Epoch: 30 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 495.835/s, 495.835/s/gpu LR: 7.913512e-09 Logit Scale: 100.000 Contrastive_loss: 0.21407 (0.22306) Loss: 0.21407 (0.22306)
2024-08-29,23:06:19 | INFO | Train Epoch: 30 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 526.827/s, 526.827/s/gpu LR: 7.904995e-09 Logit Scale: 100.000 Contrastive_loss: 0.27120 (0.22535) Loss: 0.27120 (0.22535)
2024-08-29,23:06:38 | INFO | Train Epoch: 30 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.192, 533.300/s, 533.300/s/gpu LR: 7.896465e-09 Logit Scale: 100.000 Contrastive_loss: 0.18059 (0.22331) Loss: 0.18059 (0.22331)
2024-08-29,23:06:57 | INFO | Train Epoch: 30 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 536.775/s, 536.775/s/gpu LR: 7.887923e-09 Logit Scale: 100.000 Contrastive_loss: 0.27433 (0.22553) Loss: 0.27433 (0.22553)
2024-08-29,23:07:16 | INFO | Train Epoch: 30 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 495.873/s, 495.873/s/gpu LR: 7.879368e-09 Logit Scale: 100.000 Contrastive_loss: 0.35012 (0.23072) Loss: 0.35012 (0.23072)
2024-08-29,23:07:36 | INFO | Train Epoch: 30 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 493.614/s, 493.614/s/gpu LR: 7.870800e-09 Logit Scale: 100.000 Contrastive_loss: 0.32248 (0.23439) Loss: 0.32248 (0.23439)
2024-08-29,23:07:55 | INFO | Train Epoch: 30 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 501.373/s, 501.373/s/gpu LR: 7.862220e-09 Logit Scale: 100.000 Contrastive_loss: 0.19110 (0.23273) Loss: 0.19110 (0.23273)
2024-08-29,23:08:14 | INFO | Train Epoch: 30 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.192, 503.138/s, 503.138/s/gpu LR: 7.853627e-09 Logit Scale: 100.000 Contrastive_loss: 0.23416 (0.23278) Loss: 0.23416 (0.23278)
2024-08-29,23:08:33 | INFO | Train Epoch: 30 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.192, 535.589/s, 535.589/s/gpu LR: 7.845022e-09 Logit Scale: 100.000 Contrastive_loss: 0.24304 (0.23315) Loss: 0.24304 (0.23315)
2024-08-29,23:08:53 | INFO | Train Epoch: 30 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 494.844/s, 494.844/s/gpu LR: 7.836404e-09 Logit Scale: 100.000 Contrastive_loss: 0.27369 (0.23455) Loss: 0.27369 (0.23455)
2024-08-29,23:09:12 | INFO | Train Epoch: 30 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 529.794/s, 529.794/s/gpu LR: 7.827774e-09 Logit Scale: 100.000 Contrastive_loss: 0.18861 (0.23301) Loss: 0.18861 (0.23301)
2024-08-29,23:09:31 | INFO | Train Epoch: 30 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 510.650/s, 510.650/s/gpu LR: 7.819132e-09 Logit Scale: 100.000 Contrastive_loss: 0.15541 (0.23051) Loss: 0.15541 (0.23051)
2024-08-29,23:09:51 | INFO | Train Epoch: 30 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 499.206/s, 499.206/s/gpu LR: 7.810477e-09 Logit Scale: 100.000 Contrastive_loss: 0.28815 (0.23231) Loss: 0.28815 (0.23231)
2024-08-29,23:10:10 | INFO | Train Epoch: 30 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 531.393/s, 531.393/s/gpu LR: 7.801809e-09 Logit Scale: 100.000 Contrastive_loss: 0.25979 (0.23315) Loss: 0.25979 (0.23315)
2024-08-29,23:10:29 | INFO | Train Epoch: 30 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 490.515/s, 490.515/s/gpu LR: 7.793130e-09 Logit Scale: 100.000 Contrastive_loss: 0.31328 (0.23550) Loss: 0.31328 (0.23550)
2024-08-29,23:10:49 | INFO | Train Epoch: 30 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 509.236/s, 509.236/s/gpu LR: 7.784438e-09 Logit Scale: 100.000 Contrastive_loss: 0.19964 (0.23448) Loss: 0.19964 (0.23448)
2024-08-29,23:11:08 | INFO | Train Epoch: 30 [340100/145000.0 (235%)] Data (t): 0.107 Batch (t): 0.192, 522.262/s, 522.262/s/gpu LR: 7.775734e-09 Logit Scale: 100.000 Contrastive_loss: 0.28914 (0.23600) Loss: 0.28914 (0.23600)
2024-08-29,23:11:27 | INFO | Train Epoch: 30 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 536.871/s, 536.871/s/gpu LR: 7.767018e-09 Logit Scale: 100.000 Contrastive_loss: 0.33335 (0.23863) Loss: 0.33335 (0.23863)
2024-08-29,23:11:46 | INFO | Train Epoch: 30 [360100/145000.0 (248%)] Data (t): 0.107 Batch (t): 0.193, 527.148/s, 527.148/s/gpu LR: 7.758289e-09 Logit Scale: 100.000 Contrastive_loss: 0.17499 (0.23695) Loss: 0.17499 (0.23695)
2024-08-29,23:12:05 | INFO | Train Epoch: 30 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.192, 522.770/s, 522.770/s/gpu LR: 7.749549e-09 Logit Scale: 100.000 Contrastive_loss: 0.28067 (0.23807) Loss: 0.28067 (0.23807)
2024-08-29,23:12:25 | INFO | Train Epoch: 30 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 527.838/s, 527.838/s/gpu LR: 7.740796e-09 Logit Scale: 100.000 Contrastive_loss: 0.25929 (0.23860) Loss: 0.25929 (0.23860)
2024-08-29,23:12:44 | INFO | Train Epoch: 30 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 509.301/s, 509.301/s/gpu LR: 7.732032e-09 Logit Scale: 100.000 Contrastive_loss: 0.15432 (0.23655) Loss: 0.15432 (0.23655)
2024-08-29,23:13:03 | INFO | Train Epoch: 30 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 527.990/s, 527.990/s/gpu LR: 7.723255e-09 Logit Scale: 100.000 Contrastive_loss: 0.24780 (0.23682) Loss: 0.24780 (0.23682)
2024-08-29,23:13:23 | INFO | Train Epoch: 30 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.192, 507.088/s, 507.088/s/gpu LR: 7.714467e-09 Logit Scale: 100.000 Contrastive_loss: 0.10283 (0.23370) Loss: 0.10283 (0.23370)
2024-08-29,23:13:30 | INFO | Eval Epoch: 31 [200 / 1000]	Clip Loss: 0.484909	
2024-08-29,23:13:31 | INFO | Eval Epoch: 31 image_to_text_mean_rank: 2.4570	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6610	image_to_text_R@5: 0.9110	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.6010	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6770	text_to_image_R@5: 0.9000	text_to_image_R@10: 0.9650	clip_val_loss: 0.4426	epoch: 31.0000	num_samples: 1000.0000
2024-08-29,23:13:32 | INFO | Start epoch 31
2024-08-29,23:13:32 | INFO | Train Epoch: 31 [   100/145000.0 (0%)] Data (t): 0.029 Batch (t): 0.115, 872.644/s, 872.644/s/gpu LR: 7.943220e-09 Logit Scale: 100.000 Contrastive_loss: 0.19033 (0.19033) Loss: 0.19033 (0.19033)
2024-08-29,23:13:52 | INFO | Train Epoch: 31 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 532.757/s, 532.757/s/gpu LR: 7.934748e-09 Logit Scale: 100.000 Contrastive_loss: 0.36538 (0.27786) Loss: 0.36538 (0.27786)
2024-08-29,23:14:11 | INFO | Train Epoch: 31 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 493.324/s, 493.324/s/gpu LR: 7.926263e-09 Logit Scale: 100.000 Contrastive_loss: 0.38676 (0.31416) Loss: 0.38676 (0.31416)
2024-08-29,23:14:30 | INFO | Train Epoch: 31 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 538.056/s, 538.056/s/gpu LR: 7.917765e-09 Logit Scale: 100.000 Contrastive_loss: 0.20786 (0.28758) Loss: 0.20786 (0.28758)
2024-08-29,23:14:50 | INFO | Train Epoch: 31 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 536.490/s, 536.490/s/gpu LR: 7.909255e-09 Logit Scale: 100.000 Contrastive_loss: 0.14806 (0.25968) Loss: 0.14806 (0.25968)
2024-08-29,23:15:09 | INFO | Train Epoch: 31 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 499.930/s, 499.930/s/gpu LR: 7.900732e-09 Logit Scale: 100.000 Contrastive_loss: 0.18269 (0.24685) Loss: 0.18269 (0.24685)
2024-08-29,23:15:28 | INFO | Train Epoch: 31 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 500.126/s, 500.126/s/gpu LR: 7.892196e-09 Logit Scale: 100.000 Contrastive_loss: 0.13097 (0.23029) Loss: 0.13097 (0.23029)
2024-08-29,23:15:48 | INFO | Train Epoch: 31 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 502.848/s, 502.848/s/gpu LR: 7.883647e-09 Logit Scale: 100.000 Contrastive_loss: 0.25551 (0.23345) Loss: 0.25551 (0.23345)
2024-08-29,23:16:07 | INFO | Train Epoch: 31 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 495.270/s, 495.270/s/gpu LR: 7.875086e-09 Logit Scale: 100.000 Contrastive_loss: 0.15407 (0.22463) Loss: 0.15407 (0.22463)
2024-08-29,23:16:26 | INFO | Train Epoch: 31 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.192, 499.364/s, 499.364/s/gpu LR: 7.866512e-09 Logit Scale: 100.000 Contrastive_loss: 0.23616 (0.22578) Loss: 0.23616 (0.22578)
2024-08-29,23:16:45 | INFO | Train Epoch: 31 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 529.095/s, 529.095/s/gpu LR: 7.857925e-09 Logit Scale: 100.000 Contrastive_loss: 0.16878 (0.22060) Loss: 0.16878 (0.22060)
2024-08-29,23:17:05 | INFO | Train Epoch: 31 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 501.264/s, 501.264/s/gpu LR: 7.849326e-09 Logit Scale: 100.000 Contrastive_loss: 0.19349 (0.21834) Loss: 0.19349 (0.21834)
2024-08-29,23:17:24 | INFO | Train Epoch: 31 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.192, 528.328/s, 528.328/s/gpu LR: 7.840715e-09 Logit Scale: 100.000 Contrastive_loss: 0.30320 (0.22487) Loss: 0.30320 (0.22487)
2024-08-29,23:17:43 | INFO | Train Epoch: 31 [130100/145000.0 (90%)] Data (t): 0.107 Batch (t): 0.192, 529.974/s, 529.974/s/gpu LR: 7.832091e-09 Logit Scale: 100.000 Contrastive_loss: 0.16885 (0.22087) Loss: 0.16885 (0.22087)
2024-08-29,23:18:02 | INFO | Train Epoch: 31 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 521.073/s, 521.073/s/gpu LR: 7.823454e-09 Logit Scale: 100.000 Contrastive_loss: 0.19265 (0.21898) Loss: 0.19265 (0.21898)
2024-08-29,23:18:12 | INFO | Train Epoch: 31 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.192, 530.412/s, 530.412/s/gpu LR: 7.819218e-09 Logit Scale: 100.000 Contrastive_loss: 0.24854 (0.22083) Loss: 0.24854 (0.22083)
2024-08-29,23:18:22 | INFO | Train Epoch: 31 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 516.538/s, 516.538/s/gpu LR: 7.814806e-09 Logit Scale: 100.000 Contrastive_loss: 0.22353 (0.22099) Loss: 0.22353 (0.22099)
2024-08-29,23:18:41 | INFO | Train Epoch: 31 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.192, 531.933/s, 531.933/s/gpu LR: 7.806145e-09 Logit Scale: 100.000 Contrastive_loss: 0.26431 (0.22340) Loss: 0.26431 (0.22340)
2024-08-29,23:19:00 | INFO | Train Epoch: 31 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 530.798/s, 530.798/s/gpu LR: 7.797471e-09 Logit Scale: 100.000 Contrastive_loss: 0.19624 (0.22197) Loss: 0.19624 (0.22197)
2024-08-29,23:19:19 | INFO | Train Epoch: 31 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 493.961/s, 493.961/s/gpu LR: 7.788785e-09 Logit Scale: 100.000 Contrastive_loss: 0.21299 (0.22152) Loss: 0.21299 (0.22152)
2024-08-29,23:19:39 | INFO | Train Epoch: 31 [190100/145000.0 (131%)] Data (t): 0.107 Batch (t): 0.192, 518.615/s, 518.615/s/gpu LR: 7.780087e-09 Logit Scale: 100.000 Contrastive_loss: 0.26952 (0.22381) Loss: 0.26952 (0.22381)
2024-08-29,23:19:58 | INFO | Train Epoch: 31 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 521.629/s, 521.629/s/gpu LR: 7.771377e-09 Logit Scale: 100.000 Contrastive_loss: 0.17976 (0.22180) Loss: 0.17976 (0.22180)
2024-08-29,23:20:17 | INFO | Train Epoch: 31 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 528.191/s, 528.191/s/gpu LR: 7.762655e-09 Logit Scale: 100.000 Contrastive_loss: 0.27292 (0.22403) Loss: 0.27292 (0.22403)
2024-08-29,23:20:36 | INFO | Train Epoch: 31 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.192, 503.602/s, 503.602/s/gpu LR: 7.753921e-09 Logit Scale: 100.000 Contrastive_loss: 0.34750 (0.22917) Loss: 0.34750 (0.22917)
2024-08-29,23:20:55 | INFO | Train Epoch: 31 [230100/145000.0 (159%)] Data (t): 0.107 Batch (t): 0.192, 504.635/s, 504.635/s/gpu LR: 7.745174e-09 Logit Scale: 100.000 Contrastive_loss: 0.32137 (0.23286) Loss: 0.32137 (0.23286)
2024-08-29,23:21:15 | INFO | Train Epoch: 31 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 510.507/s, 510.507/s/gpu LR: 7.736416e-09 Logit Scale: 100.000 Contrastive_loss: 0.18947 (0.23119) Loss: 0.18947 (0.23119)
2024-08-29,23:21:34 | INFO | Train Epoch: 31 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 508.195/s, 508.195/s/gpu LR: 7.727645e-09 Logit Scale: 100.000 Contrastive_loss: 0.23313 (0.23126) Loss: 0.23313 (0.23126)
2024-08-29,23:21:53 | INFO | Train Epoch: 31 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.192, 501.165/s, 501.165/s/gpu LR: 7.718862e-09 Logit Scale: 100.000 Contrastive_loss: 0.24138 (0.23162) Loss: 0.24138 (0.23162)
2024-08-29,23:22:12 | INFO | Train Epoch: 31 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 520.596/s, 520.596/s/gpu LR: 7.710068e-09 Logit Scale: 100.000 Contrastive_loss: 0.27277 (0.23304) Loss: 0.27277 (0.23304)
2024-08-29,23:22:32 | INFO | Train Epoch: 31 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 534.123/s, 534.123/s/gpu LR: 7.701262e-09 Logit Scale: 100.000 Contrastive_loss: 0.18749 (0.23152) Loss: 0.18749 (0.23152)
2024-08-29,23:22:51 | INFO | Train Epoch: 31 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.193, 517.019/s, 517.019/s/gpu LR: 7.692443e-09 Logit Scale: 100.000 Contrastive_loss: 0.15472 (0.22905) Loss: 0.15472 (0.22905)
2024-08-29,23:23:10 | INFO | Train Epoch: 31 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 496.105/s, 496.105/s/gpu LR: 7.683614e-09 Logit Scale: 100.000 Contrastive_loss: 0.28659 (0.23084) Loss: 0.28659 (0.23084)
2024-08-29,23:23:29 | INFO | Train Epoch: 31 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 497.422/s, 497.422/s/gpu LR: 7.674772e-09 Logit Scale: 100.000 Contrastive_loss: 0.25865 (0.23169) Loss: 0.25865 (0.23169)
2024-08-29,23:23:49 | INFO | Train Epoch: 31 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 518.645/s, 518.645/s/gpu LR: 7.665918e-09 Logit Scale: 100.000 Contrastive_loss: 0.31118 (0.23402) Loss: 0.31118 (0.23402)
2024-08-29,23:24:08 | INFO | Train Epoch: 31 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 507.210/s, 507.210/s/gpu LR: 7.657053e-09 Logit Scale: 100.000 Contrastive_loss: 0.19887 (0.23302) Loss: 0.19887 (0.23302)
2024-08-29,23:24:27 | INFO | Train Epoch: 31 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 533.607/s, 533.607/s/gpu LR: 7.648176e-09 Logit Scale: 100.000 Contrastive_loss: 0.28649 (0.23451) Loss: 0.28649 (0.23451)
2024-08-29,23:24:46 | INFO | Train Epoch: 31 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.193, 526.168/s, 526.168/s/gpu LR: 7.639288e-09 Logit Scale: 100.000 Contrastive_loss: 0.33080 (0.23711) Loss: 0.33080 (0.23711)
2024-08-29,23:25:06 | INFO | Train Epoch: 31 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.192, 537.502/s, 537.502/s/gpu LR: 7.630388e-09 Logit Scale: 100.000 Contrastive_loss: 0.17385 (0.23544) Loss: 0.17385 (0.23544)
2024-08-29,23:25:25 | INFO | Train Epoch: 31 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 499.909/s, 499.909/s/gpu LR: 7.621476e-09 Logit Scale: 100.000 Contrastive_loss: 0.27943 (0.23657) Loss: 0.27943 (0.23657)
2024-08-29,23:25:44 | INFO | Train Epoch: 31 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 494.406/s, 494.406/s/gpu LR: 7.612553e-09 Logit Scale: 100.000 Contrastive_loss: 0.25774 (0.23710) Loss: 0.25774 (0.23710)
2024-08-29,23:26:04 | INFO | Train Epoch: 31 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 493.389/s, 493.389/s/gpu LR: 7.603619e-09 Logit Scale: 100.000 Contrastive_loss: 0.15282 (0.23504) Loss: 0.15282 (0.23504)
2024-08-29,23:26:23 | INFO | Train Epoch: 31 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.193, 494.693/s, 494.693/s/gpu LR: 7.594673e-09 Logit Scale: 100.000 Contrastive_loss: 0.24632 (0.23531) Loss: 0.24632 (0.23531)
2024-08-29,23:26:42 | INFO | Train Epoch: 31 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 534.986/s, 534.986/s/gpu LR: 7.585715e-09 Logit Scale: 100.000 Contrastive_loss: 0.10214 (0.23222) Loss: 0.10214 (0.23222)
2024-08-29,23:26:50 | INFO | Eval Epoch: 32 [200 / 1000]	Clip Loss: 0.484422	
2024-08-29,23:26:50 | INFO | Eval Epoch: 32 image_to_text_mean_rank: 2.4540	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6610	image_to_text_R@5: 0.9110	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.6170	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6770	text_to_image_R@5: 0.9010	text_to_image_R@10: 0.9650	clip_val_loss: 0.4422	epoch: 32.0000	num_samples: 1000.0000
2024-08-29,23:26:52 | INFO | Start epoch 32
2024-08-29,23:26:52 | INFO | Train Epoch: 32 [   100/145000.0 (0%)] Data (t): 0.027 Batch (t): 0.110, 909.163/s, 909.163/s/gpu LR: 7.819132e-09 Logit Scale: 100.000 Contrastive_loss: 0.18912 (0.18912) Loss: 0.18912 (0.18912)
2024-08-29,23:27:11 | INFO | Train Epoch: 32 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 535.610/s, 535.610/s/gpu LR: 7.810477e-09 Logit Scale: 100.000 Contrastive_loss: 0.36354 (0.27633) Loss: 0.36354 (0.27633)
2024-08-29,23:27:31 | INFO | Train Epoch: 32 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 500.919/s, 500.919/s/gpu LR: 7.801809e-09 Logit Scale: 100.000 Contrastive_loss: 0.38454 (0.31240) Loss: 0.38454 (0.31240)
2024-08-29,23:27:50 | INFO | Train Epoch: 32 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 498.299/s, 498.299/s/gpu LR: 7.793130e-09 Logit Scale: 100.000 Contrastive_loss: 0.20648 (0.28592) Loss: 0.20648 (0.28592)
2024-08-29,23:28:09 | INFO | Train Epoch: 32 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 497.057/s, 497.057/s/gpu LR: 7.784438e-09 Logit Scale: 100.000 Contrastive_loss: 0.14712 (0.25816) Loss: 0.14712 (0.25816)
2024-08-29,23:28:28 | INFO | Train Epoch: 32 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.192, 497.857/s, 497.857/s/gpu LR: 7.775734e-09 Logit Scale: 100.000 Contrastive_loss: 0.18130 (0.24535) Loss: 0.18130 (0.24535)
2024-08-29,23:28:48 | INFO | Train Epoch: 32 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 544.880/s, 544.880/s/gpu LR: 7.767018e-09 Logit Scale: 100.000 Contrastive_loss: 0.13006 (0.22888) Loss: 0.13006 (0.22888)
2024-08-29,23:29:07 | INFO | Train Epoch: 32 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 533.259/s, 533.259/s/gpu LR: 7.758289e-09 Logit Scale: 100.000 Contrastive_loss: 0.25329 (0.23193) Loss: 0.25329 (0.23193)
2024-08-29,23:29:26 | INFO | Train Epoch: 32 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 499.132/s, 499.132/s/gpu LR: 7.749549e-09 Logit Scale: 100.000 Contrastive_loss: 0.15251 (0.22311) Loss: 0.15251 (0.22311)
2024-08-29,23:29:46 | INFO | Train Epoch: 32 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.192, 527.445/s, 527.445/s/gpu LR: 7.740796e-09 Logit Scale: 100.000 Contrastive_loss: 0.23461 (0.22426) Loss: 0.23461 (0.22426)
2024-08-29,23:30:05 | INFO | Train Epoch: 32 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 534.018/s, 534.018/s/gpu LR: 7.732032e-09 Logit Scale: 100.000 Contrastive_loss: 0.16774 (0.21912) Loss: 0.16774 (0.21912)
2024-08-29,23:30:24 | INFO | Train Epoch: 32 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.192, 501.232/s, 501.232/s/gpu LR: 7.723255e-09 Logit Scale: 100.000 Contrastive_loss: 0.19207 (0.21687) Loss: 0.19207 (0.21687)
2024-08-29,23:30:43 | INFO | Train Epoch: 32 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 521.280/s, 521.280/s/gpu LR: 7.714467e-09 Logit Scale: 100.000 Contrastive_loss: 0.30104 (0.22334) Loss: 0.30104 (0.22334)
2024-08-29,23:31:03 | INFO | Train Epoch: 32 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 496.790/s, 496.790/s/gpu LR: 7.705666e-09 Logit Scale: 100.000 Contrastive_loss: 0.16778 (0.21937) Loss: 0.16778 (0.21937)
2024-08-29,23:31:22 | INFO | Train Epoch: 32 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 495.258/s, 495.258/s/gpu LR: 7.696854e-09 Logit Scale: 100.000 Contrastive_loss: 0.19167 (0.21753) Loss: 0.19167 (0.21753)
2024-08-29,23:31:32 | INFO | Train Epoch: 32 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.194, 532.480/s, 532.480/s/gpu LR: 7.692532e-09 Logit Scale: 100.000 Contrastive_loss: 0.24702 (0.21937) Loss: 0.24702 (0.21937)
2024-08-29,23:31:41 | INFO | Train Epoch: 32 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.194, 534.594/s, 534.594/s/gpu LR: 7.688030e-09 Logit Scale: 100.000 Contrastive_loss: 0.22169 (0.21951) Loss: 0.22169 (0.21951)
2024-08-29,23:32:01 | INFO | Train Epoch: 32 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 511.458/s, 511.458/s/gpu LR: 7.679194e-09 Logit Scale: 100.000 Contrastive_loss: 0.26184 (0.22186) Loss: 0.26184 (0.22186)
2024-08-29,23:32:20 | INFO | Train Epoch: 32 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 530.513/s, 530.513/s/gpu LR: 7.670346e-09 Logit Scale: 100.000 Contrastive_loss: 0.19477 (0.22043) Loss: 0.19477 (0.22043)
2024-08-29,23:32:39 | INFO | Train Epoch: 32 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 508.639/s, 508.639/s/gpu LR: 7.661487e-09 Logit Scale: 100.000 Contrastive_loss: 0.21194 (0.22001) Loss: 0.21194 (0.22001)
2024-08-29,23:32:59 | INFO | Train Epoch: 32 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 520.551/s, 520.551/s/gpu LR: 7.652616e-09 Logit Scale: 100.000 Contrastive_loss: 0.26767 (0.22228) Loss: 0.26767 (0.22228)
2024-08-29,23:33:18 | INFO | Train Epoch: 32 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 494.236/s, 494.236/s/gpu LR: 7.643733e-09 Logit Scale: 100.000 Contrastive_loss: 0.17875 (0.22030) Loss: 0.17875 (0.22030)
2024-08-29,23:33:37 | INFO | Train Epoch: 32 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 498.813/s, 498.813/s/gpu LR: 7.634839e-09 Logit Scale: 100.000 Contrastive_loss: 0.27143 (0.22252) Loss: 0.27143 (0.22252)
2024-08-29,23:33:57 | INFO | Train Epoch: 32 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 510.319/s, 510.319/s/gpu LR: 7.625933e-09 Logit Scale: 100.000 Contrastive_loss: 0.34471 (0.22761) Loss: 0.34471 (0.22761)
2024-08-29,23:34:16 | INFO | Train Epoch: 32 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 490.853/s, 490.853/s/gpu LR: 7.617016e-09 Logit Scale: 100.000 Contrastive_loss: 0.32016 (0.23131) Loss: 0.32016 (0.23131)
2024-08-29,23:34:35 | INFO | Train Epoch: 32 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 533.072/s, 533.072/s/gpu LR: 7.608087e-09 Logit Scale: 100.000 Contrastive_loss: 0.18781 (0.22964) Loss: 0.18781 (0.22964)
2024-08-29,23:34:54 | INFO | Train Epoch: 32 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.192, 541.515/s, 541.515/s/gpu LR: 7.599147e-09 Logit Scale: 100.000 Contrastive_loss: 0.23202 (0.22973) Loss: 0.23202 (0.22973)
2024-08-29,23:35:14 | INFO | Train Epoch: 32 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 519.743/s, 519.743/s/gpu LR: 7.590195e-09 Logit Scale: 100.000 Contrastive_loss: 0.23981 (0.23009) Loss: 0.23981 (0.23009)
2024-08-29,23:35:33 | INFO | Train Epoch: 32 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 523.441/s, 523.441/s/gpu LR: 7.581232e-09 Logit Scale: 100.000 Contrastive_loss: 0.27175 (0.23153) Loss: 0.27175 (0.23153)
2024-08-29,23:35:52 | INFO | Train Epoch: 32 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 499.267/s, 499.267/s/gpu LR: 7.572258e-09 Logit Scale: 100.000 Contrastive_loss: 0.18612 (0.23001) Loss: 0.18612 (0.23001)
2024-08-29,23:36:11 | INFO | Train Epoch: 32 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.193, 496.724/s, 496.724/s/gpu LR: 7.563272e-09 Logit Scale: 100.000 Contrastive_loss: 0.15418 (0.22757) Loss: 0.15418 (0.22757)
2024-08-29,23:36:31 | INFO | Train Epoch: 32 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.192, 489.758/s, 489.758/s/gpu LR: 7.554276e-09 Logit Scale: 100.000 Contrastive_loss: 0.28512 (0.22936) Loss: 0.28512 (0.22936)
2024-08-29,23:36:50 | INFO | Train Epoch: 32 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 508.359/s, 508.359/s/gpu LR: 7.545267e-09 Logit Scale: 100.000 Contrastive_loss: 0.25765 (0.23022) Loss: 0.25765 (0.23022)
2024-08-29,23:37:09 | INFO | Train Epoch: 32 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 489.268/s, 489.268/s/gpu LR: 7.536248e-09 Logit Scale: 100.000 Contrastive_loss: 0.30923 (0.23255) Loss: 0.30923 (0.23255)
2024-08-29,23:37:29 | INFO | Train Epoch: 32 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.194, 494.196/s, 494.196/s/gpu LR: 7.527218e-09 Logit Scale: 100.000 Contrastive_loss: 0.19795 (0.23156) Loss: 0.19795 (0.23156)
2024-08-29,23:37:48 | INFO | Train Epoch: 32 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.192, 539.978/s, 539.978/s/gpu LR: 7.518176e-09 Logit Scale: 100.000 Contrastive_loss: 0.28429 (0.23302) Loss: 0.28429 (0.23302)
2024-08-29,23:38:07 | INFO | Train Epoch: 32 [350100/145000.0 (241%)] Data (t): 0.110 Batch (t): 0.194, 501.159/s, 501.159/s/gpu LR: 7.509124e-09 Logit Scale: 100.000 Contrastive_loss: 0.32802 (0.23559) Loss: 0.32802 (0.23559)
2024-08-29,23:38:27 | INFO | Train Epoch: 32 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.192, 522.195/s, 522.195/s/gpu LR: 7.500060e-09 Logit Scale: 100.000 Contrastive_loss: 0.17268 (0.23393) Loss: 0.17268 (0.23393)
2024-08-29,23:38:46 | INFO | Train Epoch: 32 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 495.772/s, 495.772/s/gpu LR: 7.490986e-09 Logit Scale: 100.000 Contrastive_loss: 0.27823 (0.23507) Loss: 0.27823 (0.23507)
2024-08-29,23:39:05 | INFO | Train Epoch: 32 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 532.553/s, 532.553/s/gpu LR: 7.481901e-09 Logit Scale: 100.000 Contrastive_loss: 0.25634 (0.23560) Loss: 0.25634 (0.23560)
2024-08-29,23:39:24 | INFO | Train Epoch: 32 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 498.232/s, 498.232/s/gpu LR: 7.472804e-09 Logit Scale: 100.000 Contrastive_loss: 0.15154 (0.23355) Loss: 0.15154 (0.23355)
2024-08-29,23:39:44 | INFO | Train Epoch: 32 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 505.271/s, 505.271/s/gpu LR: 7.463697e-09 Logit Scale: 100.000 Contrastive_loss: 0.24483 (0.23382) Loss: 0.24483 (0.23382)
2024-08-29,23:40:03 | INFO | Train Epoch: 32 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 532.568/s, 532.568/s/gpu LR: 7.454579e-09 Logit Scale: 100.000 Contrastive_loss: 0.10128 (0.23074) Loss: 0.10128 (0.23074)
2024-08-29,23:40:10 | INFO | Eval Epoch: 33 [200 / 1000]	Clip Loss: 0.484072	
2024-08-29,23:40:11 | INFO | Eval Epoch: 33 image_to_text_mean_rank: 2.4510	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6620	image_to_text_R@5: 0.9120	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.6260	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6770	text_to_image_R@5: 0.9010	text_to_image_R@10: 0.9650	clip_val_loss: 0.4418	epoch: 33.0000	num_samples: 1000.0000
2024-08-29,23:40:12 | INFO | Start epoch 33
2024-08-29,23:40:12 | INFO | Train Epoch: 33 [   100/145000.0 (0%)] Data (t): 0.021 Batch (t): 0.105, 952.580/s, 952.580/s/gpu LR: 7.692443e-09 Logit Scale: 100.000 Contrastive_loss: 0.18770 (0.18770) Loss: 0.18770 (0.18770)
2024-08-29,23:40:32 | INFO | Train Epoch: 33 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 524.653/s, 524.653/s/gpu LR: 7.683614e-09 Logit Scale: 100.000 Contrastive_loss: 0.36184 (0.27477) Loss: 0.36184 (0.27477)
2024-08-29,23:40:51 | INFO | Train Epoch: 33 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 503.212/s, 503.212/s/gpu LR: 7.674772e-09 Logit Scale: 100.000 Contrastive_loss: 0.38236 (0.31063) Loss: 0.38236 (0.31063)
2024-08-29,23:41:10 | INFO | Train Epoch: 33 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 498.659/s, 498.659/s/gpu LR: 7.665918e-09 Logit Scale: 100.000 Contrastive_loss: 0.20481 (0.28418) Loss: 0.20481 (0.28418)
2024-08-29,23:41:30 | INFO | Train Epoch: 33 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 497.608/s, 497.608/s/gpu LR: 7.657053e-09 Logit Scale: 100.000 Contrastive_loss: 0.14621 (0.25658) Loss: 0.14621 (0.25658)
2024-08-29,23:41:49 | INFO | Train Epoch: 33 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 512.316/s, 512.316/s/gpu LR: 7.648176e-09 Logit Scale: 100.000 Contrastive_loss: 0.17996 (0.24381) Loss: 0.17996 (0.24381)
2024-08-29,23:42:08 | INFO | Train Epoch: 33 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.192, 531.257/s, 531.257/s/gpu LR: 7.639288e-09 Logit Scale: 100.000 Contrastive_loss: 0.12929 (0.22745) Loss: 0.12929 (0.22745)
2024-08-29,23:42:28 | INFO | Train Epoch: 33 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.192, 524.777/s, 524.777/s/gpu LR: 7.630388e-09 Logit Scale: 100.000 Contrastive_loss: 0.25123 (0.23043) Loss: 0.25123 (0.23043)
2024-08-29,23:42:47 | INFO | Train Epoch: 33 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 496.882/s, 496.882/s/gpu LR: 7.621476e-09 Logit Scale: 100.000 Contrastive_loss: 0.15105 (0.22161) Loss: 0.15105 (0.22161)
2024-08-29,23:43:06 | INFO | Train Epoch: 33 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 527.193/s, 527.193/s/gpu LR: 7.612553e-09 Logit Scale: 100.000 Contrastive_loss: 0.23332 (0.22278) Loss: 0.23332 (0.22278)
2024-08-29,23:43:25 | INFO | Train Epoch: 33 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.192, 506.511/s, 506.511/s/gpu LR: 7.603619e-09 Logit Scale: 100.000 Contrastive_loss: 0.16686 (0.21769) Loss: 0.16686 (0.21769)
2024-08-29,23:43:45 | INFO | Train Epoch: 33 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.194, 517.704/s, 517.704/s/gpu LR: 7.594673e-09 Logit Scale: 100.000 Contrastive_loss: 0.19066 (0.21544) Loss: 0.19066 (0.21544)
2024-08-29,23:44:04 | INFO | Train Epoch: 33 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.192, 527.120/s, 527.120/s/gpu LR: 7.585715e-09 Logit Scale: 100.000 Contrastive_loss: 0.29910 (0.22188) Loss: 0.29910 (0.22188)
2024-08-29,23:44:23 | INFO | Train Epoch: 33 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 528.862/s, 528.862/s/gpu LR: 7.576747e-09 Logit Scale: 100.000 Contrastive_loss: 0.16678 (0.21794) Loss: 0.16678 (0.21794)
2024-08-29,23:44:43 | INFO | Train Epoch: 33 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 535.488/s, 535.488/s/gpu LR: 7.567767e-09 Logit Scale: 100.000 Contrastive_loss: 0.19078 (0.21613) Loss: 0.19078 (0.21613)
2024-08-29,23:44:52 | INFO | Train Epoch: 33 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.192, 537.926/s, 537.926/s/gpu LR: 7.563362e-09 Logit Scale: 100.000 Contrastive_loss: 0.24569 (0.21798) Loss: 0.24569 (0.21798)
2024-08-29,23:45:02 | INFO | Train Epoch: 33 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 535.709/s, 535.709/s/gpu LR: 7.558775e-09 Logit Scale: 100.000 Contrastive_loss: 0.22001 (0.21810) Loss: 0.22001 (0.21810)
2024-08-29,23:45:21 | INFO | Train Epoch: 33 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 501.156/s, 501.156/s/gpu LR: 7.549773e-09 Logit Scale: 100.000 Contrastive_loss: 0.25965 (0.22041) Loss: 0.25965 (0.22041)
2024-08-29,23:45:40 | INFO | Train Epoch: 33 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 536.951/s, 536.951/s/gpu LR: 7.540759e-09 Logit Scale: 100.000 Contrastive_loss: 0.19337 (0.21898) Loss: 0.19337 (0.21898)
2024-08-29,23:46:00 | INFO | Train Epoch: 33 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 534.606/s, 534.606/s/gpu LR: 7.531734e-09 Logit Scale: 100.000 Contrastive_loss: 0.21102 (0.21859) Loss: 0.21102 (0.21859)
2024-08-29,23:46:19 | INFO | Train Epoch: 33 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 502.827/s, 502.827/s/gpu LR: 7.522699e-09 Logit Scale: 100.000 Contrastive_loss: 0.26611 (0.22085) Loss: 0.26611 (0.22085)
2024-08-29,23:46:38 | INFO | Train Epoch: 33 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.192, 520.707/s, 520.707/s/gpu LR: 7.513652e-09 Logit Scale: 100.000 Contrastive_loss: 0.17786 (0.21889) Loss: 0.17786 (0.21889)
2024-08-29,23:46:57 | INFO | Train Epoch: 33 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 540.946/s, 540.946/s/gpu LR: 7.504594e-09 Logit Scale: 100.000 Contrastive_loss: 0.26999 (0.22112) Loss: 0.26999 (0.22112)
2024-08-29,23:47:17 | INFO | Train Epoch: 33 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 491.913/s, 491.913/s/gpu LR: 7.495525e-09 Logit Scale: 100.000 Contrastive_loss: 0.34239 (0.22617) Loss: 0.34239 (0.22617)
2024-08-29,23:47:36 | INFO | Train Epoch: 33 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 497.647/s, 497.647/s/gpu LR: 7.486445e-09 Logit Scale: 100.000 Contrastive_loss: 0.31885 (0.22988) Loss: 0.31885 (0.22988)
2024-08-29,23:47:55 | INFO | Train Epoch: 33 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 504.863/s, 504.863/s/gpu LR: 7.477354e-09 Logit Scale: 100.000 Contrastive_loss: 0.18653 (0.22821) Loss: 0.18653 (0.22821)
2024-08-29,23:48:14 | INFO | Train Epoch: 33 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 493.490/s, 493.490/s/gpu LR: 7.468252e-09 Logit Scale: 100.000 Contrastive_loss: 0.23109 (0.22832) Loss: 0.23109 (0.22832)
2024-08-29,23:48:34 | INFO | Train Epoch: 33 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 527.293/s, 527.293/s/gpu LR: 7.459140e-09 Logit Scale: 100.000 Contrastive_loss: 0.23824 (0.22867) Loss: 0.23824 (0.22867)
2024-08-29,23:48:53 | INFO | Train Epoch: 33 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 506.781/s, 506.781/s/gpu LR: 7.450016e-09 Logit Scale: 100.000 Contrastive_loss: 0.27073 (0.23012) Loss: 0.27073 (0.23012)
2024-08-29,23:49:12 | INFO | Train Epoch: 33 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 517.258/s, 517.258/s/gpu LR: 7.440882e-09 Logit Scale: 100.000 Contrastive_loss: 0.18489 (0.22861) Loss: 0.18489 (0.22861)
2024-08-29,23:49:32 | INFO | Train Epoch: 33 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.192, 522.967/s, 522.967/s/gpu LR: 7.431737e-09 Logit Scale: 100.000 Contrastive_loss: 0.15361 (0.22619) Loss: 0.15361 (0.22619)
2024-08-29,23:49:51 | INFO | Train Epoch: 33 [300100/145000.0 (207%)] Data (t): 0.107 Batch (t): 0.192, 533.808/s, 533.808/s/gpu LR: 7.422582e-09 Logit Scale: 100.000 Contrastive_loss: 0.28369 (0.22799) Loss: 0.28369 (0.22799)
2024-08-29,23:50:10 | INFO | Train Epoch: 33 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 490.153/s, 490.153/s/gpu LR: 7.413416e-09 Logit Scale: 100.000 Contrastive_loss: 0.25655 (0.22886) Loss: 0.25655 (0.22886)
2024-08-29,23:50:29 | INFO | Train Epoch: 33 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 525.638/s, 525.638/s/gpu LR: 7.404239e-09 Logit Scale: 100.000 Contrastive_loss: 0.30722 (0.23116) Loss: 0.30722 (0.23116)
2024-08-29,23:50:49 | INFO | Train Epoch: 33 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 497.903/s, 497.903/s/gpu LR: 7.395052e-09 Logit Scale: 100.000 Contrastive_loss: 0.19729 (0.23019) Loss: 0.19729 (0.23019)
2024-08-29,23:51:08 | INFO | Train Epoch: 33 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 507.631/s, 507.631/s/gpu LR: 7.385854e-09 Logit Scale: 100.000 Contrastive_loss: 0.28182 (0.23163) Loss: 0.28182 (0.23163)
2024-08-29,23:51:27 | INFO | Train Epoch: 33 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.194, 503.088/s, 503.088/s/gpu LR: 7.376646e-09 Logit Scale: 100.000 Contrastive_loss: 0.32593 (0.23418) Loss: 0.32593 (0.23418)
2024-08-29,23:51:46 | INFO | Train Epoch: 33 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.192, 542.024/s, 542.024/s/gpu LR: 7.367428e-09 Logit Scale: 100.000 Contrastive_loss: 0.17172 (0.23253) Loss: 0.17172 (0.23253)
2024-08-29,23:52:06 | INFO | Train Epoch: 33 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 491.882/s, 491.882/s/gpu LR: 7.358199e-09 Logit Scale: 100.000 Contrastive_loss: 0.27694 (0.23367) Loss: 0.27694 (0.23367)
2024-08-29,23:52:25 | INFO | Train Epoch: 33 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 528.862/s, 528.862/s/gpu LR: 7.348959e-09 Logit Scale: 100.000 Contrastive_loss: 0.25484 (0.23420) Loss: 0.25484 (0.23420)
2024-08-29,23:52:44 | INFO | Train Epoch: 33 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 530.226/s, 530.226/s/gpu LR: 7.339710e-09 Logit Scale: 100.000 Contrastive_loss: 0.15005 (0.23215) Loss: 0.15005 (0.23215)
2024-08-29,23:53:03 | INFO | Train Epoch: 33 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 533.206/s, 533.206/s/gpu LR: 7.330450e-09 Logit Scale: 100.000 Contrastive_loss: 0.24362 (0.23242) Loss: 0.24362 (0.23242)
2024-08-29,23:53:23 | INFO | Train Epoch: 33 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 506.259/s, 506.259/s/gpu LR: 7.321180e-09 Logit Scale: 100.000 Contrastive_loss: 0.10048 (0.22935) Loss: 0.10048 (0.22935)
2024-08-29,23:53:30 | INFO | Eval Epoch: 34 [200 / 1000]	Clip Loss: 0.483639	
2024-08-29,23:53:31 | INFO | Eval Epoch: 34 image_to_text_mean_rank: 2.4510	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6620	image_to_text_R@5: 0.9110	image_to_text_R@10: 0.9600	text_to_image_mean_rank: 3.6480	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6770	text_to_image_R@5: 0.9020	text_to_image_R@10: 0.9650	clip_val_loss: 0.4414	epoch: 34.0000	num_samples: 1000.0000
2024-08-29,23:53:32 | INFO | Start epoch 34
2024-08-29,23:53:32 | INFO | Train Epoch: 34 [   100/145000.0 (0%)] Data (t): 0.020 Batch (t): 0.108, 926.973/s, 926.973/s/gpu LR: 7.563272e-09 Logit Scale: 100.000 Contrastive_loss: 0.18646 (0.18646) Loss: 0.18646 (0.18646)
2024-08-29,23:53:52 | INFO | Train Epoch: 34 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 492.686/s, 492.686/s/gpu LR: 7.554276e-09 Logit Scale: 100.000 Contrastive_loss: 0.36026 (0.27336) Loss: 0.36026 (0.27336)
2024-08-29,23:54:11 | INFO | Train Epoch: 34 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.194, 492.263/s, 492.263/s/gpu LR: 7.545267e-09 Logit Scale: 100.000 Contrastive_loss: 0.38025 (0.30899) Loss: 0.38025 (0.30899)
2024-08-29,23:54:31 | INFO | Train Epoch: 34 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 519.941/s, 519.941/s/gpu LR: 7.536248e-09 Logit Scale: 100.000 Contrastive_loss: 0.20336 (0.28258) Loss: 0.20336 (0.28258)
2024-08-29,23:54:50 | INFO | Train Epoch: 34 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 523.809/s, 523.809/s/gpu LR: 7.527218e-09 Logit Scale: 100.000 Contrastive_loss: 0.14533 (0.25513) Loss: 0.14533 (0.25513)
2024-08-29,23:55:09 | INFO | Train Epoch: 34 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 527.093/s, 527.093/s/gpu LR: 7.518176e-09 Logit Scale: 100.000 Contrastive_loss: 0.17870 (0.24239) Loss: 0.17870 (0.24239)
2024-08-29,23:55:28 | INFO | Train Epoch: 34 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 532.425/s, 532.425/s/gpu LR: 7.509124e-09 Logit Scale: 100.000 Contrastive_loss: 0.12838 (0.22611) Loss: 0.12838 (0.22611)
2024-08-29,23:55:48 | INFO | Train Epoch: 34 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 509.051/s, 509.051/s/gpu LR: 7.500060e-09 Logit Scale: 100.000 Contrastive_loss: 0.24918 (0.22899) Loss: 0.24918 (0.22899)
2024-08-29,23:56:07 | INFO | Train Epoch: 34 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.194, 491.387/s, 491.387/s/gpu LR: 7.490986e-09 Logit Scale: 100.000 Contrastive_loss: 0.14957 (0.22017) Loss: 0.14957 (0.22017)
2024-08-29,23:56:26 | INFO | Train Epoch: 34 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 492.663/s, 492.663/s/gpu LR: 7.481901e-09 Logit Scale: 100.000 Contrastive_loss: 0.23193 (0.22134) Loss: 0.23193 (0.22134)
2024-08-29,23:56:46 | INFO | Train Epoch: 34 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 504.868/s, 504.868/s/gpu LR: 7.472804e-09 Logit Scale: 100.000 Contrastive_loss: 0.16574 (0.21629) Loss: 0.16574 (0.21629)
2024-08-29,23:57:05 | INFO | Train Epoch: 34 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 500.480/s, 500.480/s/gpu LR: 7.463697e-09 Logit Scale: 100.000 Contrastive_loss: 0.18933 (0.21404) Loss: 0.18933 (0.21404)
2024-08-29,23:57:24 | INFO | Train Epoch: 34 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.192, 525.055/s, 525.055/s/gpu LR: 7.454579e-09 Logit Scale: 100.000 Contrastive_loss: 0.29719 (0.22044) Loss: 0.29719 (0.22044)
2024-08-29,23:57:44 | INFO | Train Epoch: 34 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 521.511/s, 521.511/s/gpu LR: 7.445451e-09 Logit Scale: 100.000 Contrastive_loss: 0.16560 (0.21652) Loss: 0.16560 (0.21652)
2024-08-29,23:58:03 | INFO | Train Epoch: 34 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 534.719/s, 534.719/s/gpu LR: 7.436311e-09 Logit Scale: 100.000 Contrastive_loss: 0.18987 (0.21474) Loss: 0.18987 (0.21474)
2024-08-29,23:58:12 | INFO | Train Epoch: 34 [145000/145000.0 (100%)] Data (t): 0.107 Batch (t): 0.192, 506.066/s, 506.066/s/gpu LR: 7.431829e-09 Logit Scale: 100.000 Contrastive_loss: 0.24434 (0.21659) Loss: 0.24434 (0.21659)
2024-08-29,23:58:22 | INFO | Train Epoch: 34 [150100/145000.0 (104%)] Data (t): 0.110 Batch (t): 0.194, 521.615/s, 521.615/s/gpu LR: 7.427161e-09 Logit Scale: 100.000 Contrastive_loss: 0.21848 (0.21670) Loss: 0.21848 (0.21670)
2024-08-29,23:58:41 | INFO | Train Epoch: 34 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 528.503/s, 528.503/s/gpu LR: 7.418000e-09 Logit Scale: 100.000 Contrastive_loss: 0.25749 (0.21897) Loss: 0.25749 (0.21897)
2024-08-29,23:59:01 | INFO | Train Epoch: 34 [170100/145000.0 (117%)] Data (t): 0.107 Batch (t): 0.192, 499.205/s, 499.205/s/gpu LR: 7.408829e-09 Logit Scale: 100.000 Contrastive_loss: 0.19197 (0.21755) Loss: 0.19197 (0.21755)
2024-08-29,23:59:20 | INFO | Train Epoch: 34 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 528.615/s, 528.615/s/gpu LR: 7.399647e-09 Logit Scale: 100.000 Contrastive_loss: 0.21007 (0.21718) Loss: 0.21007 (0.21718)
2024-08-29,23:59:39 | INFO | Train Epoch: 34 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 524.746/s, 524.746/s/gpu LR: 7.390455e-09 Logit Scale: 100.000 Contrastive_loss: 0.26446 (0.21943) Loss: 0.26446 (0.21943)
2024-08-29,23:59:59 | INFO | Train Epoch: 34 [200100/145000.0 (138%)] Data (t): 0.111 Batch (t): 0.195, 501.244/s, 501.244/s/gpu LR: 7.381252e-09 Logit Scale: 100.000 Contrastive_loss: 0.17697 (0.21750) Loss: 0.17697 (0.21750)
2024-08-30,00:00:18 | INFO | Train Epoch: 34 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 531.717/s, 531.717/s/gpu LR: 7.372038e-09 Logit Scale: 100.000 Contrastive_loss: 0.26874 (0.21973) Loss: 0.26874 (0.21973)
2024-08-30,00:00:37 | INFO | Train Epoch: 34 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 523.390/s, 523.390/s/gpu LR: 7.362814e-09 Logit Scale: 100.000 Contrastive_loss: 0.33982 (0.22473) Loss: 0.33982 (0.22473)
2024-08-30,00:00:57 | INFO | Train Epoch: 34 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 496.869/s, 496.869/s/gpu LR: 7.353580e-09 Logit Scale: 100.000 Contrastive_loss: 0.31775 (0.22845) Loss: 0.31775 (0.22845)
2024-08-30,00:01:16 | INFO | Train Epoch: 34 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 492.419/s, 492.419/s/gpu LR: 7.344336e-09 Logit Scale: 100.000 Contrastive_loss: 0.18493 (0.22678) Loss: 0.18493 (0.22678)
2024-08-30,00:01:35 | INFO | Train Epoch: 34 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.194, 542.776/s, 542.776/s/gpu LR: 7.335081e-09 Logit Scale: 100.000 Contrastive_loss: 0.22990 (0.22689) Loss: 0.22990 (0.22689)
2024-08-30,00:01:55 | INFO | Train Epoch: 34 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 525.226/s, 525.226/s/gpu LR: 7.325816e-09 Logit Scale: 100.000 Contrastive_loss: 0.23690 (0.22725) Loss: 0.23690 (0.22725)
2024-08-30,00:02:14 | INFO | Train Epoch: 34 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 500.590/s, 500.590/s/gpu LR: 7.316541e-09 Logit Scale: 100.000 Contrastive_loss: 0.26984 (0.22872) Loss: 0.26984 (0.22872)
2024-08-30,00:02:33 | INFO | Train Epoch: 34 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 526.608/s, 526.608/s/gpu LR: 7.307255e-09 Logit Scale: 100.000 Contrastive_loss: 0.18379 (0.22722) Loss: 0.18379 (0.22722)
2024-08-30,00:02:53 | INFO | Train Epoch: 34 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.194, 520.786/s, 520.786/s/gpu LR: 7.297960e-09 Logit Scale: 100.000 Contrastive_loss: 0.15293 (0.22482) Loss: 0.15293 (0.22482)
2024-08-30,00:03:12 | INFO | Train Epoch: 34 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 529.462/s, 529.462/s/gpu LR: 7.288654e-09 Logit Scale: 100.000 Contrastive_loss: 0.28240 (0.22662) Loss: 0.28240 (0.22662)
2024-08-30,00:03:31 | INFO | Train Epoch: 34 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 496.246/s, 496.246/s/gpu LR: 7.279339e-09 Logit Scale: 100.000 Contrastive_loss: 0.25560 (0.22750) Loss: 0.25560 (0.22750)
2024-08-30,00:03:50 | INFO | Train Epoch: 34 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.192, 528.309/s, 528.309/s/gpu LR: 7.270013e-09 Logit Scale: 100.000 Contrastive_loss: 0.30527 (0.22979) Loss: 0.30527 (0.22979)
2024-08-30,00:04:10 | INFO | Train Epoch: 34 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 521.346/s, 521.346/s/gpu LR: 7.260678e-09 Logit Scale: 100.000 Contrastive_loss: 0.19648 (0.22884) Loss: 0.19648 (0.22884)
2024-08-30,00:04:29 | INFO | Train Epoch: 34 [340100/145000.0 (235%)] Data (t): 0.107 Batch (t): 0.192, 499.939/s, 499.939/s/gpu LR: 7.251332e-09 Logit Scale: 100.000 Contrastive_loss: 0.27959 (0.23025) Loss: 0.27959 (0.23025)
2024-08-30,00:04:48 | INFO | Train Epoch: 34 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 498.985/s, 498.985/s/gpu LR: 7.241977e-09 Logit Scale: 100.000 Contrastive_loss: 0.32387 (0.23278) Loss: 0.32387 (0.23278)
2024-08-30,00:05:07 | INFO | Train Epoch: 34 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.192, 509.917/s, 509.917/s/gpu LR: 7.232612e-09 Logit Scale: 100.000 Contrastive_loss: 0.17072 (0.23114) Loss: 0.17072 (0.23114)
2024-08-30,00:05:27 | INFO | Train Epoch: 34 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 492.396/s, 492.396/s/gpu LR: 7.223237e-09 Logit Scale: 100.000 Contrastive_loss: 0.27584 (0.23229) Loss: 0.27584 (0.23229)
2024-08-30,00:05:46 | INFO | Train Epoch: 34 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.194, 504.910/s, 504.910/s/gpu LR: 7.213852e-09 Logit Scale: 100.000 Contrastive_loss: 0.25323 (0.23281) Loss: 0.25323 (0.23281)
2024-08-30,00:06:05 | INFO | Train Epoch: 34 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.193, 505.788/s, 505.788/s/gpu LR: 7.204457e-09 Logit Scale: 100.000 Contrastive_loss: 0.14882 (0.23076) Loss: 0.14882 (0.23076)
2024-08-30,00:06:25 | INFO | Train Epoch: 34 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.193, 507.373/s, 507.373/s/gpu LR: 7.195053e-09 Logit Scale: 100.000 Contrastive_loss: 0.24219 (0.23104) Loss: 0.24219 (0.23104)
2024-08-30,00:06:44 | INFO | Train Epoch: 34 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 497.717/s, 497.717/s/gpu LR: 7.185639e-09 Logit Scale: 100.000 Contrastive_loss: 0.099664 (0.22798) Loss: 0.099664 (0.22798)
2024-08-30,00:06:51 | INFO | Eval Epoch: 35 [200 / 1000]	Clip Loss: 0.483438	
2024-08-30,00:06:52 | INFO | Eval Epoch: 35 image_to_text_mean_rank: 2.4490	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6620	image_to_text_R@5: 0.9120	image_to_text_R@10: 0.9600	text_to_image_mean_rank: 3.6590	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6760	text_to_image_R@5: 0.9010	text_to_image_R@10: 0.9650	clip_val_loss: 0.4411	epoch: 35.0000	num_samples: 1000.0000
2024-08-30,00:06:54 | INFO | Start epoch 35
2024-08-30,00:06:54 | INFO | Train Epoch: 35 [   100/145000.0 (0%)] Data (t): 0.023 Batch (t): 0.102, 981.578/s, 981.578/s/gpu LR: 7.431737e-09 Logit Scale: 100.000 Contrastive_loss: 0.18524 (0.18524) Loss: 0.18524 (0.18524)
2024-08-30,00:07:13 | INFO | Train Epoch: 35 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 535.334/s, 535.334/s/gpu LR: 7.422582e-09 Logit Scale: 100.000 Contrastive_loss: 0.35850 (0.27187) Loss: 0.35850 (0.27187)
2024-08-30,00:07:33 | INFO | Train Epoch: 35 [ 20100/145000.0 (14%)] Data (t): 0.110 Batch (t): 0.194, 539.184/s, 539.184/s/gpu LR: 7.413416e-09 Logit Scale: 100.000 Contrastive_loss: 0.37827 (0.30734) Loss: 0.37827 (0.30734)
2024-08-30,00:07:52 | INFO | Train Epoch: 35 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 504.341/s, 504.341/s/gpu LR: 7.404239e-09 Logit Scale: 100.000 Contrastive_loss: 0.20197 (0.28100) Loss: 0.20197 (0.28100)
2024-08-30,00:08:11 | INFO | Train Epoch: 35 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 494.741/s, 494.741/s/gpu LR: 7.395052e-09 Logit Scale: 100.000 Contrastive_loss: 0.14447 (0.25369) Loss: 0.14447 (0.25369)
2024-08-30,00:08:31 | INFO | Train Epoch: 35 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 500.489/s, 500.489/s/gpu LR: 7.385854e-09 Logit Scale: 100.000 Contrastive_loss: 0.17736 (0.24097) Loss: 0.17736 (0.24097)
2024-08-30,00:08:50 | INFO | Train Epoch: 35 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 500.149/s, 500.149/s/gpu LR: 7.376646e-09 Logit Scale: 100.000 Contrastive_loss: 0.12757 (0.22477) Loss: 0.12757 (0.22477)
2024-08-30,00:09:09 | INFO | Train Epoch: 35 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 525.304/s, 525.304/s/gpu LR: 7.367428e-09 Logit Scale: 100.000 Contrastive_loss: 0.24717 (0.22757) Loss: 0.24717 (0.22757)
2024-08-30,00:09:29 | INFO | Train Epoch: 35 [ 80100/145000.0 (55%)] Data (t): 0.107 Batch (t): 0.193, 502.970/s, 502.970/s/gpu LR: 7.358199e-09 Logit Scale: 100.000 Contrastive_loss: 0.14817 (0.21875) Loss: 0.14817 (0.21875)
2024-08-30,00:09:48 | INFO | Train Epoch: 35 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 500.690/s, 500.690/s/gpu LR: 7.348959e-09 Logit Scale: 100.000 Contrastive_loss: 0.23071 (0.21994) Loss: 0.23071 (0.21994)
2024-08-30,00:10:07 | INFO | Train Epoch: 35 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 501.673/s, 501.673/s/gpu LR: 7.339710e-09 Logit Scale: 100.000 Contrastive_loss: 0.16478 (0.21493) Loss: 0.16478 (0.21493)
2024-08-30,00:10:27 | INFO | Train Epoch: 35 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 522.049/s, 522.049/s/gpu LR: 7.330450e-09 Logit Scale: 100.000 Contrastive_loss: 0.18773 (0.21266) Loss: 0.18773 (0.21266)
2024-08-30,00:10:46 | INFO | Train Epoch: 35 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.194, 514.280/s, 514.280/s/gpu LR: 7.321180e-09 Logit Scale: 100.000 Contrastive_loss: 0.29532 (0.21902) Loss: 0.29532 (0.21902)
2024-08-30,00:11:05 | INFO | Train Epoch: 35 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 498.377/s, 498.377/s/gpu LR: 7.311899e-09 Logit Scale: 100.000 Contrastive_loss: 0.16465 (0.21514) Loss: 0.16465 (0.21514)
2024-08-30,00:11:25 | INFO | Train Epoch: 35 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.194, 503.413/s, 503.413/s/gpu LR: 7.302609e-09 Logit Scale: 100.000 Contrastive_loss: 0.18909 (0.21340) Loss: 0.18909 (0.21340)
2024-08-30,00:11:34 | INFO | Train Epoch: 35 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 532.489/s, 532.489/s/gpu LR: 7.298053e-09 Logit Scale: 100.000 Contrastive_loss: 0.24305 (0.21525) Loss: 0.24305 (0.21525)
2024-08-30,00:11:44 | INFO | Train Epoch: 35 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.194, 524.511/s, 524.511/s/gpu LR: 7.293308e-09 Logit Scale: 100.000 Contrastive_loss: 0.21693 (0.21535) Loss: 0.21693 (0.21535)
2024-08-30,00:12:03 | INFO | Train Epoch: 35 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 526.890/s, 526.890/s/gpu LR: 7.283998e-09 Logit Scale: 100.000 Contrastive_loss: 0.25526 (0.21757) Loss: 0.25526 (0.21757)
2024-08-30,00:12:23 | INFO | Train Epoch: 35 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 494.291/s, 494.291/s/gpu LR: 7.274677e-09 Logit Scale: 100.000 Contrastive_loss: 0.19056 (0.21615) Loss: 0.19056 (0.21615)
2024-08-30,00:12:42 | INFO | Train Epoch: 35 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 494.545/s, 494.545/s/gpu LR: 7.265347e-09 Logit Scale: 100.000 Contrastive_loss: 0.20917 (0.21580) Loss: 0.20917 (0.21580)
2024-08-30,00:13:01 | INFO | Train Epoch: 35 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.194, 503.104/s, 503.104/s/gpu LR: 7.256006e-09 Logit Scale: 100.000 Contrastive_loss: 0.26290 (0.21804) Loss: 0.26290 (0.21804)
2024-08-30,00:13:21 | INFO | Train Epoch: 35 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 523.936/s, 523.936/s/gpu LR: 7.246656e-09 Logit Scale: 100.000 Contrastive_loss: 0.17623 (0.21614) Loss: 0.17623 (0.21614)
2024-08-30,00:13:40 | INFO | Train Epoch: 35 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 501.378/s, 501.378/s/gpu LR: 7.237295e-09 Logit Scale: 100.000 Contrastive_loss: 0.26743 (0.21837) Loss: 0.26743 (0.21837)
2024-08-30,00:13:59 | INFO | Train Epoch: 35 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.192, 526.185/s, 526.185/s/gpu LR: 7.227925e-09 Logit Scale: 100.000 Contrastive_loss: 0.33753 (0.22334) Loss: 0.33753 (0.22334)
2024-08-30,00:14:18 | INFO | Train Epoch: 35 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 522.053/s, 522.053/s/gpu LR: 7.218545e-09 Logit Scale: 100.000 Contrastive_loss: 0.31650 (0.22706) Loss: 0.31650 (0.22706)
2024-08-30,00:14:38 | INFO | Train Epoch: 35 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 501.272/s, 501.272/s/gpu LR: 7.209156e-09 Logit Scale: 100.000 Contrastive_loss: 0.18353 (0.22539) Loss: 0.18353 (0.22539)
2024-08-30,00:14:57 | INFO | Train Epoch: 35 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 505.988/s, 505.988/s/gpu LR: 7.199757e-09 Logit Scale: 100.000 Contrastive_loss: 0.22899 (0.22552) Loss: 0.22899 (0.22552)
2024-08-30,00:15:16 | INFO | Train Epoch: 35 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 502.744/s, 502.744/s/gpu LR: 7.190348e-09 Logit Scale: 100.000 Contrastive_loss: 0.23539 (0.22587) Loss: 0.23539 (0.22587)
2024-08-30,00:15:35 | INFO | Train Epoch: 35 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 514.870/s, 514.870/s/gpu LR: 7.180929e-09 Logit Scale: 100.000 Contrastive_loss: 0.26896 (0.22736) Loss: 0.26896 (0.22736)
2024-08-30,00:15:55 | INFO | Train Epoch: 35 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 502.385/s, 502.385/s/gpu LR: 7.171501e-09 Logit Scale: 100.000 Contrastive_loss: 0.18285 (0.22588) Loss: 0.18285 (0.22588)
2024-08-30,00:16:14 | INFO | Train Epoch: 35 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 492.548/s, 492.548/s/gpu LR: 7.162063e-09 Logit Scale: 100.000 Contrastive_loss: 0.15230 (0.22350) Loss: 0.15230 (0.22350)
2024-08-30,00:16:33 | INFO | Train Epoch: 35 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.194, 496.755/s, 496.755/s/gpu LR: 7.152616e-09 Logit Scale: 100.000 Contrastive_loss: 0.28084 (0.22529) Loss: 0.28084 (0.22529)
2024-08-30,00:16:53 | INFO | Train Epoch: 35 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 492.743/s, 492.743/s/gpu LR: 7.143160e-09 Logit Scale: 100.000 Contrastive_loss: 0.25465 (0.22618) Loss: 0.25465 (0.22618)
2024-08-30,00:17:12 | INFO | Train Epoch: 35 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 500.410/s, 500.410/s/gpu LR: 7.133694e-09 Logit Scale: 100.000 Contrastive_loss: 0.30371 (0.22846) Loss: 0.30371 (0.22846)
2024-08-30,00:17:31 | INFO | Train Epoch: 35 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 527.809/s, 527.809/s/gpu LR: 7.124218e-09 Logit Scale: 100.000 Contrastive_loss: 0.19571 (0.22753) Loss: 0.19571 (0.22753)
2024-08-30,00:17:51 | INFO | Train Epoch: 35 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.193, 535.790/s, 535.790/s/gpu LR: 7.114734e-09 Logit Scale: 100.000 Contrastive_loss: 0.27736 (0.22891) Loss: 0.27736 (0.22891)
2024-08-30,00:18:10 | INFO | Train Epoch: 35 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 492.496/s, 492.496/s/gpu LR: 7.105240e-09 Logit Scale: 100.000 Contrastive_loss: 0.32196 (0.23143) Loss: 0.32196 (0.23143)
2024-08-30,00:18:29 | INFO | Train Epoch: 35 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 524.741/s, 524.741/s/gpu LR: 7.095737e-09 Logit Scale: 100.000 Contrastive_loss: 0.16927 (0.22979) Loss: 0.16927 (0.22979)
2024-08-30,00:18:48 | INFO | Train Epoch: 35 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 534.414/s, 534.414/s/gpu LR: 7.086224e-09 Logit Scale: 100.000 Contrastive_loss: 0.27469 (0.23094) Loss: 0.27469 (0.23094)
2024-08-30,00:19:08 | INFO | Train Epoch: 35 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 515.884/s, 515.884/s/gpu LR: 7.076703e-09 Logit Scale: 100.000 Contrastive_loss: 0.25200 (0.23147) Loss: 0.25200 (0.23147)
2024-08-30,00:19:27 | INFO | Train Epoch: 35 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.193, 496.482/s, 496.482/s/gpu LR: 7.067172e-09 Logit Scale: 100.000 Contrastive_loss: 0.14755 (0.22942) Loss: 0.14755 (0.22942)
2024-08-30,00:19:46 | INFO | Train Epoch: 35 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.192, 526.174/s, 526.174/s/gpu LR: 7.057632e-09 Logit Scale: 100.000 Contrastive_loss: 0.24096 (0.22970) Loss: 0.24096 (0.22970)
2024-08-30,00:20:06 | INFO | Train Epoch: 35 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 532.960/s, 532.960/s/gpu LR: 7.048084e-09 Logit Scale: 100.000 Contrastive_loss: 0.098976 (0.22666) Loss: 0.098976 (0.22666)
2024-08-30,00:20:13 | INFO | Eval Epoch: 36 [200 / 1000]	Clip Loss: 0.483074	
2024-08-30,00:20:14 | INFO | Eval Epoch: 36 image_to_text_mean_rank: 2.4460	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6620	image_to_text_R@5: 0.9120	image_to_text_R@10: 0.9600	text_to_image_mean_rank: 3.6700	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6750	text_to_image_R@5: 0.9010	text_to_image_R@10: 0.9650	clip_val_loss: 0.4408	epoch: 36.0000	num_samples: 1000.0000
2024-08-30,00:20:15 | INFO | Start epoch 36
2024-08-30,00:20:15 | INFO | Train Epoch: 36 [   100/145000.0 (0%)] Data (t): 0.034 Batch (t): 0.120, 830.996/s, 830.996/s/gpu LR: 7.297960e-09 Logit Scale: 100.000 Contrastive_loss: 0.18415 (0.18415) Loss: 0.18415 (0.18415)
2024-08-30,00:20:35 | INFO | Train Epoch: 36 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 528.416/s, 528.416/s/gpu LR: 7.288654e-09 Logit Scale: 100.000 Contrastive_loss: 0.35691 (0.27053) Loss: 0.35691 (0.27053)
2024-08-30,00:20:54 | INFO | Train Epoch: 36 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.193, 524.332/s, 524.332/s/gpu LR: 7.279339e-09 Logit Scale: 100.000 Contrastive_loss: 0.37614 (0.30573) Loss: 0.37614 (0.30573)
2024-08-30,00:21:13 | INFO | Train Epoch: 36 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.194, 527.301/s, 527.301/s/gpu LR: 7.270013e-09 Logit Scale: 100.000 Contrastive_loss: 0.20054 (0.27943) Loss: 0.20054 (0.27943)
2024-08-30,00:21:33 | INFO | Train Epoch: 36 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 527.762/s, 527.762/s/gpu LR: 7.260678e-09 Logit Scale: 100.000 Contrastive_loss: 0.14368 (0.25228) Loss: 0.14368 (0.25228)
2024-08-30,00:21:52 | INFO | Train Epoch: 36 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 522.911/s, 522.911/s/gpu LR: 7.251332e-09 Logit Scale: 100.000 Contrastive_loss: 0.17626 (0.23961) Loss: 0.17626 (0.23961)
2024-08-30,00:22:11 | INFO | Train Epoch: 36 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 500.954/s, 500.954/s/gpu LR: 7.241977e-09 Logit Scale: 100.000 Contrastive_loss: 0.12692 (0.22351) Loss: 0.12692 (0.22351)
2024-08-30,00:22:31 | INFO | Train Epoch: 36 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 498.566/s, 498.566/s/gpu LR: 7.232612e-09 Logit Scale: 100.000 Contrastive_loss: 0.24540 (0.22625) Loss: 0.24540 (0.22625)
2024-08-30,00:22:50 | INFO | Train Epoch: 36 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 500.417/s, 500.417/s/gpu LR: 7.223237e-09 Logit Scale: 100.000 Contrastive_loss: 0.14682 (0.21742) Loss: 0.14682 (0.21742)
2024-08-30,00:23:09 | INFO | Train Epoch: 36 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.194, 494.344/s, 494.344/s/gpu LR: 7.213852e-09 Logit Scale: 100.000 Contrastive_loss: 0.22935 (0.21862) Loss: 0.22935 (0.21862)
2024-08-30,00:23:29 | INFO | Train Epoch: 36 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 526.050/s, 526.050/s/gpu LR: 7.204457e-09 Logit Scale: 100.000 Contrastive_loss: 0.16397 (0.21365) Loss: 0.16397 (0.21365)
2024-08-30,00:23:48 | INFO | Train Epoch: 36 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 500.483/s, 500.483/s/gpu LR: 7.195053e-09 Logit Scale: 100.000 Contrastive_loss: 0.18667 (0.21140) Loss: 0.18667 (0.21140)
2024-08-30,00:24:07 | INFO | Train Epoch: 36 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 506.903/s, 506.903/s/gpu LR: 7.185639e-09 Logit Scale: 100.000 Contrastive_loss: 0.29362 (0.21773) Loss: 0.29362 (0.21773)
2024-08-30,00:24:26 | INFO | Train Epoch: 36 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 533.047/s, 533.047/s/gpu LR: 7.176216e-09 Logit Scale: 100.000 Contrastive_loss: 0.16369 (0.21387) Loss: 0.16369 (0.21387)
2024-08-30,00:24:46 | INFO | Train Epoch: 36 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 537.204/s, 537.204/s/gpu LR: 7.166783e-09 Logit Scale: 100.000 Contrastive_loss: 0.18828 (0.21216) Loss: 0.18828 (0.21216)
2024-08-30,00:24:55 | INFO | Train Epoch: 36 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 525.168/s, 525.168/s/gpu LR: 7.162158e-09 Logit Scale: 100.000 Contrastive_loss: 0.24172 (0.21401) Loss: 0.24172 (0.21401)
2024-08-30,00:25:05 | INFO | Train Epoch: 36 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 488.307/s, 488.307/s/gpu LR: 7.157341e-09 Logit Scale: 100.000 Contrastive_loss: 0.21536 (0.21409) Loss: 0.21536 (0.21409)
2024-08-30,00:25:24 | INFO | Train Epoch: 36 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 491.449/s, 491.449/s/gpu LR: 7.147889e-09 Logit Scale: 100.000 Contrastive_loss: 0.25343 (0.21627) Loss: 0.25343 (0.21627)
2024-08-30,00:25:44 | INFO | Train Epoch: 36 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 497.997/s, 497.997/s/gpu LR: 7.138428e-09 Logit Scale: 100.000 Contrastive_loss: 0.18933 (0.21486) Loss: 0.18933 (0.21486)
2024-08-30,00:26:03 | INFO | Train Epoch: 36 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 535.854/s, 535.854/s/gpu LR: 7.128957e-09 Logit Scale: 100.000 Contrastive_loss: 0.20830 (0.21453) Loss: 0.20830 (0.21453)
2024-08-30,00:26:22 | INFO | Train Epoch: 36 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 504.325/s, 504.325/s/gpu LR: 7.119477e-09 Logit Scale: 100.000 Contrastive_loss: 0.26154 (0.21677) Loss: 0.26154 (0.21677)
2024-08-30,00:26:41 | INFO | Train Epoch: 36 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 526.199/s, 526.199/s/gpu LR: 7.109988e-09 Logit Scale: 100.000 Contrastive_loss: 0.17543 (0.21489) Loss: 0.17543 (0.21489)
2024-08-30,00:27:01 | INFO | Train Epoch: 36 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 496.701/s, 496.701/s/gpu LR: 7.100489e-09 Logit Scale: 100.000 Contrastive_loss: 0.26603 (0.21711) Loss: 0.26603 (0.21711)
2024-08-30,00:27:20 | INFO | Train Epoch: 36 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.194, 528.381/s, 528.381/s/gpu LR: 7.090982e-09 Logit Scale: 100.000 Contrastive_loss: 0.33519 (0.22203) Loss: 0.33519 (0.22203)
2024-08-30,00:27:39 | INFO | Train Epoch: 36 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 534.481/s, 534.481/s/gpu LR: 7.081465e-09 Logit Scale: 100.000 Contrastive_loss: 0.31525 (0.22576) Loss: 0.31525 (0.22576)
2024-08-30,00:27:59 | INFO | Train Epoch: 36 [240100/145000.0 (166%)] Data (t): 0.107 Batch (t): 0.192, 524.949/s, 524.949/s/gpu LR: 7.071939e-09 Logit Scale: 100.000 Contrastive_loss: 0.18212 (0.22408) Loss: 0.18212 (0.22408)
2024-08-30,00:28:18 | INFO | Train Epoch: 36 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 538.379/s, 538.379/s/gpu LR: 7.062403e-09 Logit Scale: 100.000 Contrastive_loss: 0.22801 (0.22423) Loss: 0.22801 (0.22423)
2024-08-30,00:28:37 | INFO | Train Epoch: 36 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 494.746/s, 494.746/s/gpu LR: 7.052859e-09 Logit Scale: 100.000 Contrastive_loss: 0.23415 (0.22458) Loss: 0.23415 (0.22458)
2024-08-30,00:28:57 | INFO | Train Epoch: 36 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 499.432/s, 499.432/s/gpu LR: 7.043306e-09 Logit Scale: 100.000 Contrastive_loss: 0.26801 (0.22608) Loss: 0.26801 (0.22608)
2024-08-30,00:29:16 | INFO | Train Epoch: 36 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 520.319/s, 520.319/s/gpu LR: 7.033744e-09 Logit Scale: 100.000 Contrastive_loss: 0.18176 (0.22460) Loss: 0.18176 (0.22460)
2024-08-30,00:29:35 | INFO | Train Epoch: 36 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.193, 531.226/s, 531.226/s/gpu LR: 7.024173e-09 Logit Scale: 100.000 Contrastive_loss: 0.15180 (0.22225) Loss: 0.15180 (0.22225)
2024-08-30,00:29:54 | INFO | Train Epoch: 36 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 533.140/s, 533.140/s/gpu LR: 7.014593e-09 Logit Scale: 100.000 Contrastive_loss: 0.27965 (0.22405) Loss: 0.27965 (0.22405)
2024-08-30,00:30:14 | INFO | Train Epoch: 36 [310100/145000.0 (214%)] Data (t): 0.107 Batch (t): 0.192, 500.497/s, 500.497/s/gpu LR: 7.005004e-09 Logit Scale: 100.000 Contrastive_loss: 0.25379 (0.22495) Loss: 0.25379 (0.22495)
2024-08-30,00:30:33 | INFO | Train Epoch: 36 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 534.303/s, 534.303/s/gpu LR: 6.995406e-09 Logit Scale: 100.000 Contrastive_loss: 0.30184 (0.22721) Loss: 0.30184 (0.22721)
2024-08-30,00:30:52 | INFO | Train Epoch: 36 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 530.571/s, 530.571/s/gpu LR: 6.985800e-09 Logit Scale: 100.000 Contrastive_loss: 0.19496 (0.22629) Loss: 0.19496 (0.22629)
2024-08-30,00:31:11 | INFO | Train Epoch: 36 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 526.277/s, 526.277/s/gpu LR: 6.976185e-09 Logit Scale: 100.000 Contrastive_loss: 0.27513 (0.22764) Loss: 0.27513 (0.22764)
2024-08-30,00:31:31 | INFO | Train Epoch: 36 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 497.934/s, 497.934/s/gpu LR: 6.966561e-09 Logit Scale: 100.000 Contrastive_loss: 0.32014 (0.23014) Loss: 0.32014 (0.23014)
2024-08-30,00:31:50 | INFO | Train Epoch: 36 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.194, 532.646/s, 532.646/s/gpu LR: 6.956929e-09 Logit Scale: 100.000 Contrastive_loss: 0.16843 (0.22852) Loss: 0.16843 (0.22852)
2024-08-30,00:32:09 | INFO | Train Epoch: 36 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 499.501/s, 499.501/s/gpu LR: 6.947288e-09 Logit Scale: 100.000 Contrastive_loss: 0.27358 (0.22968) Loss: 0.27358 (0.22968)
2024-08-30,00:32:29 | INFO | Train Epoch: 36 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 534.968/s, 534.968/s/gpu LR: 6.937638e-09 Logit Scale: 100.000 Contrastive_loss: 0.25069 (0.23020) Loss: 0.25069 (0.23020)
2024-08-30,00:32:48 | INFO | Train Epoch: 36 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.193, 533.904/s, 533.904/s/gpu LR: 6.927980e-09 Logit Scale: 100.000 Contrastive_loss: 0.14633 (0.22816) Loss: 0.14633 (0.22816)
2024-08-30,00:33:07 | INFO | Train Epoch: 36 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 492.932/s, 492.932/s/gpu LR: 6.918314e-09 Logit Scale: 100.000 Contrastive_loss: 0.23945 (0.22842) Loss: 0.23945 (0.22842)
2024-08-30,00:33:27 | INFO | Train Epoch: 36 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 525.725/s, 525.725/s/gpu LR: 6.908639e-09 Logit Scale: 100.000 Contrastive_loss: 0.098153 (0.22540) Loss: 0.098153 (0.22540)
2024-08-30,00:33:34 | INFO | Eval Epoch: 37 [200 / 1000]	Clip Loss: 0.482942	
2024-08-30,00:33:35 | INFO | Eval Epoch: 37 image_to_text_mean_rank: 2.4400	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6630	image_to_text_R@5: 0.9120	image_to_text_R@10: 0.9600	text_to_image_mean_rank: 3.6830	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6750	text_to_image_R@5: 0.9020	text_to_image_R@10: 0.9660	clip_val_loss: 0.4406	epoch: 37.0000	num_samples: 1000.0000
2024-08-30,00:33:36 | INFO | Start epoch 37
2024-08-30,00:33:36 | INFO | Train Epoch: 37 [   100/145000.0 (0%)] Data (t): 0.023 Batch (t): 0.110, 909.092/s, 909.092/s/gpu LR: 7.162063e-09 Logit Scale: 100.000 Contrastive_loss: 0.18312 (0.18312) Loss: 0.18312 (0.18312)
2024-08-30,00:33:56 | INFO | Train Epoch: 37 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 534.265/s, 534.265/s/gpu LR: 7.152616e-09 Logit Scale: 100.000 Contrastive_loss: 0.35550 (0.26931) Loss: 0.35550 (0.26931)
2024-08-30,00:34:15 | INFO | Train Epoch: 37 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.193, 528.301/s, 528.301/s/gpu LR: 7.143160e-09 Logit Scale: 100.000 Contrastive_loss: 0.37422 (0.30428) Loss: 0.37422 (0.30428)
2024-08-30,00:34:34 | INFO | Train Epoch: 37 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 531.934/s, 531.934/s/gpu LR: 7.133694e-09 Logit Scale: 100.000 Contrastive_loss: 0.19927 (0.27803) Loss: 0.19927 (0.27803)
2024-08-30,00:34:54 | INFO | Train Epoch: 37 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.194, 520.413/s, 520.413/s/gpu LR: 7.124218e-09 Logit Scale: 100.000 Contrastive_loss: 0.14283 (0.25099) Loss: 0.14283 (0.25099)
2024-08-30,00:35:13 | INFO | Train Epoch: 37 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.192, 530.634/s, 530.634/s/gpu LR: 7.114734e-09 Logit Scale: 100.000 Contrastive_loss: 0.17515 (0.23835) Loss: 0.17515 (0.23835)
2024-08-30,00:35:32 | INFO | Train Epoch: 37 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 536.749/s, 536.749/s/gpu LR: 7.105240e-09 Logit Scale: 100.000 Contrastive_loss: 0.12619 (0.22233) Loss: 0.12619 (0.22233)
2024-08-30,00:35:51 | INFO | Train Epoch: 37 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 532.180/s, 532.180/s/gpu LR: 7.095737e-09 Logit Scale: 100.000 Contrastive_loss: 0.24341 (0.22496) Loss: 0.24341 (0.22496)
2024-08-30,00:36:11 | INFO | Train Epoch: 37 [ 80100/145000.0 (55%)] Data (t): 0.110 Batch (t): 0.194, 520.033/s, 520.033/s/gpu LR: 7.086224e-09 Logit Scale: 100.000 Contrastive_loss: 0.14561 (0.21615) Loss: 0.14561 (0.21615)
2024-08-30,00:36:30 | INFO | Train Epoch: 37 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 499.081/s, 499.081/s/gpu LR: 7.076703e-09 Logit Scale: 100.000 Contrastive_loss: 0.22823 (0.21735) Loss: 0.22823 (0.21735)
2024-08-30,00:36:49 | INFO | Train Epoch: 37 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 496.822/s, 496.822/s/gpu LR: 7.067172e-09 Logit Scale: 100.000 Contrastive_loss: 0.16297 (0.21241) Loss: 0.16297 (0.21241)
2024-08-30,00:37:09 | INFO | Train Epoch: 37 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.193, 503.772/s, 503.772/s/gpu LR: 7.057632e-09 Logit Scale: 100.000 Contrastive_loss: 0.18545 (0.21016) Loss: 0.18545 (0.21016)
2024-08-30,00:37:28 | INFO | Train Epoch: 37 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 503.539/s, 503.539/s/gpu LR: 7.048084e-09 Logit Scale: 100.000 Contrastive_loss: 0.29197 (0.21646) Loss: 0.29197 (0.21646)
2024-08-30,00:37:47 | INFO | Train Epoch: 37 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 525.653/s, 525.653/s/gpu LR: 7.038526e-09 Logit Scale: 100.000 Contrastive_loss: 0.16279 (0.21262) Loss: 0.16279 (0.21262)
2024-08-30,00:38:06 | INFO | Train Epoch: 37 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.192, 497.700/s, 497.700/s/gpu LR: 7.028959e-09 Logit Scale: 100.000 Contrastive_loss: 0.18746 (0.21095) Loss: 0.18746 (0.21095)
2024-08-30,00:38:16 | INFO | Train Epoch: 37 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 536.347/s, 536.347/s/gpu LR: 7.024268e-09 Logit Scale: 100.000 Contrastive_loss: 0.24039 (0.21279) Loss: 0.24039 (0.21279)
2024-08-30,00:38:26 | INFO | Train Epoch: 37 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.194, 500.003/s, 500.003/s/gpu LR: 7.019384e-09 Logit Scale: 100.000 Contrastive_loss: 0.21380 (0.21285) Loss: 0.21380 (0.21285)
2024-08-30,00:38:45 | INFO | Train Epoch: 37 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 518.962/s, 518.962/s/gpu LR: 7.009799e-09 Logit Scale: 100.000 Contrastive_loss: 0.25164 (0.21500) Loss: 0.25164 (0.21500)
2024-08-30,00:39:04 | INFO | Train Epoch: 37 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 534.827/s, 534.827/s/gpu LR: 7.000206e-09 Logit Scale: 100.000 Contrastive_loss: 0.18811 (0.21359) Loss: 0.18811 (0.21359)
2024-08-30,00:39:24 | INFO | Train Epoch: 37 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 532.993/s, 532.993/s/gpu LR: 6.990604e-09 Logit Scale: 100.000 Contrastive_loss: 0.20745 (0.21328) Loss: 0.20745 (0.21328)
2024-08-30,00:39:43 | INFO | Train Epoch: 37 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 531.654/s, 531.654/s/gpu LR: 6.980994e-09 Logit Scale: 100.000 Contrastive_loss: 0.26004 (0.21551) Loss: 0.26004 (0.21551)
2024-08-30,00:40:02 | INFO | Train Epoch: 37 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 502.183/s, 502.183/s/gpu LR: 6.971374e-09 Logit Scale: 100.000 Contrastive_loss: 0.17477 (0.21365) Loss: 0.17477 (0.21365)
2024-08-30,00:40:22 | INFO | Train Epoch: 37 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 521.822/s, 521.822/s/gpu LR: 6.961746e-09 Logit Scale: 100.000 Contrastive_loss: 0.26483 (0.21588) Loss: 0.26483 (0.21588)
2024-08-30,00:40:41 | INFO | Train Epoch: 37 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 493.576/s, 493.576/s/gpu LR: 6.952109e-09 Logit Scale: 100.000 Contrastive_loss: 0.33304 (0.22076) Loss: 0.33304 (0.22076)
2024-08-30,00:41:00 | INFO | Train Epoch: 37 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 498.896/s, 498.896/s/gpu LR: 6.942464e-09 Logit Scale: 100.000 Contrastive_loss: 0.31412 (0.22450) Loss: 0.31412 (0.22450)
2024-08-30,00:41:19 | INFO | Train Epoch: 37 [240100/145000.0 (166%)] Data (t): 0.107 Batch (t): 0.192, 527.869/s, 527.869/s/gpu LR: 6.932810e-09 Logit Scale: 100.000 Contrastive_loss: 0.18075 (0.22281) Loss: 0.18075 (0.22281)
2024-08-30,00:41:39 | INFO | Train Epoch: 37 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 508.317/s, 508.317/s/gpu LR: 6.923148e-09 Logit Scale: 100.000 Contrastive_loss: 0.22709 (0.22297) Loss: 0.22709 (0.22297)
2024-08-30,00:41:58 | INFO | Train Epoch: 37 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 504.186/s, 504.186/s/gpu LR: 6.913478e-09 Logit Scale: 100.000 Contrastive_loss: 0.23285 (0.22332) Loss: 0.23285 (0.22332)
2024-08-30,00:42:17 | INFO | Train Epoch: 37 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 539.669/s, 539.669/s/gpu LR: 6.903799e-09 Logit Scale: 100.000 Contrastive_loss: 0.26714 (0.22483) Loss: 0.26714 (0.22483)
2024-08-30,00:42:37 | INFO | Train Epoch: 37 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 526.453/s, 526.453/s/gpu LR: 6.894111e-09 Logit Scale: 100.000 Contrastive_loss: 0.18071 (0.22336) Loss: 0.18071 (0.22336)
2024-08-30,00:42:56 | INFO | Train Epoch: 37 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 504.063/s, 504.063/s/gpu LR: 6.884415e-09 Logit Scale: 100.000 Contrastive_loss: 0.15120 (0.22104) Loss: 0.15120 (0.22104)
2024-08-30,00:43:15 | INFO | Train Epoch: 37 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 536.769/s, 536.769/s/gpu LR: 6.874712e-09 Logit Scale: 100.000 Contrastive_loss: 0.27827 (0.22282) Loss: 0.27827 (0.22282)
2024-08-30,00:43:35 | INFO | Train Epoch: 37 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 506.568/s, 506.568/s/gpu LR: 6.864999e-09 Logit Scale: 100.000 Contrastive_loss: 0.25293 (0.22374) Loss: 0.25293 (0.22374)
2024-08-30,00:43:54 | INFO | Train Epoch: 37 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 525.022/s, 525.022/s/gpu LR: 6.855279e-09 Logit Scale: 100.000 Contrastive_loss: 0.30021 (0.22599) Loss: 0.30021 (0.22599)
2024-08-30,00:44:13 | INFO | Train Epoch: 37 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.193, 529.933/s, 529.933/s/gpu LR: 6.845550e-09 Logit Scale: 100.000 Contrastive_loss: 0.19427 (0.22508) Loss: 0.19427 (0.22508)
2024-08-30,00:44:32 | INFO | Train Epoch: 37 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.193, 498.911/s, 498.911/s/gpu LR: 6.835814e-09 Logit Scale: 100.000 Contrastive_loss: 0.27314 (0.22641) Loss: 0.27314 (0.22641)
2024-08-30,00:44:52 | INFO | Train Epoch: 37 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 527.532/s, 527.532/s/gpu LR: 6.826069e-09 Logit Scale: 100.000 Contrastive_loss: 0.31823 (0.22890) Loss: 0.31823 (0.22890)
2024-08-30,00:45:11 | INFO | Train Epoch: 37 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.194, 524.743/s, 524.743/s/gpu LR: 6.816316e-09 Logit Scale: 100.000 Contrastive_loss: 0.16742 (0.22728) Loss: 0.16742 (0.22728)
2024-08-30,00:45:30 | INFO | Train Epoch: 37 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 539.027/s, 539.027/s/gpu LR: 6.806556e-09 Logit Scale: 100.000 Contrastive_loss: 0.27251 (0.22844) Loss: 0.27251 (0.22844)
2024-08-30,00:45:50 | INFO | Train Epoch: 37 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 531.076/s, 531.076/s/gpu LR: 6.796787e-09 Logit Scale: 100.000 Contrastive_loss: 0.24950 (0.22896) Loss: 0.24950 (0.22896)
2024-08-30,00:46:09 | INFO | Train Epoch: 37 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 525.337/s, 525.337/s/gpu LR: 6.787011e-09 Logit Scale: 100.000 Contrastive_loss: 0.14520 (0.22692) Loss: 0.14520 (0.22692)
2024-08-30,00:46:28 | INFO | Train Epoch: 37 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.193, 496.990/s, 496.990/s/gpu LR: 6.777227e-09 Logit Scale: 100.000 Contrastive_loss: 0.23814 (0.22719) Loss: 0.23814 (0.22719)
2024-08-30,00:46:48 | INFO | Train Epoch: 37 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 500.092/s, 500.092/s/gpu LR: 6.767434e-09 Logit Scale: 100.000 Contrastive_loss: 0.097355 (0.22417) Loss: 0.097355 (0.22417)
2024-08-30,00:46:55 | INFO | Eval Epoch: 38 [200 / 1000]	Clip Loss: 0.482776	
2024-08-30,00:46:56 | INFO | Eval Epoch: 38 image_to_text_mean_rank: 2.4400	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6620	image_to_text_R@5: 0.9120	image_to_text_R@10: 0.9610	text_to_image_mean_rank: 3.6900	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6740	text_to_image_R@5: 0.9030	text_to_image_R@10: 0.9660	clip_val_loss: 0.4404	epoch: 38.0000	num_samples: 1000.0000
2024-08-30,00:46:57 | INFO | Start epoch 38
2024-08-30,00:46:57 | INFO | Train Epoch: 38 [   100/145000.0 (0%)] Data (t): 0.019 Batch (t): 0.108, 928.911/s, 928.911/s/gpu LR: 7.024173e-09 Logit Scale: 100.000 Contrastive_loss: 0.18202 (0.18202) Loss: 0.18202 (0.18202)
2024-08-30,00:47:17 | INFO | Train Epoch: 38 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 495.598/s, 495.598/s/gpu LR: 7.014593e-09 Logit Scale: 100.000 Contrastive_loss: 0.35389 (0.26796) Loss: 0.35389 (0.26796)
2024-08-30,00:47:36 | INFO | Train Epoch: 38 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.194, 502.594/s, 502.594/s/gpu LR: 7.005004e-09 Logit Scale: 100.000 Contrastive_loss: 0.37216 (0.30269) Loss: 0.37216 (0.30269)
2024-08-30,00:47:56 | INFO | Train Epoch: 38 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 496.120/s, 496.120/s/gpu LR: 6.995406e-09 Logit Scale: 100.000 Contrastive_loss: 0.19781 (0.27647) Loss: 0.19781 (0.27647)
2024-08-30,00:48:15 | INFO | Train Epoch: 38 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 486.552/s, 486.552/s/gpu LR: 6.985800e-09 Logit Scale: 100.000 Contrastive_loss: 0.14212 (0.24960) Loss: 0.14212 (0.24960)
2024-08-30,00:48:34 | INFO | Train Epoch: 38 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.192, 491.914/s, 491.914/s/gpu LR: 6.976185e-09 Logit Scale: 100.000 Contrastive_loss: 0.17386 (0.23698) Loss: 0.17386 (0.23698)
2024-08-30,00:48:53 | INFO | Train Epoch: 38 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 519.495/s, 519.495/s/gpu LR: 6.966561e-09 Logit Scale: 100.000 Contrastive_loss: 0.12543 (0.22104) Loss: 0.12543 (0.22104)
2024-08-30,00:49:13 | INFO | Train Epoch: 38 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.194, 490.178/s, 490.178/s/gpu LR: 6.956929e-09 Logit Scale: 100.000 Contrastive_loss: 0.24157 (0.22361) Loss: 0.24157 (0.22361)
2024-08-30,00:49:32 | INFO | Train Epoch: 38 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 534.131/s, 534.131/s/gpu LR: 6.947288e-09 Logit Scale: 100.000 Contrastive_loss: 0.14433 (0.21480) Loss: 0.14433 (0.21480)
2024-08-30,00:49:51 | INFO | Train Epoch: 38 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.192, 544.247/s, 544.247/s/gpu LR: 6.937638e-09 Logit Scale: 100.000 Contrastive_loss: 0.22694 (0.21601) Loss: 0.22694 (0.21601)
2024-08-30,00:50:11 | INFO | Train Epoch: 38 [100100/145000.0 (69%)] Data (t): 0.110 Batch (t): 0.194, 495.011/s, 495.011/s/gpu LR: 6.927980e-09 Logit Scale: 100.000 Contrastive_loss: 0.16216 (0.21112) Loss: 0.16216 (0.21112)
2024-08-30,00:50:30 | INFO | Train Epoch: 38 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 505.744/s, 505.744/s/gpu LR: 6.918314e-09 Logit Scale: 100.000 Contrastive_loss: 0.18432 (0.20889) Loss: 0.18432 (0.20889)
2024-08-30,00:50:49 | INFO | Train Epoch: 38 [120100/145000.0 (83%)] Data (t): 0.107 Batch (t): 0.192, 522.677/s, 522.677/s/gpu LR: 6.908639e-09 Logit Scale: 100.000 Contrastive_loss: 0.29037 (0.21515) Loss: 0.29037 (0.21515)
2024-08-30,00:51:08 | INFO | Train Epoch: 38 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 532.144/s, 532.144/s/gpu LR: 6.898956e-09 Logit Scale: 100.000 Contrastive_loss: 0.16182 (0.21134) Loss: 0.16182 (0.21134)
2024-08-30,00:51:28 | INFO | Train Epoch: 38 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.192, 537.571/s, 537.571/s/gpu LR: 6.889264e-09 Logit Scale: 100.000 Contrastive_loss: 0.18680 (0.20971) Loss: 0.18680 (0.20971)
2024-08-30,00:51:37 | INFO | Train Epoch: 38 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 524.793/s, 524.793/s/gpu LR: 6.884512e-09 Logit Scale: 100.000 Contrastive_loss: 0.23923 (0.21155) Loss: 0.23923 (0.21155)
2024-08-30,00:51:47 | INFO | Train Epoch: 38 [150100/145000.0 (104%)] Data (t): 0.110 Batch (t): 0.194, 503.972/s, 503.972/s/gpu LR: 6.879565e-09 Logit Scale: 100.000 Contrastive_loss: 0.21239 (0.21160) Loss: 0.21239 (0.21160)
2024-08-30,00:52:06 | INFO | Train Epoch: 38 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.192, 524.355/s, 524.355/s/gpu LR: 6.869856e-09 Logit Scale: 100.000 Contrastive_loss: 0.24967 (0.21372) Loss: 0.24967 (0.21372)
2024-08-30,00:52:26 | INFO | Train Epoch: 38 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 509.816/s, 509.816/s/gpu LR: 6.860140e-09 Logit Scale: 100.000 Contrastive_loss: 0.18693 (0.21231) Loss: 0.18693 (0.21231)
2024-08-30,00:52:45 | INFO | Train Epoch: 38 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 495.213/s, 495.213/s/gpu LR: 6.850416e-09 Logit Scale: 100.000 Contrastive_loss: 0.20648 (0.21202) Loss: 0.20648 (0.21202)
2024-08-30,00:53:04 | INFO | Train Epoch: 38 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 535.521/s, 535.521/s/gpu LR: 6.840683e-09 Logit Scale: 100.000 Contrastive_loss: 0.25871 (0.21424) Loss: 0.25871 (0.21424)
2024-08-30,00:53:23 | INFO | Train Epoch: 38 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.192, 526.688/s, 526.688/s/gpu LR: 6.830942e-09 Logit Scale: 100.000 Contrastive_loss: 0.17390 (0.21241) Loss: 0.17390 (0.21241)
2024-08-30,00:53:43 | INFO | Train Epoch: 38 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 535.600/s, 535.600/s/gpu LR: 6.821194e-09 Logit Scale: 100.000 Contrastive_loss: 0.26370 (0.21464) Loss: 0.26370 (0.21464)
2024-08-30,00:54:02 | INFO | Train Epoch: 38 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 536.934/s, 536.934/s/gpu LR: 6.811437e-09 Logit Scale: 100.000 Contrastive_loss: 0.33087 (0.21948) Loss: 0.33087 (0.21948)
2024-08-30,00:54:21 | INFO | Train Epoch: 38 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 493.101/s, 493.101/s/gpu LR: 6.801673e-09 Logit Scale: 100.000 Contrastive_loss: 0.31293 (0.22322) Loss: 0.31293 (0.22322)
2024-08-30,00:54:40 | INFO | Train Epoch: 38 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 520.453/s, 520.453/s/gpu LR: 6.791900e-09 Logit Scale: 100.000 Contrastive_loss: 0.17943 (0.22153) Loss: 0.17943 (0.22153)
2024-08-30,00:55:00 | INFO | Train Epoch: 38 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 513.827/s, 513.827/s/gpu LR: 6.782120e-09 Logit Scale: 100.000 Contrastive_loss: 0.22609 (0.22170) Loss: 0.22609 (0.22170)
2024-08-30,00:55:19 | INFO | Train Epoch: 38 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 495.338/s, 495.338/s/gpu LR: 6.772331e-09 Logit Scale: 100.000 Contrastive_loss: 0.23159 (0.22205) Loss: 0.23159 (0.22205)
2024-08-30,00:55:38 | INFO | Train Epoch: 38 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 536.066/s, 536.066/s/gpu LR: 6.762536e-09 Logit Scale: 100.000 Contrastive_loss: 0.26630 (0.22358) Loss: 0.26630 (0.22358)
2024-08-30,00:55:57 | INFO | Train Epoch: 38 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 528.383/s, 528.383/s/gpu LR: 6.752732e-09 Logit Scale: 100.000 Contrastive_loss: 0.17977 (0.22212) Loss: 0.17977 (0.22212)
2024-08-30,00:56:17 | INFO | Train Epoch: 38 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.192, 496.898/s, 496.898/s/gpu LR: 6.742920e-09 Logit Scale: 100.000 Contrastive_loss: 0.15071 (0.21982) Loss: 0.15071 (0.21982)
2024-08-30,00:56:36 | INFO | Train Epoch: 38 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 527.688/s, 527.688/s/gpu LR: 6.733101e-09 Logit Scale: 100.000 Contrastive_loss: 0.27701 (0.22160) Loss: 0.27701 (0.22160)
2024-08-30,00:56:55 | INFO | Train Epoch: 38 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 539.687/s, 539.687/s/gpu LR: 6.723275e-09 Logit Scale: 100.000 Contrastive_loss: 0.25217 (0.22253) Loss: 0.25217 (0.22253)
2024-08-30,00:57:15 | INFO | Train Epoch: 38 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.194, 491.187/s, 491.187/s/gpu LR: 6.713441e-09 Logit Scale: 100.000 Contrastive_loss: 0.29857 (0.22477) Loss: 0.29857 (0.22477)
2024-08-30,00:57:34 | INFO | Train Epoch: 38 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 530.419/s, 530.419/s/gpu LR: 6.703599e-09 Logit Scale: 100.000 Contrastive_loss: 0.19360 (0.22388) Loss: 0.19360 (0.22388)
2024-08-30,00:57:53 | INFO | Train Epoch: 38 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.193, 501.304/s, 501.304/s/gpu LR: 6.693750e-09 Logit Scale: 100.000 Contrastive_loss: 0.27103 (0.22519) Loss: 0.27103 (0.22519)
2024-08-30,00:58:12 | INFO | Train Epoch: 38 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 535.761/s, 535.761/s/gpu LR: 6.683893e-09 Logit Scale: 100.000 Contrastive_loss: 0.31668 (0.22766) Loss: 0.31668 (0.22766)
2024-08-30,00:58:32 | INFO | Train Epoch: 38 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.192, 523.939/s, 523.939/s/gpu LR: 6.674029e-09 Logit Scale: 100.000 Contrastive_loss: 0.16648 (0.22605) Loss: 0.16648 (0.22605)
2024-08-30,00:58:51 | INFO | Train Epoch: 38 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 503.085/s, 503.085/s/gpu LR: 6.664158e-09 Logit Scale: 100.000 Contrastive_loss: 0.27133 (0.22721) Loss: 0.27133 (0.22721)
2024-08-30,00:59:10 | INFO | Train Epoch: 38 [380100/145000.0 (262%)] Data (t): 0.107 Batch (t): 0.192, 532.267/s, 532.267/s/gpu LR: 6.654279e-09 Logit Scale: 100.000 Contrastive_loss: 0.24822 (0.22773) Loss: 0.24822 (0.22773)
2024-08-30,00:59:29 | INFO | Train Epoch: 38 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 494.625/s, 494.625/s/gpu LR: 6.644393e-09 Logit Scale: 100.000 Contrastive_loss: 0.14413 (0.22570) Loss: 0.14413 (0.22570)
2024-08-30,00:59:49 | INFO | Train Epoch: 38 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 522.949/s, 522.949/s/gpu LR: 6.634500e-09 Logit Scale: 100.000 Contrastive_loss: 0.23709 (0.22597) Loss: 0.23709 (0.22597)
2024-08-30,01:00:08 | INFO | Train Epoch: 38 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 528.272/s, 528.272/s/gpu LR: 6.624600e-09 Logit Scale: 100.000 Contrastive_loss: 0.096711 (0.22296) Loss: 0.096711 (0.22296)
2024-08-30,01:00:15 | INFO | Eval Epoch: 39 [200 / 1000]	Clip Loss: 0.482546	
2024-08-30,01:00:16 | INFO | Eval Epoch: 39 image_to_text_mean_rank: 2.4380	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6620	image_to_text_R@5: 0.9120	image_to_text_R@10: 0.9620	text_to_image_mean_rank: 3.6970	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6730	text_to_image_R@5: 0.9040	text_to_image_R@10: 0.9670	clip_val_loss: 0.4401	epoch: 39.0000	num_samples: 1000.0000
2024-08-30,01:00:17 | INFO | Start epoch 39
2024-08-30,01:00:18 | INFO | Train Epoch: 39 [   100/145000.0 (0%)] Data (t): 0.033 Batch (t): 0.112, 892.726/s, 892.726/s/gpu LR: 6.884415e-09 Logit Scale: 100.000 Contrastive_loss: 0.18102 (0.18102) Loss: 0.18102 (0.18102)
2024-08-30,01:00:37 | INFO | Train Epoch: 39 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 536.731/s, 536.731/s/gpu LR: 6.874712e-09 Logit Scale: 100.000 Contrastive_loss: 0.35239 (0.26671) Loss: 0.35239 (0.26671)
2024-08-30,01:00:56 | INFO | Train Epoch: 39 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 497.479/s, 497.479/s/gpu LR: 6.864999e-09 Logit Scale: 100.000 Contrastive_loss: 0.37043 (0.30128) Loss: 0.37043 (0.30128)
2024-08-30,01:01:16 | INFO | Train Epoch: 39 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.194, 531.952/s, 531.952/s/gpu LR: 6.855279e-09 Logit Scale: 100.000 Contrastive_loss: 0.19659 (0.27511) Loss: 0.19659 (0.27511)
2024-08-30,01:01:35 | INFO | Train Epoch: 39 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 537.489/s, 537.489/s/gpu LR: 6.845550e-09 Logit Scale: 100.000 Contrastive_loss: 0.14143 (0.24837) Loss: 0.14143 (0.24837)
2024-08-30,01:01:54 | INFO | Train Epoch: 39 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 493.082/s, 493.082/s/gpu LR: 6.835814e-09 Logit Scale: 100.000 Contrastive_loss: 0.17276 (0.23577) Loss: 0.17276 (0.23577)
2024-08-30,01:02:14 | INFO | Train Epoch: 39 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 523.709/s, 523.709/s/gpu LR: 6.826069e-09 Logit Scale: 100.000 Contrastive_loss: 0.12468 (0.21990) Loss: 0.12468 (0.21990)
2024-08-30,01:02:33 | INFO | Train Epoch: 39 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 536.783/s, 536.783/s/gpu LR: 6.816316e-09 Logit Scale: 100.000 Contrastive_loss: 0.23984 (0.22239) Loss: 0.23984 (0.22239)
2024-08-30,01:02:52 | INFO | Train Epoch: 39 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 515.671/s, 515.671/s/gpu LR: 6.806556e-09 Logit Scale: 100.000 Contrastive_loss: 0.14325 (0.21360) Loss: 0.14325 (0.21360)
2024-08-30,01:03:12 | INFO | Train Epoch: 39 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 521.974/s, 521.974/s/gpu LR: 6.796787e-09 Logit Scale: 100.000 Contrastive_loss: 0.22593 (0.21483) Loss: 0.22593 (0.21483)
2024-08-30,01:03:31 | INFO | Train Epoch: 39 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 505.969/s, 505.969/s/gpu LR: 6.787011e-09 Logit Scale: 100.000 Contrastive_loss: 0.16139 (0.20997) Loss: 0.16139 (0.20997)
2024-08-30,01:03:50 | INFO | Train Epoch: 39 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 534.508/s, 534.508/s/gpu LR: 6.777227e-09 Logit Scale: 100.000 Contrastive_loss: 0.18309 (0.20773) Loss: 0.18309 (0.20773)
2024-08-30,01:04:09 | INFO | Train Epoch: 39 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 530.023/s, 530.023/s/gpu LR: 6.767434e-09 Logit Scale: 100.000 Contrastive_loss: 0.28878 (0.21397) Loss: 0.28878 (0.21397)
2024-08-30,01:04:29 | INFO | Train Epoch: 39 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 525.455/s, 525.455/s/gpu LR: 6.757635e-09 Logit Scale: 100.000 Contrastive_loss: 0.16096 (0.21018) Loss: 0.16096 (0.21018)
2024-08-30,01:04:48 | INFO | Train Epoch: 39 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 536.486/s, 536.486/s/gpu LR: 6.747827e-09 Logit Scale: 100.000 Contrastive_loss: 0.18602 (0.20857) Loss: 0.18602 (0.20857)
2024-08-30,01:04:57 | INFO | Train Epoch: 39 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.194, 493.327/s, 493.327/s/gpu LR: 6.743019e-09 Logit Scale: 100.000 Contrastive_loss: 0.23804 (0.21041) Loss: 0.23804 (0.21041)
2024-08-30,01:05:07 | INFO | Train Epoch: 39 [150100/145000.0 (104%)] Data (t): 0.110 Batch (t): 0.194, 501.560/s, 501.560/s/gpu LR: 6.738012e-09 Logit Scale: 100.000 Contrastive_loss: 0.21106 (0.21045) Loss: 0.21106 (0.21045)
2024-08-30,01:05:27 | INFO | Train Epoch: 39 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 504.607/s, 504.607/s/gpu LR: 6.728189e-09 Logit Scale: 100.000 Contrastive_loss: 0.24791 (0.21253) Loss: 0.24791 (0.21253)
2024-08-30,01:05:46 | INFO | Train Epoch: 39 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 535.264/s, 535.264/s/gpu LR: 6.718359e-09 Logit Scale: 100.000 Contrastive_loss: 0.18569 (0.21112) Loss: 0.18569 (0.21112)
2024-08-30,01:06:05 | INFO | Train Epoch: 39 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 526.980/s, 526.980/s/gpu LR: 6.708521e-09 Logit Scale: 100.000 Contrastive_loss: 0.20576 (0.21085) Loss: 0.20576 (0.21085)
2024-08-30,01:06:25 | INFO | Train Epoch: 39 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 497.397/s, 497.397/s/gpu LR: 6.698675e-09 Logit Scale: 100.000 Contrastive_loss: 0.25739 (0.21307) Loss: 0.25739 (0.21307)
2024-08-30,01:06:44 | INFO | Train Epoch: 39 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 501.558/s, 501.558/s/gpu LR: 6.688822e-09 Logit Scale: 100.000 Contrastive_loss: 0.17324 (0.21126) Loss: 0.17324 (0.21126)
2024-08-30,01:07:03 | INFO | Train Epoch: 39 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 491.094/s, 491.094/s/gpu LR: 6.678962e-09 Logit Scale: 100.000 Contrastive_loss: 0.26247 (0.21348) Loss: 0.26247 (0.21348)
2024-08-30,01:07:22 | INFO | Train Epoch: 39 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 496.705/s, 496.705/s/gpu LR: 6.669094e-09 Logit Scale: 100.000 Contrastive_loss: 0.32896 (0.21830) Loss: 0.32896 (0.21830)
2024-08-30,01:07:42 | INFO | Train Epoch: 39 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 527.481/s, 527.481/s/gpu LR: 6.659219e-09 Logit Scale: 100.000 Contrastive_loss: 0.31185 (0.22204) Loss: 0.31185 (0.22204)
2024-08-30,01:08:01 | INFO | Train Epoch: 39 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 493.148/s, 493.148/s/gpu LR: 6.649337e-09 Logit Scale: 100.000 Contrastive_loss: 0.17807 (0.22035) Loss: 0.17807 (0.22035)
2024-08-30,01:08:20 | INFO | Train Epoch: 39 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 538.079/s, 538.079/s/gpu LR: 6.639448e-09 Logit Scale: 100.000 Contrastive_loss: 0.22525 (0.22053) Loss: 0.22525 (0.22053)
2024-08-30,01:08:40 | INFO | Train Epoch: 39 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 500.109/s, 500.109/s/gpu LR: 6.629551e-09 Logit Scale: 100.000 Contrastive_loss: 0.23032 (0.22088) Loss: 0.23032 (0.22088)
2024-08-30,01:08:59 | INFO | Train Epoch: 39 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.194, 497.834/s, 497.834/s/gpu LR: 6.619647e-09 Logit Scale: 100.000 Contrastive_loss: 0.26550 (0.22242) Loss: 0.26550 (0.22242)
2024-08-30,01:09:18 | INFO | Train Epoch: 39 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.192, 499.834/s, 499.834/s/gpu LR: 6.609736e-09 Logit Scale: 100.000 Contrastive_loss: 0.17877 (0.22096) Loss: 0.17877 (0.22096)
2024-08-30,01:09:38 | INFO | Train Epoch: 39 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 526.984/s, 526.984/s/gpu LR: 6.599818e-09 Logit Scale: 100.000 Contrastive_loss: 0.15019 (0.21868) Loss: 0.15019 (0.21868)
2024-08-30,01:09:57 | INFO | Train Epoch: 39 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 528.961/s, 528.961/s/gpu LR: 6.589893e-09 Logit Scale: 100.000 Contrastive_loss: 0.27566 (0.22046) Loss: 0.27566 (0.22046)
2024-08-30,01:10:16 | INFO | Train Epoch: 39 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.194, 529.508/s, 529.508/s/gpu LR: 6.579961e-09 Logit Scale: 100.000 Contrastive_loss: 0.25136 (0.22140) Loss: 0.25136 (0.22140)
2024-08-30,01:10:35 | INFO | Train Epoch: 39 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 495.761/s, 495.761/s/gpu LR: 6.570022e-09 Logit Scale: 100.000 Contrastive_loss: 0.29706 (0.22362) Loss: 0.29706 (0.22362)
2024-08-30,01:10:55 | INFO | Train Epoch: 39 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.193, 500.702/s, 500.702/s/gpu LR: 6.560076e-09 Logit Scale: 100.000 Contrastive_loss: 0.19282 (0.22274) Loss: 0.19282 (0.22274)
2024-08-30,01:11:14 | INFO | Train Epoch: 39 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 534.502/s, 534.502/s/gpu LR: 6.550124e-09 Logit Scale: 100.000 Contrastive_loss: 0.26910 (0.22403) Loss: 0.26910 (0.22403)
2024-08-30,01:11:33 | INFO | Train Epoch: 39 [350100/145000.0 (241%)] Data (t): 0.107 Batch (t): 0.192, 526.727/s, 526.727/s/gpu LR: 6.540164e-09 Logit Scale: 100.000 Contrastive_loss: 0.31502 (0.22649) Loss: 0.31502 (0.22649)
2024-08-30,01:11:53 | INFO | Train Epoch: 39 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.192, 495.727/s, 495.727/s/gpu LR: 6.530198e-09 Logit Scale: 100.000 Contrastive_loss: 0.16569 (0.22489) Loss: 0.16569 (0.22489)
2024-08-30,01:12:12 | INFO | Train Epoch: 39 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 497.334/s, 497.334/s/gpu LR: 6.520225e-09 Logit Scale: 100.000 Contrastive_loss: 0.27015 (0.22605) Loss: 0.27015 (0.22605)
2024-08-30,01:12:31 | INFO | Train Epoch: 39 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 535.237/s, 535.237/s/gpu LR: 6.510245e-09 Logit Scale: 100.000 Contrastive_loss: 0.24716 (0.22658) Loss: 0.24716 (0.22658)
2024-08-30,01:12:50 | INFO | Train Epoch: 39 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 502.532/s, 502.532/s/gpu LR: 6.500259e-09 Logit Scale: 100.000 Contrastive_loss: 0.14304 (0.22454) Loss: 0.14304 (0.22454)
2024-08-30,01:13:10 | INFO | Train Epoch: 39 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.193, 532.700/s, 532.700/s/gpu LR: 6.490266e-09 Logit Scale: 100.000 Contrastive_loss: 0.23586 (0.22481) Loss: 0.23586 (0.22481)
2024-08-30,01:13:29 | INFO | Train Epoch: 39 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 538.830/s, 538.830/s/gpu LR: 6.480267e-09 Logit Scale: 100.000 Contrastive_loss: 0.096010 (0.22181) Loss: 0.096010 (0.22181)
2024-08-30,01:13:37 | INFO | Eval Epoch: 40 [200 / 1000]	Clip Loss: 0.482401	
2024-08-30,01:13:37 | INFO | Eval Epoch: 40 image_to_text_mean_rank: 2.4370	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6630	image_to_text_R@5: 0.9120	image_to_text_R@10: 0.9620	text_to_image_mean_rank: 3.7030	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6730	text_to_image_R@5: 0.9040	text_to_image_R@10: 0.9670	clip_val_loss: 0.4400	epoch: 40.0000	num_samples: 1000.0000
2024-08-30,01:13:39 | INFO | Start epoch 40
2024-08-30,01:13:39 | INFO | Train Epoch: 40 [   100/145000.0 (0%)] Data (t): 0.025 Batch (t): 0.112, 895.518/s, 895.518/s/gpu LR: 6.742920e-09 Logit Scale: 100.000 Contrastive_loss: 0.17993 (0.17993) Loss: 0.17993 (0.17993)
2024-08-30,01:13:58 | INFO | Train Epoch: 40 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 532.489/s, 532.489/s/gpu LR: 6.733101e-09 Logit Scale: 100.000 Contrastive_loss: 0.35114 (0.26553) Loss: 0.35114 (0.26553)
2024-08-30,01:14:18 | INFO | Train Epoch: 40 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.194, 498.878/s, 498.878/s/gpu LR: 6.723275e-09 Logit Scale: 100.000 Contrastive_loss: 0.36854 (0.29987) Loss: 0.36854 (0.29987)
2024-08-30,01:14:37 | INFO | Train Epoch: 40 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 500.552/s, 500.552/s/gpu LR: 6.713441e-09 Logit Scale: 100.000 Contrastive_loss: 0.19537 (0.27374) Loss: 0.19537 (0.27374)
2024-08-30,01:14:56 | INFO | Train Epoch: 40 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 539.427/s, 539.427/s/gpu LR: 6.703599e-09 Logit Scale: 100.000 Contrastive_loss: 0.14067 (0.24713) Loss: 0.14067 (0.24713)
2024-08-30,01:15:16 | INFO | Train Epoch: 40 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 529.422/s, 529.422/s/gpu LR: 6.693750e-09 Logit Scale: 100.000 Contrastive_loss: 0.17166 (0.23455) Loss: 0.17166 (0.23455)
2024-08-30,01:15:35 | INFO | Train Epoch: 40 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 541.048/s, 541.048/s/gpu LR: 6.683893e-09 Logit Scale: 100.000 Contrastive_loss: 0.12392 (0.21874) Loss: 0.12392 (0.21874)
2024-08-30,01:15:54 | INFO | Train Epoch: 40 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 534.849/s, 534.849/s/gpu LR: 6.674029e-09 Logit Scale: 100.000 Contrastive_loss: 0.23821 (0.22118) Loss: 0.23821 (0.22118)
2024-08-30,01:16:13 | INFO | Train Epoch: 40 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.192, 529.841/s, 529.841/s/gpu LR: 6.664158e-09 Logit Scale: 100.000 Contrastive_loss: 0.14204 (0.21239) Loss: 0.14204 (0.21239)
2024-08-30,01:16:33 | INFO | Train Epoch: 40 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 531.513/s, 531.513/s/gpu LR: 6.654279e-09 Logit Scale: 100.000 Contrastive_loss: 0.22451 (0.21360) Loss: 0.22451 (0.21360)
2024-08-30,01:16:52 | INFO | Train Epoch: 40 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 496.313/s, 496.313/s/gpu LR: 6.644393e-09 Logit Scale: 100.000 Contrastive_loss: 0.16047 (0.20877) Loss: 0.16047 (0.20877)
2024-08-30,01:17:11 | INFO | Train Epoch: 40 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 534.298/s, 534.298/s/gpu LR: 6.634500e-09 Logit Scale: 100.000 Contrastive_loss: 0.18192 (0.20653) Loss: 0.18192 (0.20653)
2024-08-30,01:17:31 | INFO | Train Epoch: 40 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 529.389/s, 529.389/s/gpu LR: 6.624600e-09 Logit Scale: 100.000 Contrastive_loss: 0.28731 (0.21274) Loss: 0.28731 (0.21274)
2024-08-30,01:17:50 | INFO | Train Epoch: 40 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 531.931/s, 531.931/s/gpu LR: 6.614692e-09 Logit Scale: 100.000 Contrastive_loss: 0.16020 (0.20899) Loss: 0.16020 (0.20899)
2024-08-30,01:18:09 | INFO | Train Epoch: 40 [140100/145000.0 (97%)] Data (t): 0.107 Batch (t): 0.193, 541.963/s, 541.963/s/gpu LR: 6.604778e-09 Logit Scale: 100.000 Contrastive_loss: 0.18531 (0.20741) Loss: 0.18531 (0.20741)
2024-08-30,01:18:19 | INFO | Train Epoch: 40 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 523.537/s, 523.537/s/gpu LR: 6.599917e-09 Logit Scale: 100.000 Contrastive_loss: 0.23686 (0.20925) Loss: 0.23686 (0.20925)
2024-08-30,01:18:28 | INFO | Train Epoch: 40 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 519.942/s, 519.942/s/gpu LR: 6.594856e-09 Logit Scale: 100.000 Contrastive_loss: 0.20987 (0.20929) Loss: 0.20987 (0.20929)
2024-08-30,01:18:48 | INFO | Train Epoch: 40 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 527.421/s, 527.421/s/gpu LR: 6.584928e-09 Logit Scale: 100.000 Contrastive_loss: 0.24619 (0.21134) Loss: 0.24619 (0.21134)
2024-08-30,01:19:07 | INFO | Train Epoch: 40 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 531.710/s, 531.710/s/gpu LR: 6.574992e-09 Logit Scale: 100.000 Contrastive_loss: 0.18449 (0.20993) Loss: 0.18449 (0.20993)
2024-08-30,01:19:26 | INFO | Train Epoch: 40 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 531.189/s, 531.189/s/gpu LR: 6.565050e-09 Logit Scale: 100.000 Contrastive_loss: 0.20505 (0.20968) Loss: 0.20505 (0.20968)
2024-08-30,01:19:46 | INFO | Train Epoch: 40 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 507.439/s, 507.439/s/gpu LR: 6.555101e-09 Logit Scale: 100.000 Contrastive_loss: 0.25621 (0.21190) Loss: 0.25621 (0.21190)
2024-08-30,01:20:05 | INFO | Train Epoch: 40 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.192, 511.109/s, 511.109/s/gpu LR: 6.545145e-09 Logit Scale: 100.000 Contrastive_loss: 0.17257 (0.21011) Loss: 0.17257 (0.21011)
2024-08-30,01:20:24 | INFO | Train Epoch: 40 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 506.753/s, 506.753/s/gpu LR: 6.535182e-09 Logit Scale: 100.000 Contrastive_loss: 0.26138 (0.21234) Loss: 0.26138 (0.21234)
2024-08-30,01:20:43 | INFO | Train Epoch: 40 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.192, 524.003/s, 524.003/s/gpu LR: 6.525212e-09 Logit Scale: 100.000 Contrastive_loss: 0.32700 (0.21712) Loss: 0.32700 (0.21712)
2024-08-30,01:21:03 | INFO | Train Epoch: 40 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 499.425/s, 499.425/s/gpu LR: 6.515236e-09 Logit Scale: 100.000 Contrastive_loss: 0.31072 (0.22086) Loss: 0.31072 (0.22086)
2024-08-30,01:21:22 | INFO | Train Epoch: 40 [240100/145000.0 (166%)] Data (t): 0.110 Batch (t): 0.194, 500.815/s, 500.815/s/gpu LR: 6.505253e-09 Logit Scale: 100.000 Contrastive_loss: 0.17681 (0.21917) Loss: 0.17681 (0.21917)
2024-08-30,01:21:41 | INFO | Train Epoch: 40 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 527.962/s, 527.962/s/gpu LR: 6.495264e-09 Logit Scale: 100.000 Contrastive_loss: 0.22439 (0.21936) Loss: 0.22439 (0.21936)
2024-08-30,01:22:01 | INFO | Train Epoch: 40 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 526.584/s, 526.584/s/gpu LR: 6.485268e-09 Logit Scale: 100.000 Contrastive_loss: 0.22913 (0.21971) Loss: 0.22913 (0.21971)
2024-08-30,01:22:20 | INFO | Train Epoch: 40 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 536.481/s, 536.481/s/gpu LR: 6.475265e-09 Logit Scale: 100.000 Contrastive_loss: 0.26469 (0.22126) Loss: 0.26469 (0.22126)
2024-08-30,01:22:39 | INFO | Train Epoch: 40 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 538.515/s, 538.515/s/gpu LR: 6.465256e-09 Logit Scale: 100.000 Contrastive_loss: 0.17801 (0.21982) Loss: 0.17801 (0.21982)
2024-08-30,01:22:58 | INFO | Train Epoch: 40 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 528.221/s, 528.221/s/gpu LR: 6.455240e-09 Logit Scale: 100.000 Contrastive_loss: 0.14975 (0.21756) Loss: 0.14975 (0.21756)
2024-08-30,01:23:18 | INFO | Train Epoch: 40 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 496.286/s, 496.286/s/gpu LR: 6.445218e-09 Logit Scale: 100.000 Contrastive_loss: 0.27452 (0.21934) Loss: 0.27452 (0.21934)
2024-08-30,01:23:37 | INFO | Train Epoch: 40 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 496.877/s, 496.877/s/gpu LR: 6.435190e-09 Logit Scale: 100.000 Contrastive_loss: 0.25065 (0.22029) Loss: 0.25065 (0.22029)
2024-08-30,01:23:56 | INFO | Train Epoch: 40 [320100/145000.0 (221%)] Data (t): 0.107 Batch (t): 0.192, 537.093/s, 537.093/s/gpu LR: 6.425156e-09 Logit Scale: 100.000 Contrastive_loss: 0.29559 (0.22250) Loss: 0.29559 (0.22250)
2024-08-30,01:24:15 | INFO | Train Epoch: 40 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 519.590/s, 519.590/s/gpu LR: 6.415115e-09 Logit Scale: 100.000 Contrastive_loss: 0.19221 (0.22164) Loss: 0.19221 (0.22164)
2024-08-30,01:24:34 | INFO | Train Epoch: 40 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.192, 504.156/s, 504.156/s/gpu LR: 6.405068e-09 Logit Scale: 100.000 Contrastive_loss: 0.26702 (0.22290) Loss: 0.26702 (0.22290)
2024-08-30,01:24:54 | INFO | Train Epoch: 40 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.193, 490.105/s, 490.105/s/gpu LR: 6.395015e-09 Logit Scale: 100.000 Contrastive_loss: 0.31351 (0.22535) Loss: 0.31351 (0.22535)
2024-08-30,01:25:13 | INFO | Train Epoch: 40 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 502.160/s, 502.160/s/gpu LR: 6.384956e-09 Logit Scale: 100.000 Contrastive_loss: 0.16467 (0.22375) Loss: 0.16467 (0.22375)
2024-08-30,01:25:32 | INFO | Train Epoch: 40 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.194, 494.400/s, 494.400/s/gpu LR: 6.374890e-09 Logit Scale: 100.000 Contrastive_loss: 0.26918 (0.22491) Loss: 0.26918 (0.22491)
2024-08-30,01:25:52 | INFO | Train Epoch: 40 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 500.391/s, 500.391/s/gpu LR: 6.364819e-09 Logit Scale: 100.000 Contrastive_loss: 0.24592 (0.22544) Loss: 0.24592 (0.22544)
2024-08-30,01:26:11 | INFO | Train Epoch: 40 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 534.195/s, 534.195/s/gpu LR: 6.354742e-09 Logit Scale: 100.000 Contrastive_loss: 0.14188 (0.22340) Loss: 0.14188 (0.22340)
2024-08-30,01:26:30 | INFO | Train Epoch: 40 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.192, 506.849/s, 506.849/s/gpu LR: 6.344658e-09 Logit Scale: 100.000 Contrastive_loss: 0.23463 (0.22367) Loss: 0.23463 (0.22367)
2024-08-30,01:26:50 | INFO | Train Epoch: 40 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 501.718/s, 501.718/s/gpu LR: 6.334569e-09 Logit Scale: 100.000 Contrastive_loss: 0.095334 (0.22068) Loss: 0.095334 (0.22068)
2024-08-30,01:26:57 | INFO | Eval Epoch: 41 [200 / 1000]	Clip Loss: 0.482324	
2024-08-30,01:26:58 | INFO | Eval Epoch: 41 image_to_text_mean_rank: 2.4300	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6660	image_to_text_R@5: 0.9120	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.7240	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9050	text_to_image_R@10: 0.9670	clip_val_loss: 0.4399	epoch: 41.0000	num_samples: 1000.0000
2024-08-30,01:26:59 | INFO | Start epoch 41
2024-08-30,01:26:59 | INFO | Train Epoch: 41 [   100/145000.0 (0%)] Data (t): 0.023 Batch (t): 0.112, 890.087/s, 890.087/s/gpu LR: 6.599818e-09 Logit Scale: 100.000 Contrastive_loss: 0.17900 (0.17900) Loss: 0.17900 (0.17900)
2024-08-30,01:27:19 | INFO | Train Epoch: 41 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 498.096/s, 498.096/s/gpu LR: 6.589893e-09 Logit Scale: 100.000 Contrastive_loss: 0.34983 (0.26442) Loss: 0.34983 (0.26442)
2024-08-30,01:27:38 | INFO | Train Epoch: 41 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 495.389/s, 495.389/s/gpu LR: 6.579961e-09 Logit Scale: 100.000 Contrastive_loss: 0.36677 (0.29853) Loss: 0.36677 (0.29853)
2024-08-30,01:27:57 | INFO | Train Epoch: 41 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 502.755/s, 502.755/s/gpu LR: 6.570022e-09 Logit Scale: 100.000 Contrastive_loss: 0.19401 (0.27240) Loss: 0.19401 (0.27240)
2024-08-30,01:28:17 | INFO | Train Epoch: 41 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 498.323/s, 498.323/s/gpu LR: 6.560076e-09 Logit Scale: 100.000 Contrastive_loss: 0.13995 (0.24591) Loss: 0.13995 (0.24591)
2024-08-30,01:28:36 | INFO | Train Epoch: 41 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 533.490/s, 533.490/s/gpu LR: 6.550124e-09 Logit Scale: 100.000 Contrastive_loss: 0.17062 (0.23336) Loss: 0.17062 (0.23336)
2024-08-30,01:28:55 | INFO | Train Epoch: 41 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 489.708/s, 489.708/s/gpu LR: 6.540164e-09 Logit Scale: 100.000 Contrastive_loss: 0.12335 (0.21765) Loss: 0.12335 (0.21765)
2024-08-30,01:29:15 | INFO | Train Epoch: 41 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 525.580/s, 525.580/s/gpu LR: 6.530198e-09 Logit Scale: 100.000 Contrastive_loss: 0.23626 (0.21997) Loss: 0.23626 (0.21997)
2024-08-30,01:29:34 | INFO | Train Epoch: 41 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 504.612/s, 504.612/s/gpu LR: 6.520225e-09 Logit Scale: 100.000 Contrastive_loss: 0.14101 (0.21120) Loss: 0.14101 (0.21120)
2024-08-30,01:29:53 | INFO | Train Epoch: 41 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 546.580/s, 546.580/s/gpu LR: 6.510245e-09 Logit Scale: 100.000 Contrastive_loss: 0.22356 (0.21244) Loss: 0.22356 (0.21244)
2024-08-30,01:30:12 | INFO | Train Epoch: 41 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 507.288/s, 507.288/s/gpu LR: 6.500259e-09 Logit Scale: 100.000 Contrastive_loss: 0.15967 (0.20764) Loss: 0.15967 (0.20764)
2024-08-30,01:30:32 | INFO | Train Epoch: 41 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.193, 537.296/s, 537.296/s/gpu LR: 6.490266e-09 Logit Scale: 100.000 Contrastive_loss: 0.18094 (0.20542) Loss: 0.18094 (0.20542)
2024-08-30,01:30:51 | INFO | Train Epoch: 41 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 525.799/s, 525.799/s/gpu LR: 6.480267e-09 Logit Scale: 100.000 Contrastive_loss: 0.28592 (0.21161) Loss: 0.28592 (0.21161)
2024-08-30,01:31:10 | INFO | Train Epoch: 41 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 540.313/s, 540.313/s/gpu LR: 6.470261e-09 Logit Scale: 100.000 Contrastive_loss: 0.15948 (0.20788) Loss: 0.15948 (0.20788)
2024-08-30,01:31:30 | INFO | Train Epoch: 41 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 489.462/s, 489.462/s/gpu LR: 6.460249e-09 Logit Scale: 100.000 Contrastive_loss: 0.18464 (0.20633) Loss: 0.18464 (0.20633)
2024-08-30,01:31:39 | INFO | Train Epoch: 41 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.192, 530.443/s, 530.443/s/gpu LR: 6.455341e-09 Logit Scale: 100.000 Contrastive_loss: 0.23552 (0.20816) Loss: 0.23552 (0.20816)
2024-08-30,01:31:49 | INFO | Train Epoch: 41 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 535.423/s, 535.423/s/gpu LR: 6.450230e-09 Logit Scale: 100.000 Contrastive_loss: 0.20871 (0.20819) Loss: 0.20871 (0.20819)
2024-08-30,01:32:08 | INFO | Train Epoch: 41 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 534.048/s, 534.048/s/gpu LR: 6.440205e-09 Logit Scale: 100.000 Contrastive_loss: 0.24439 (0.21020) Loss: 0.24439 (0.21020)
2024-08-30,01:32:27 | INFO | Train Epoch: 41 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 524.832/s, 524.832/s/gpu LR: 6.430174e-09 Logit Scale: 100.000 Contrastive_loss: 0.18336 (0.20879) Loss: 0.18336 (0.20879)
2024-08-30,01:32:47 | INFO | Train Epoch: 41 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 536.126/s, 536.126/s/gpu LR: 6.420136e-09 Logit Scale: 100.000 Contrastive_loss: 0.20422 (0.20856) Loss: 0.20422 (0.20856)
2024-08-30,01:33:06 | INFO | Train Epoch: 41 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 500.855/s, 500.855/s/gpu LR: 6.410092e-09 Logit Scale: 100.000 Contrastive_loss: 0.25483 (0.21076) Loss: 0.25483 (0.21076)
2024-08-30,01:33:25 | INFO | Train Epoch: 41 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 529.275/s, 529.275/s/gpu LR: 6.400042e-09 Logit Scale: 100.000 Contrastive_loss: 0.17196 (0.20900) Loss: 0.17196 (0.20900)
2024-08-30,01:33:45 | INFO | Train Epoch: 41 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 524.083/s, 524.083/s/gpu LR: 6.389986e-09 Logit Scale: 100.000 Contrastive_loss: 0.26032 (0.21123) Loss: 0.26032 (0.21123)
2024-08-30,01:34:04 | INFO | Train Epoch: 41 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 563.927/s, 563.927/s/gpu LR: 6.379924e-09 Logit Scale: 100.000 Contrastive_loss: 0.32512 (0.21598) Loss: 0.32512 (0.21598)
2024-08-30,01:34:23 | INFO | Train Epoch: 41 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 500.422/s, 500.422/s/gpu LR: 6.369855e-09 Logit Scale: 100.000 Contrastive_loss: 0.30955 (0.21972) Loss: 0.30955 (0.21972)
2024-08-30,01:34:43 | INFO | Train Epoch: 41 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 490.973/s, 490.973/s/gpu LR: 6.359781e-09 Logit Scale: 100.000 Contrastive_loss: 0.17560 (0.21802) Loss: 0.17560 (0.21802)
2024-08-30,01:35:02 | INFO | Train Epoch: 41 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 520.752/s, 520.752/s/gpu LR: 6.349701e-09 Logit Scale: 100.000 Contrastive_loss: 0.22358 (0.21823) Loss: 0.22358 (0.21823)
2024-08-30,01:35:21 | INFO | Train Epoch: 41 [260100/145000.0 (179%)] Data (t): 0.107 Batch (t): 0.192, 497.500/s, 497.500/s/gpu LR: 6.339614e-09 Logit Scale: 100.000 Contrastive_loss: 0.22800 (0.21858) Loss: 0.22800 (0.21858)
2024-08-30,01:35:40 | INFO | Train Epoch: 41 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 493.079/s, 493.079/s/gpu LR: 6.329522e-09 Logit Scale: 100.000 Contrastive_loss: 0.26365 (0.22013) Loss: 0.26365 (0.22013)
2024-08-30,01:36:00 | INFO | Train Epoch: 41 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 493.238/s, 493.238/s/gpu LR: 6.319424e-09 Logit Scale: 100.000 Contrastive_loss: 0.17694 (0.21869) Loss: 0.17694 (0.21869)
2024-08-30,01:36:19 | INFO | Train Epoch: 41 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.193, 494.588/s, 494.588/s/gpu LR: 6.309321e-09 Logit Scale: 100.000 Contrastive_loss: 0.14918 (0.21645) Loss: 0.14918 (0.21645)
2024-08-30,01:36:38 | INFO | Train Epoch: 41 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 496.064/s, 496.064/s/gpu LR: 6.299211e-09 Logit Scale: 100.000 Contrastive_loss: 0.27330 (0.21823) Loss: 0.27330 (0.21823)
2024-08-30,01:36:58 | INFO | Train Epoch: 41 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 523.277/s, 523.277/s/gpu LR: 6.289096e-09 Logit Scale: 100.000 Contrastive_loss: 0.24980 (0.21918) Loss: 0.24980 (0.21918)
2024-08-30,01:37:17 | INFO | Train Epoch: 41 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 496.685/s, 496.685/s/gpu LR: 6.278975e-09 Logit Scale: 100.000 Contrastive_loss: 0.29404 (0.22139) Loss: 0.29404 (0.22139)
2024-08-30,01:37:36 | INFO | Train Epoch: 41 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 499.676/s, 499.676/s/gpu LR: 6.268849e-09 Logit Scale: 100.000 Contrastive_loss: 0.19162 (0.22053) Loss: 0.19162 (0.22053)
2024-08-30,01:37:55 | INFO | Train Epoch: 41 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.192, 517.590/s, 517.590/s/gpu LR: 6.258717e-09 Logit Scale: 100.000 Contrastive_loss: 0.26538 (0.22178) Loss: 0.26538 (0.22178)
2024-08-30,01:38:15 | INFO | Train Epoch: 41 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.194, 496.112/s, 496.112/s/gpu LR: 6.248579e-09 Logit Scale: 100.000 Contrastive_loss: 0.31196 (0.22422) Loss: 0.31196 (0.22422)
2024-08-30,01:38:34 | INFO | Train Epoch: 41 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 497.048/s, 497.048/s/gpu LR: 6.238436e-09 Logit Scale: 100.000 Contrastive_loss: 0.16392 (0.22263) Loss: 0.16392 (0.22263)
2024-08-30,01:38:54 | INFO | Train Epoch: 41 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.194, 491.938/s, 491.938/s/gpu LR: 6.228288e-09 Logit Scale: 100.000 Contrastive_loss: 0.26817 (0.22380) Loss: 0.26817 (0.22380)
2024-08-30,01:39:13 | INFO | Train Epoch: 41 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 524.361/s, 524.361/s/gpu LR: 6.218134e-09 Logit Scale: 100.000 Contrastive_loss: 0.24485 (0.22432) Loss: 0.24485 (0.22432)
2024-08-30,01:39:32 | INFO | Train Epoch: 41 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 526.697/s, 526.697/s/gpu LR: 6.207975e-09 Logit Scale: 100.000 Contrastive_loss: 0.14087 (0.22229) Loss: 0.14087 (0.22229)
2024-08-30,01:39:51 | INFO | Train Epoch: 41 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 534.935/s, 534.935/s/gpu LR: 6.197810e-09 Logit Scale: 100.000 Contrastive_loss: 0.23356 (0.22256) Loss: 0.23356 (0.22256)
2024-08-30,01:40:11 | INFO | Train Epoch: 41 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.194, 496.585/s, 496.585/s/gpu LR: 6.187640e-09 Logit Scale: 100.000 Contrastive_loss: 0.094720 (0.21958) Loss: 0.094720 (0.21958)
2024-08-30,01:40:18 | INFO | Eval Epoch: 42 [200 / 1000]	Clip Loss: 0.482218	
2024-08-30,01:40:19 | INFO | Eval Epoch: 42 image_to_text_mean_rank: 2.4280	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6660	image_to_text_R@5: 0.9120	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.7500	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6700	text_to_image_R@5: 0.9050	text_to_image_R@10: 0.9670	clip_val_loss: 0.4397	epoch: 42.0000	num_samples: 1000.0000
2024-08-30,01:40:20 | INFO | Start epoch 42
2024-08-30,01:40:20 | INFO | Train Epoch: 42 [   100/145000.0 (0%)] Data (t): 0.024 Batch (t): 0.115, 872.488/s, 872.488/s/gpu LR: 6.455240e-09 Logit Scale: 100.000 Contrastive_loss: 0.17808 (0.17808) Loss: 0.17808 (0.17808)
2024-08-30,01:40:40 | INFO | Train Epoch: 42 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.195, 492.880/s, 492.880/s/gpu LR: 6.445218e-09 Logit Scale: 100.000 Contrastive_loss: 0.34858 (0.26333) Loss: 0.34858 (0.26333)
2024-08-30,01:40:59 | INFO | Train Epoch: 42 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 534.739/s, 534.739/s/gpu LR: 6.435190e-09 Logit Scale: 100.000 Contrastive_loss: 0.36512 (0.29726) Loss: 0.36512 (0.29726)
2024-08-30,01:41:19 | INFO | Train Epoch: 42 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 529.904/s, 529.904/s/gpu LR: 6.425156e-09 Logit Scale: 100.000 Contrastive_loss: 0.19283 (0.27115) Loss: 0.19283 (0.27115)
2024-08-30,01:41:38 | INFO | Train Epoch: 42 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 525.445/s, 525.445/s/gpu LR: 6.415115e-09 Logit Scale: 100.000 Contrastive_loss: 0.13923 (0.24477) Loss: 0.13923 (0.24477)
2024-08-30,01:41:57 | INFO | Train Epoch: 42 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 530.634/s, 530.634/s/gpu LR: 6.405068e-09 Logit Scale: 100.000 Contrastive_loss: 0.16937 (0.23220) Loss: 0.16937 (0.23220)
2024-08-30,01:42:17 | INFO | Train Epoch: 42 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.194, 499.584/s, 499.584/s/gpu LR: 6.395015e-09 Logit Scale: 100.000 Contrastive_loss: 0.12264 (0.21655) Loss: 0.12264 (0.21655)
2024-08-30,01:42:36 | INFO | Train Epoch: 42 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 517.994/s, 517.994/s/gpu LR: 6.384956e-09 Logit Scale: 100.000 Contrastive_loss: 0.23460 (0.21881) Loss: 0.23460 (0.21881)
2024-08-30,01:42:55 | INFO | Train Epoch: 42 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 493.822/s, 493.822/s/gpu LR: 6.374890e-09 Logit Scale: 100.000 Contrastive_loss: 0.13994 (0.21004) Loss: 0.13994 (0.21004)
2024-08-30,01:43:14 | INFO | Train Epoch: 42 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 503.665/s, 503.665/s/gpu LR: 6.364819e-09 Logit Scale: 100.000 Contrastive_loss: 0.22256 (0.21130) Loss: 0.22256 (0.21130)
2024-08-30,01:43:34 | INFO | Train Epoch: 42 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 489.604/s, 489.604/s/gpu LR: 6.354742e-09 Logit Scale: 100.000 Contrastive_loss: 0.15886 (0.20653) Loss: 0.15886 (0.20653)
2024-08-30,01:43:53 | INFO | Train Epoch: 42 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 497.397/s, 497.397/s/gpu LR: 6.344658e-09 Logit Scale: 100.000 Contrastive_loss: 0.17988 (0.20431) Loss: 0.17988 (0.20431)
2024-08-30,01:44:12 | INFO | Train Epoch: 42 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 507.830/s, 507.830/s/gpu LR: 6.334569e-09 Logit Scale: 100.000 Contrastive_loss: 0.28464 (0.21049) Loss: 0.28464 (0.21049)
2024-08-30,01:44:32 | INFO | Train Epoch: 42 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 497.940/s, 497.940/s/gpu LR: 6.324474e-09 Logit Scale: 100.000 Contrastive_loss: 0.15864 (0.20678) Loss: 0.15864 (0.20678)
2024-08-30,01:44:51 | INFO | Train Epoch: 42 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 509.720/s, 509.720/s/gpu LR: 6.314373e-09 Logit Scale: 100.000 Contrastive_loss: 0.18400 (0.20526) Loss: 0.18400 (0.20526)
2024-08-30,01:45:00 | INFO | Train Epoch: 42 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 532.398/s, 532.398/s/gpu LR: 6.309422e-09 Logit Scale: 100.000 Contrastive_loss: 0.23437 (0.20708) Loss: 0.23437 (0.20708)
2024-08-30,01:45:10 | INFO | Train Epoch: 42 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 521.399/s, 521.399/s/gpu LR: 6.304267e-09 Logit Scale: 100.000 Contrastive_loss: 0.20723 (0.20709) Loss: 0.20723 (0.20709)
2024-08-30,01:45:29 | INFO | Train Epoch: 42 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 494.591/s, 494.591/s/gpu LR: 6.294154e-09 Logit Scale: 100.000 Contrastive_loss: 0.24275 (0.20907) Loss: 0.24275 (0.20907)
2024-08-30,01:45:49 | INFO | Train Epoch: 42 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 502.077/s, 502.077/s/gpu LR: 6.284036e-09 Logit Scale: 100.000 Contrastive_loss: 0.18233 (0.20767) Loss: 0.18233 (0.20767)
2024-08-30,01:46:08 | INFO | Train Epoch: 42 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 495.909/s, 495.909/s/gpu LR: 6.273913e-09 Logit Scale: 100.000 Contrastive_loss: 0.20355 (0.20746) Loss: 0.20355 (0.20746)
2024-08-30,01:46:27 | INFO | Train Epoch: 42 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 491.752/s, 491.752/s/gpu LR: 6.263783e-09 Logit Scale: 100.000 Contrastive_loss: 0.25377 (0.20966) Loss: 0.25377 (0.20966)
2024-08-30,01:46:47 | INFO | Train Epoch: 42 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 520.870/s, 520.870/s/gpu LR: 6.253649e-09 Logit Scale: 100.000 Contrastive_loss: 0.17111 (0.20791) Loss: 0.17111 (0.20791)
2024-08-30,01:47:06 | INFO | Train Epoch: 42 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 524.699/s, 524.699/s/gpu LR: 6.243508e-09 Logit Scale: 100.000 Contrastive_loss: 0.25924 (0.21014) Loss: 0.25924 (0.21014)
2024-08-30,01:47:25 | INFO | Train Epoch: 42 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 501.691/s, 501.691/s/gpu LR: 6.233363e-09 Logit Scale: 100.000 Contrastive_loss: 0.32325 (0.21486) Loss: 0.32325 (0.21486)
2024-08-30,01:47:44 | INFO | Train Epoch: 42 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 528.388/s, 528.388/s/gpu LR: 6.223211e-09 Logit Scale: 100.000 Contrastive_loss: 0.30862 (0.21861) Loss: 0.30862 (0.21861)
2024-08-30,01:48:04 | INFO | Train Epoch: 42 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 533.311/s, 533.311/s/gpu LR: 6.213055e-09 Logit Scale: 100.000 Contrastive_loss: 0.17438 (0.21691) Loss: 0.17438 (0.21691)
2024-08-30,01:48:23 | INFO | Train Epoch: 42 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 528.457/s, 528.457/s/gpu LR: 6.202893e-09 Logit Scale: 100.000 Contrastive_loss: 0.22283 (0.21713) Loss: 0.22283 (0.21713)
2024-08-30,01:48:42 | INFO | Train Epoch: 42 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 531.784/s, 531.784/s/gpu LR: 6.192726e-09 Logit Scale: 100.000 Contrastive_loss: 0.22700 (0.21748) Loss: 0.22700 (0.21748)
2024-08-30,01:49:02 | INFO | Train Epoch: 42 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.194, 524.308/s, 524.308/s/gpu LR: 6.182554e-09 Logit Scale: 100.000 Contrastive_loss: 0.26284 (0.21904) Loss: 0.26284 (0.21904)
2024-08-30,01:49:21 | INFO | Train Epoch: 42 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 498.011/s, 498.011/s/gpu LR: 6.172376e-09 Logit Scale: 100.000 Contrastive_loss: 0.17606 (0.21761) Loss: 0.17606 (0.21761)
2024-08-30,01:49:40 | INFO | Train Epoch: 42 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 494.203/s, 494.203/s/gpu LR: 6.162193e-09 Logit Scale: 100.000 Contrastive_loss: 0.14876 (0.21539) Loss: 0.14876 (0.21539)
2024-08-30,01:49:59 | INFO | Train Epoch: 42 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 503.317/s, 503.317/s/gpu LR: 6.152006e-09 Logit Scale: 100.000 Contrastive_loss: 0.27217 (0.21716) Loss: 0.27217 (0.21716)
2024-08-30,01:50:19 | INFO | Train Epoch: 42 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 531.730/s, 531.730/s/gpu LR: 6.141813e-09 Logit Scale: 100.000 Contrastive_loss: 0.24931 (0.21814) Loss: 0.24931 (0.21814)
2024-08-30,01:50:38 | INFO | Train Epoch: 42 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 524.886/s, 524.886/s/gpu LR: 6.131615e-09 Logit Scale: 100.000 Contrastive_loss: 0.29271 (0.22033) Loss: 0.29271 (0.22033)
2024-08-30,01:50:57 | INFO | Train Epoch: 42 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 531.050/s, 531.050/s/gpu LR: 6.121412e-09 Logit Scale: 100.000 Contrastive_loss: 0.19094 (0.21949) Loss: 0.19094 (0.21949)
2024-08-30,01:51:17 | INFO | Train Epoch: 42 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 497.916/s, 497.916/s/gpu LR: 6.111204e-09 Logit Scale: 100.000 Contrastive_loss: 0.26346 (0.22071) Loss: 0.26346 (0.22071)
2024-08-30,01:51:36 | INFO | Train Epoch: 42 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 512.213/s, 512.213/s/gpu LR: 6.100992e-09 Logit Scale: 100.000 Contrastive_loss: 0.31052 (0.22314) Loss: 0.31052 (0.22314)
2024-08-30,01:51:55 | INFO | Train Epoch: 42 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 492.804/s, 492.804/s/gpu LR: 6.090774e-09 Logit Scale: 100.000 Contrastive_loss: 0.16286 (0.22155) Loss: 0.16286 (0.22155)
2024-08-30,01:52:14 | INFO | Train Epoch: 42 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 493.587/s, 493.587/s/gpu LR: 6.080552e-09 Logit Scale: 100.000 Contrastive_loss: 0.26717 (0.22272) Loss: 0.26717 (0.22272)
2024-08-30,01:52:34 | INFO | Train Epoch: 42 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.192, 497.017/s, 497.017/s/gpu LR: 6.070325e-09 Logit Scale: 100.000 Contrastive_loss: 0.24378 (0.22325) Loss: 0.24378 (0.22325)
2024-08-30,01:52:53 | INFO | Train Epoch: 42 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 504.217/s, 504.217/s/gpu LR: 6.060094e-09 Logit Scale: 100.000 Contrastive_loss: 0.13997 (0.22122) Loss: 0.13997 (0.22122)
2024-08-30,01:53:12 | INFO | Train Epoch: 42 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.194, 526.004/s, 526.004/s/gpu LR: 6.049857e-09 Logit Scale: 100.000 Contrastive_loss: 0.23238 (0.22148) Loss: 0.23238 (0.22148)
2024-08-30,01:53:32 | INFO | Train Epoch: 42 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.194, 501.491/s, 501.491/s/gpu LR: 6.039616e-09 Logit Scale: 100.000 Contrastive_loss: 0.094122 (0.21852) Loss: 0.094122 (0.21852)
2024-08-30,01:53:39 | INFO | Eval Epoch: 43 [200 / 1000]	Clip Loss: 0.482133	
2024-08-30,01:53:40 | INFO | Eval Epoch: 43 image_to_text_mean_rank: 2.4320	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6640	image_to_text_R@5: 0.9120	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.7570	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9050	text_to_image_R@10: 0.9670	clip_val_loss: 0.4396	epoch: 43.0000	num_samples: 1000.0000
2024-08-30,01:53:41 | INFO | Start epoch 43
2024-08-30,01:53:41 | INFO | Train Epoch: 43 [   100/145000.0 (0%)] Data (t): 0.035 Batch (t): 0.115, 865.894/s, 865.894/s/gpu LR: 6.309321e-09 Logit Scale: 100.000 Contrastive_loss: 0.17704 (0.17704) Loss: 0.17704 (0.17704)
2024-08-30,01:54:01 | INFO | Train Epoch: 43 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.195, 497.354/s, 497.354/s/gpu LR: 6.299211e-09 Logit Scale: 100.000 Contrastive_loss: 0.34746 (0.26225) Loss: 0.34746 (0.26225)
2024-08-30,01:54:20 | INFO | Train Epoch: 43 [ 20100/145000.0 (14%)] Data (t): 0.110 Batch (t): 0.194, 524.184/s, 524.184/s/gpu LR: 6.289096e-09 Logit Scale: 100.000 Contrastive_loss: 0.36342 (0.29597) Loss: 0.36342 (0.29597)
2024-08-30,01:54:40 | INFO | Train Epoch: 43 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.194, 502.563/s, 502.563/s/gpu LR: 6.278975e-09 Logit Scale: 100.000 Contrastive_loss: 0.19173 (0.26991) Loss: 0.19173 (0.26991)
2024-08-30,01:54:59 | INFO | Train Epoch: 43 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 495.901/s, 495.901/s/gpu LR: 6.268849e-09 Logit Scale: 100.000 Contrastive_loss: 0.13859 (0.24365) Loss: 0.13859 (0.24365)
2024-08-30,01:55:18 | INFO | Train Epoch: 43 [ 50100/145000.0 (35%)] Data (t): 0.110 Batch (t): 0.193, 510.161/s, 510.161/s/gpu LR: 6.258717e-09 Logit Scale: 100.000 Contrastive_loss: 0.16829 (0.23109) Loss: 0.16829 (0.23109)
2024-08-30,01:55:38 | INFO | Train Epoch: 43 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 523.061/s, 523.061/s/gpu LR: 6.248579e-09 Logit Scale: 100.000 Contrastive_loss: 0.12207 (0.21551) Loss: 0.12207 (0.21551)
2024-08-30,01:55:57 | INFO | Train Epoch: 43 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 529.911/s, 529.911/s/gpu LR: 6.238436e-09 Logit Scale: 100.000 Contrastive_loss: 0.23301 (0.21770) Loss: 0.23301 (0.21770)
2024-08-30,01:56:16 | INFO | Train Epoch: 43 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.194, 494.584/s, 494.584/s/gpu LR: 6.228288e-09 Logit Scale: 100.000 Contrastive_loss: 0.13888 (0.20894) Loss: 0.13888 (0.20894)
2024-08-30,01:56:36 | INFO | Train Epoch: 43 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 529.876/s, 529.876/s/gpu LR: 6.218134e-09 Logit Scale: 100.000 Contrastive_loss: 0.22162 (0.21021) Loss: 0.22162 (0.21021)
2024-08-30,01:56:55 | INFO | Train Epoch: 43 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.194, 534.016/s, 534.016/s/gpu LR: 6.207975e-09 Logit Scale: 100.000 Contrastive_loss: 0.15807 (0.20547) Loss: 0.15807 (0.20547)
2024-08-30,01:57:14 | INFO | Train Epoch: 43 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 498.655/s, 498.655/s/gpu LR: 6.197810e-09 Logit Scale: 100.000 Contrastive_loss: 0.17879 (0.20325) Loss: 0.17879 (0.20325)
2024-08-30,01:57:34 | INFO | Train Epoch: 43 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 494.533/s, 494.533/s/gpu LR: 6.187640e-09 Logit Scale: 100.000 Contrastive_loss: 0.28341 (0.20941) Loss: 0.28341 (0.20941)
2024-08-30,01:57:53 | INFO | Train Epoch: 43 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 499.257/s, 499.257/s/gpu LR: 6.177465e-09 Logit Scale: 100.000 Contrastive_loss: 0.15800 (0.20574) Loss: 0.15800 (0.20574)
2024-08-30,01:58:12 | INFO | Train Epoch: 43 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 533.995/s, 533.995/s/gpu LR: 6.167285e-09 Logit Scale: 100.000 Contrastive_loss: 0.18327 (0.20424) Loss: 0.18327 (0.20424)
2024-08-30,01:58:22 | INFO | Train Epoch: 43 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 533.491/s, 533.491/s/gpu LR: 6.162295e-09 Logit Scale: 100.000 Contrastive_loss: 0.23328 (0.20606) Loss: 0.23328 (0.20606)
2024-08-30,01:58:32 | INFO | Train Epoch: 43 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 507.121/s, 507.121/s/gpu LR: 6.157100e-09 Logit Scale: 100.000 Contrastive_loss: 0.20622 (0.20607) Loss: 0.20622 (0.20607)
2024-08-30,01:58:51 | INFO | Train Epoch: 43 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.192, 522.444/s, 522.444/s/gpu LR: 6.146910e-09 Logit Scale: 100.000 Contrastive_loss: 0.24123 (0.20802) Loss: 0.24123 (0.20802)
2024-08-30,01:59:10 | INFO | Train Epoch: 43 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 535.514/s, 535.514/s/gpu LR: 6.136715e-09 Logit Scale: 100.000 Contrastive_loss: 0.18121 (0.20661) Loss: 0.18121 (0.20661)
2024-08-30,01:59:29 | INFO | Train Epoch: 43 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 503.507/s, 503.507/s/gpu LR: 6.126514e-09 Logit Scale: 100.000 Contrastive_loss: 0.20291 (0.20642) Loss: 0.20291 (0.20642)
2024-08-30,01:59:49 | INFO | Train Epoch: 43 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 531.292/s, 531.292/s/gpu LR: 6.116309e-09 Logit Scale: 100.000 Contrastive_loss: 0.25258 (0.20862) Loss: 0.25258 (0.20862)
2024-08-30,02:00:08 | INFO | Train Epoch: 43 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 501.225/s, 501.225/s/gpu LR: 6.106099e-09 Logit Scale: 100.000 Contrastive_loss: 0.17045 (0.20689) Loss: 0.17045 (0.20689)
2024-08-30,02:00:27 | INFO | Train Epoch: 43 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 527.217/s, 527.217/s/gpu LR: 6.095884e-09 Logit Scale: 100.000 Contrastive_loss: 0.25827 (0.20912) Loss: 0.25827 (0.20912)
2024-08-30,02:00:47 | INFO | Train Epoch: 43 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 491.758/s, 491.758/s/gpu LR: 6.085664e-09 Logit Scale: 100.000 Contrastive_loss: 0.32152 (0.21381) Loss: 0.32152 (0.21381)
2024-08-30,02:01:06 | INFO | Train Epoch: 43 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 520.722/s, 520.722/s/gpu LR: 6.075439e-09 Logit Scale: 100.000 Contrastive_loss: 0.30764 (0.21756) Loss: 0.30764 (0.21756)
2024-08-30,02:01:25 | INFO | Train Epoch: 43 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 490.023/s, 490.023/s/gpu LR: 6.065210e-09 Logit Scale: 100.000 Contrastive_loss: 0.17322 (0.21585) Loss: 0.17322 (0.21585)
2024-08-30,02:01:44 | INFO | Train Epoch: 43 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 511.031/s, 511.031/s/gpu LR: 6.054976e-09 Logit Scale: 100.000 Contrastive_loss: 0.22210 (0.21608) Loss: 0.22210 (0.21608)
2024-08-30,02:02:04 | INFO | Train Epoch: 43 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 534.400/s, 534.400/s/gpu LR: 6.044738e-09 Logit Scale: 100.000 Contrastive_loss: 0.22575 (0.21643) Loss: 0.22575 (0.21643)
2024-08-30,02:02:23 | INFO | Train Epoch: 43 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 508.070/s, 508.070/s/gpu LR: 6.034494e-09 Logit Scale: 100.000 Contrastive_loss: 0.26206 (0.21800) Loss: 0.26206 (0.21800)
2024-08-30,02:02:42 | INFO | Train Epoch: 43 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 527.494/s, 527.494/s/gpu LR: 6.024247e-09 Logit Scale: 100.000 Contrastive_loss: 0.17536 (0.21658) Loss: 0.17536 (0.21658)
2024-08-30,02:03:01 | INFO | Train Epoch: 43 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.192, 530.845/s, 530.845/s/gpu LR: 6.013994e-09 Logit Scale: 100.000 Contrastive_loss: 0.14841 (0.21438) Loss: 0.14841 (0.21438)
2024-08-30,02:03:21 | INFO | Train Epoch: 43 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 527.515/s, 527.515/s/gpu LR: 6.003738e-09 Logit Scale: 100.000 Contrastive_loss: 0.27100 (0.21615) Loss: 0.27100 (0.21615)
2024-08-30,02:03:40 | INFO | Train Epoch: 43 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 526.493/s, 526.493/s/gpu LR: 5.993477e-09 Logit Scale: 100.000 Contrastive_loss: 0.24867 (0.21714) Loss: 0.24867 (0.21714)
2024-08-30,02:03:59 | INFO | Train Epoch: 43 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 499.732/s, 499.732/s/gpu LR: 5.983211e-09 Logit Scale: 100.000 Contrastive_loss: 0.29137 (0.21932) Loss: 0.29137 (0.21932)
2024-08-30,02:04:18 | INFO | Train Epoch: 43 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 533.506/s, 533.506/s/gpu LR: 5.972942e-09 Logit Scale: 100.000 Contrastive_loss: 0.19032 (0.21849) Loss: 0.19032 (0.21849)
2024-08-30,02:04:38 | INFO | Train Epoch: 43 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.193, 497.397/s, 497.397/s/gpu LR: 5.962668e-09 Logit Scale: 100.000 Contrastive_loss: 0.26197 (0.21970) Loss: 0.26197 (0.21970)
2024-08-30,02:04:57 | INFO | Train Epoch: 43 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 527.612/s, 527.612/s/gpu LR: 5.952389e-09 Logit Scale: 100.000 Contrastive_loss: 0.30930 (0.22212) Loss: 0.30930 (0.22212)
2024-08-30,02:05:16 | INFO | Train Epoch: 43 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 545.606/s, 545.606/s/gpu LR: 5.942107e-09 Logit Scale: 100.000 Contrastive_loss: 0.16215 (0.22054) Loss: 0.16215 (0.22054)
2024-08-30,02:05:36 | INFO | Train Epoch: 43 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.194, 493.124/s, 493.124/s/gpu LR: 5.931820e-09 Logit Scale: 100.000 Contrastive_loss: 0.26625 (0.22171) Loss: 0.26625 (0.22171)
2024-08-30,02:05:55 | INFO | Train Epoch: 43 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 524.586/s, 524.586/s/gpu LR: 5.921530e-09 Logit Scale: 100.000 Contrastive_loss: 0.24281 (0.22224) Loss: 0.24281 (0.22224)
2024-08-30,02:06:14 | INFO | Train Epoch: 43 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 517.799/s, 517.799/s/gpu LR: 5.911235e-09 Logit Scale: 100.000 Contrastive_loss: 0.13891 (0.22021) Loss: 0.13891 (0.22021)
2024-08-30,02:06:34 | INFO | Train Epoch: 43 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 529.694/s, 529.694/s/gpu LR: 5.900936e-09 Logit Scale: 100.000 Contrastive_loss: 0.23132 (0.22047) Loss: 0.23132 (0.22047)
2024-08-30,02:06:53 | INFO | Train Epoch: 43 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 535.809/s, 535.809/s/gpu LR: 5.890634e-09 Logit Scale: 100.000 Contrastive_loss: 0.093386 (0.21752) Loss: 0.093386 (0.21752)
2024-08-30,02:07:01 | INFO | Eval Epoch: 44 [200 / 1000]	Clip Loss: 0.482070	
2024-08-30,02:07:01 | INFO | Eval Epoch: 44 image_to_text_mean_rank: 2.4330	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6640	image_to_text_R@5: 0.9110	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.7690	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9050	text_to_image_R@10: 0.9670	clip_val_loss: 0.4395	epoch: 44.0000	num_samples: 1000.0000
2024-08-30,02:07:02 | INFO | Start epoch 44
2024-08-30,02:07:03 | INFO | Train Epoch: 44 [   100/145000.0 (0%)] Data (t): 0.023 Batch (t): 0.108, 926.482/s, 926.482/s/gpu LR: 6.162193e-09 Logit Scale: 100.000 Contrastive_loss: 0.17611 (0.17611) Loss: 0.17611 (0.17611)
2024-08-30,02:07:22 | INFO | Train Epoch: 44 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 521.545/s, 521.545/s/gpu LR: 6.152006e-09 Logit Scale: 100.000 Contrastive_loss: 0.34622 (0.26116) Loss: 0.34622 (0.26116)
2024-08-30,02:07:41 | INFO | Train Epoch: 44 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 529.258/s, 529.258/s/gpu LR: 6.141813e-09 Logit Scale: 100.000 Contrastive_loss: 0.36200 (0.29478) Loss: 0.36200 (0.29478)
2024-08-30,02:08:01 | INFO | Train Epoch: 44 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 499.085/s, 499.085/s/gpu LR: 6.131615e-09 Logit Scale: 100.000 Contrastive_loss: 0.19053 (0.26871) Loss: 0.19053 (0.26871)
2024-08-30,02:08:20 | INFO | Train Epoch: 44 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 507.330/s, 507.330/s/gpu LR: 6.121412e-09 Logit Scale: 100.000 Contrastive_loss: 0.13797 (0.24256) Loss: 0.13797 (0.24256)
2024-08-30,02:08:39 | INFO | Train Epoch: 44 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 499.120/s, 499.120/s/gpu LR: 6.111204e-09 Logit Scale: 100.000 Contrastive_loss: 0.16733 (0.23003) Loss: 0.16733 (0.23003)
2024-08-30,02:08:58 | INFO | Train Epoch: 44 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 522.775/s, 522.775/s/gpu LR: 6.100992e-09 Logit Scale: 100.000 Contrastive_loss: 0.12140 (0.21451) Loss: 0.12140 (0.21451)
2024-08-30,02:09:18 | INFO | Train Epoch: 44 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 492.814/s, 492.814/s/gpu LR: 6.090774e-09 Logit Scale: 100.000 Contrastive_loss: 0.23138 (0.21662) Loss: 0.23138 (0.21662)
2024-08-30,02:09:37 | INFO | Train Epoch: 44 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 531.001/s, 531.001/s/gpu LR: 6.080552e-09 Logit Scale: 100.000 Contrastive_loss: 0.13791 (0.20787) Loss: 0.13791 (0.20787)
2024-08-30,02:09:56 | INFO | Train Epoch: 44 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 510.305/s, 510.305/s/gpu LR: 6.070325e-09 Logit Scale: 100.000 Contrastive_loss: 0.22043 (0.20913) Loss: 0.22043 (0.20913)
2024-08-30,02:10:15 | INFO | Train Epoch: 44 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 532.966/s, 532.966/s/gpu LR: 6.060094e-09 Logit Scale: 100.000 Contrastive_loss: 0.15744 (0.20443) Loss: 0.15744 (0.20443)
2024-08-30,02:10:35 | INFO | Train Epoch: 44 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.193, 526.709/s, 526.709/s/gpu LR: 6.049857e-09 Logit Scale: 100.000 Contrastive_loss: 0.17781 (0.20221) Loss: 0.17781 (0.20221)
2024-08-30,02:10:54 | INFO | Train Epoch: 44 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.192, 496.791/s, 496.791/s/gpu LR: 6.039616e-09 Logit Scale: 100.000 Contrastive_loss: 0.28218 (0.20836) Loss: 0.28218 (0.20836)
2024-08-30,02:11:13 | INFO | Train Epoch: 44 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 492.832/s, 492.832/s/gpu LR: 6.029371e-09 Logit Scale: 100.000 Contrastive_loss: 0.15737 (0.20472) Loss: 0.15737 (0.20472)
2024-08-30,02:11:32 | INFO | Train Epoch: 44 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.192, 500.657/s, 500.657/s/gpu LR: 6.019121e-09 Logit Scale: 100.000 Contrastive_loss: 0.18272 (0.20325) Loss: 0.18272 (0.20325)
2024-08-30,02:11:42 | INFO | Train Epoch: 44 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 537.734/s, 537.734/s/gpu LR: 6.014097e-09 Logit Scale: 100.000 Contrastive_loss: 0.23210 (0.20506) Loss: 0.23210 (0.20506)
2024-08-30,02:11:52 | INFO | Train Epoch: 44 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 494.439/s, 494.439/s/gpu LR: 6.008867e-09 Logit Scale: 100.000 Contrastive_loss: 0.20507 (0.20506) Loss: 0.20507 (0.20506)
2024-08-30,02:12:11 | INFO | Train Epoch: 44 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 529.820/s, 529.820/s/gpu LR: 5.998608e-09 Logit Scale: 100.000 Contrastive_loss: 0.23963 (0.20698) Loss: 0.23963 (0.20698)
2024-08-30,02:12:30 | INFO | Train Epoch: 44 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 529.526/s, 529.526/s/gpu LR: 5.988345e-09 Logit Scale: 100.000 Contrastive_loss: 0.18024 (0.20557) Loss: 0.18024 (0.20557)
2024-08-30,02:12:50 | INFO | Train Epoch: 44 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 502.927/s, 502.927/s/gpu LR: 5.978077e-09 Logit Scale: 100.000 Contrastive_loss: 0.20220 (0.20540) Loss: 0.20220 (0.20540)
2024-08-30,02:13:09 | INFO | Train Epoch: 44 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 527.441/s, 527.441/s/gpu LR: 5.967805e-09 Logit Scale: 100.000 Contrastive_loss: 0.25151 (0.20760) Loss: 0.25151 (0.20760)
2024-08-30,02:13:28 | INFO | Train Epoch: 44 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 494.106/s, 494.106/s/gpu LR: 5.957529e-09 Logit Scale: 100.000 Contrastive_loss: 0.16988 (0.20588) Loss: 0.16988 (0.20588)
2024-08-30,02:13:47 | INFO | Train Epoch: 44 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 495.382/s, 495.382/s/gpu LR: 5.947249e-09 Logit Scale: 100.000 Contrastive_loss: 0.25721 (0.20811) Loss: 0.25721 (0.20811)
2024-08-30,02:14:07 | INFO | Train Epoch: 44 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.192, 520.633/s, 520.633/s/gpu LR: 5.936964e-09 Logit Scale: 100.000 Contrastive_loss: 0.31982 (0.21277) Loss: 0.31982 (0.21277)
2024-08-30,02:14:26 | INFO | Train Epoch: 44 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 517.519/s, 517.519/s/gpu LR: 5.926676e-09 Logit Scale: 100.000 Contrastive_loss: 0.30665 (0.21652) Loss: 0.30665 (0.21652)
2024-08-30,02:14:45 | INFO | Train Epoch: 44 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 500.623/s, 500.623/s/gpu LR: 5.916383e-09 Logit Scale: 100.000 Contrastive_loss: 0.17222 (0.21482) Loss: 0.17222 (0.21482)
2024-08-30,02:15:05 | INFO | Train Epoch: 44 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 500.060/s, 500.060/s/gpu LR: 5.906086e-09 Logit Scale: 100.000 Contrastive_loss: 0.22121 (0.21506) Loss: 0.22121 (0.21506)
2024-08-30,02:15:24 | INFO | Train Epoch: 44 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 502.644/s, 502.644/s/gpu LR: 5.895786e-09 Logit Scale: 100.000 Contrastive_loss: 0.22486 (0.21541) Loss: 0.22486 (0.21541)
2024-08-30,02:15:43 | INFO | Train Epoch: 44 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 521.270/s, 521.270/s/gpu LR: 5.885481e-09 Logit Scale: 100.000 Contrastive_loss: 0.26132 (0.21699) Loss: 0.26132 (0.21699)
2024-08-30,02:16:02 | INFO | Train Epoch: 44 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 492.963/s, 492.963/s/gpu LR: 5.875173e-09 Logit Scale: 100.000 Contrastive_loss: 0.17452 (0.21557) Loss: 0.17452 (0.21557)
2024-08-30,02:16:22 | INFO | Train Epoch: 44 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 532.491/s, 532.491/s/gpu LR: 5.864860e-09 Logit Scale: 100.000 Contrastive_loss: 0.14797 (0.21339) Loss: 0.14797 (0.21339)
2024-08-30,02:16:41 | INFO | Train Epoch: 44 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.194, 494.284/s, 494.284/s/gpu LR: 5.854544e-09 Logit Scale: 100.000 Contrastive_loss: 0.26994 (0.21516) Loss: 0.26994 (0.21516)
2024-08-30,02:17:00 | INFO | Train Epoch: 44 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 498.083/s, 498.083/s/gpu LR: 5.844224e-09 Logit Scale: 100.000 Contrastive_loss: 0.24802 (0.21616) Loss: 0.24802 (0.21616)
2024-08-30,02:17:20 | INFO | Train Epoch: 44 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 537.447/s, 537.447/s/gpu LR: 5.833901e-09 Logit Scale: 100.000 Contrastive_loss: 0.29012 (0.21833) Loss: 0.29012 (0.21833)
2024-08-30,02:17:39 | INFO | Train Epoch: 44 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 532.997/s, 532.997/s/gpu LR: 5.823574e-09 Logit Scale: 100.000 Contrastive_loss: 0.18974 (0.21751) Loss: 0.18974 (0.21751)
2024-08-30,02:17:58 | INFO | Train Epoch: 44 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.194, 518.280/s, 518.280/s/gpu LR: 5.813243e-09 Logit Scale: 100.000 Contrastive_loss: 0.26032 (0.21870) Loss: 0.26032 (0.21870)
2024-08-30,02:18:18 | INFO | Train Epoch: 44 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 526.396/s, 526.396/s/gpu LR: 5.802908e-09 Logit Scale: 100.000 Contrastive_loss: 0.30796 (0.22112) Loss: 0.30796 (0.22112)
2024-08-30,02:18:37 | INFO | Train Epoch: 44 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 531.652/s, 531.652/s/gpu LR: 5.792571e-09 Logit Scale: 100.000 Contrastive_loss: 0.16129 (0.21954) Loss: 0.16129 (0.21954)
2024-08-30,02:18:56 | INFO | Train Epoch: 44 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 506.628/s, 506.628/s/gpu LR: 5.782229e-09 Logit Scale: 100.000 Contrastive_loss: 0.26533 (0.22072) Loss: 0.26533 (0.22072)
2024-08-30,02:19:15 | INFO | Train Epoch: 44 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 536.612/s, 536.612/s/gpu LR: 5.771884e-09 Logit Scale: 100.000 Contrastive_loss: 0.24192 (0.22125) Loss: 0.24192 (0.22125)
2024-08-30,02:19:35 | INFO | Train Epoch: 44 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.193, 533.058/s, 533.058/s/gpu LR: 5.761536e-09 Logit Scale: 100.000 Contrastive_loss: 0.13799 (0.21922) Loss: 0.13799 (0.21922)
2024-08-30,02:19:54 | INFO | Train Epoch: 44 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 535.081/s, 535.081/s/gpu LR: 5.751185e-09 Logit Scale: 100.000 Contrastive_loss: 0.23035 (0.21948) Loss: 0.23035 (0.21948)
2024-08-30,02:20:13 | INFO | Train Epoch: 44 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.192, 535.442/s, 535.442/s/gpu LR: 5.740830e-09 Logit Scale: 100.000 Contrastive_loss: 0.092850 (0.21654) Loss: 0.092850 (0.21654)
2024-08-30,02:20:21 | INFO | Eval Epoch: 45 [200 / 1000]	Clip Loss: 0.482064	
2024-08-30,02:20:22 | INFO | Eval Epoch: 45 image_to_text_mean_rank: 2.4300	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6640	image_to_text_R@5: 0.9110	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.7840	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9060	text_to_image_R@10: 0.9670	clip_val_loss: 0.4395	epoch: 45.0000	num_samples: 1000.0000
2024-08-30,02:20:23 | INFO | Start epoch 45
2024-08-30,02:20:23 | INFO | Train Epoch: 45 [   100/145000.0 (0%)] Data (t): 0.038 Batch (t): 0.120, 835.822/s, 835.822/s/gpu LR: 6.013994e-09 Logit Scale: 100.000 Contrastive_loss: 0.17527 (0.17527) Loss: 0.17527 (0.17527)
2024-08-30,02:20:42 | INFO | Train Epoch: 45 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 535.015/s, 535.015/s/gpu LR: 6.003738e-09 Logit Scale: 100.000 Contrastive_loss: 0.34509 (0.26018) Loss: 0.34509 (0.26018)
2024-08-30,02:21:02 | INFO | Train Epoch: 45 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.194, 505.816/s, 505.816/s/gpu LR: 5.993477e-09 Logit Scale: 100.000 Contrastive_loss: 0.36019 (0.29352) Loss: 0.36019 (0.29352)
2024-08-30,02:21:21 | INFO | Train Epoch: 45 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 499.449/s, 499.449/s/gpu LR: 5.983211e-09 Logit Scale: 100.000 Contrastive_loss: 0.18941 (0.26749) Loss: 0.18941 (0.26749)
2024-08-30,02:21:41 | INFO | Train Epoch: 45 [ 40100/145000.0 (28%)] Data (t): 0.111 Batch (t): 0.196, 528.008/s, 528.008/s/gpu LR: 5.972942e-09 Logit Scale: 100.000 Contrastive_loss: 0.13735 (0.24146) Loss: 0.13735 (0.24146)
2024-08-30,02:22:00 | INFO | Train Epoch: 45 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.194, 526.856/s, 526.856/s/gpu LR: 5.962668e-09 Logit Scale: 100.000 Contrastive_loss: 0.16638 (0.22895) Loss: 0.16638 (0.22895)
2024-08-30,02:22:19 | INFO | Train Epoch: 45 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 535.036/s, 535.036/s/gpu LR: 5.952389e-09 Logit Scale: 100.000 Contrastive_loss: 0.12075 (0.21349) Loss: 0.12075 (0.21349)
2024-08-30,02:22:39 | INFO | Train Epoch: 45 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 491.421/s, 491.421/s/gpu LR: 5.942107e-09 Logit Scale: 100.000 Contrastive_loss: 0.22983 (0.21553) Loss: 0.22983 (0.21553)
2024-08-30,02:22:58 | INFO | Train Epoch: 45 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 532.082/s, 532.082/s/gpu LR: 5.931820e-09 Logit Scale: 100.000 Contrastive_loss: 0.13704 (0.20681) Loss: 0.13704 (0.20681)
2024-08-30,02:23:17 | INFO | Train Epoch: 45 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 508.916/s, 508.916/s/gpu LR: 5.921530e-09 Logit Scale: 100.000 Contrastive_loss: 0.21944 (0.20807) Loss: 0.21944 (0.20807)
2024-08-30,02:23:37 | INFO | Train Epoch: 45 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 523.266/s, 523.266/s/gpu LR: 5.911235e-09 Logit Scale: 100.000 Contrastive_loss: 0.15666 (0.20340) Loss: 0.15666 (0.20340)
2024-08-30,02:23:56 | INFO | Train Epoch: 45 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 497.646/s, 497.646/s/gpu LR: 5.900936e-09 Logit Scale: 100.000 Contrastive_loss: 0.17689 (0.20119) Loss: 0.17689 (0.20119)
2024-08-30,02:24:15 | INFO | Train Epoch: 45 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 530.414/s, 530.414/s/gpu LR: 5.890634e-09 Logit Scale: 100.000 Contrastive_loss: 0.28109 (0.20734) Loss: 0.28109 (0.20734)
2024-08-30,02:24:35 | INFO | Train Epoch: 45 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 516.985/s, 516.985/s/gpu LR: 5.880327e-09 Logit Scale: 100.000 Contrastive_loss: 0.15665 (0.20372) Loss: 0.15665 (0.20372)
2024-08-30,02:24:54 | INFO | Train Epoch: 45 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.194, 499.141/s, 499.141/s/gpu LR: 5.870017e-09 Logit Scale: 100.000 Contrastive_loss: 0.18209 (0.20228) Loss: 0.18209 (0.20228)
2024-08-30,02:25:03 | INFO | Train Epoch: 45 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 534.671/s, 534.671/s/gpu LR: 5.864963e-09 Logit Scale: 100.000 Contrastive_loss: 0.23094 (0.20407) Loss: 0.23094 (0.20407)
2024-08-30,02:25:13 | INFO | Train Epoch: 45 [150100/145000.0 (104%)] Data (t): 0.110 Batch (t): 0.194, 535.331/s, 535.331/s/gpu LR: 5.859703e-09 Logit Scale: 100.000 Contrastive_loss: 0.20408 (0.20407) Loss: 0.20408 (0.20407)
2024-08-30,02:25:33 | INFO | Train Epoch: 45 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 508.848/s, 508.848/s/gpu LR: 5.849385e-09 Logit Scale: 100.000 Contrastive_loss: 0.23819 (0.20596) Loss: 0.23819 (0.20596)
2024-08-30,02:25:52 | INFO | Train Epoch: 45 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 498.860/s, 498.860/s/gpu LR: 5.839063e-09 Logit Scale: 100.000 Contrastive_loss: 0.17937 (0.20456) Loss: 0.17937 (0.20456)
2024-08-30,02:26:11 | INFO | Train Epoch: 45 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 504.500/s, 504.500/s/gpu LR: 5.828738e-09 Logit Scale: 100.000 Contrastive_loss: 0.20153 (0.20441) Loss: 0.20153 (0.20441)
2024-08-30,02:26:31 | INFO | Train Epoch: 45 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 499.305/s, 499.305/s/gpu LR: 5.818409e-09 Logit Scale: 100.000 Contrastive_loss: 0.25032 (0.20660) Loss: 0.25032 (0.20660)
2024-08-30,02:26:50 | INFO | Train Epoch: 45 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 525.652/s, 525.652/s/gpu LR: 5.808076e-09 Logit Scale: 100.000 Contrastive_loss: 0.16918 (0.20490) Loss: 0.16918 (0.20490)
2024-08-30,02:27:09 | INFO | Train Epoch: 45 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 516.569/s, 516.569/s/gpu LR: 5.797740e-09 Logit Scale: 100.000 Contrastive_loss: 0.25627 (0.20713) Loss: 0.25627 (0.20713)
2024-08-30,02:27:29 | INFO | Train Epoch: 45 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 504.673/s, 504.673/s/gpu LR: 5.787400e-09 Logit Scale: 100.000 Contrastive_loss: 0.31824 (0.21176) Loss: 0.31824 (0.21176)
2024-08-30,02:27:48 | INFO | Train Epoch: 45 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.194, 499.623/s, 499.623/s/gpu LR: 5.777057e-09 Logit Scale: 100.000 Contrastive_loss: 0.30579 (0.21552) Loss: 0.30579 (0.21552)
2024-08-30,02:28:07 | INFO | Train Epoch: 45 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.194, 523.624/s, 523.624/s/gpu LR: 5.766711e-09 Logit Scale: 100.000 Contrastive_loss: 0.17100 (0.21381) Loss: 0.17100 (0.21381)
2024-08-30,02:28:27 | INFO | Train Epoch: 45 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 504.542/s, 504.542/s/gpu LR: 5.756361e-09 Logit Scale: 100.000 Contrastive_loss: 0.22055 (0.21406) Loss: 0.22055 (0.21406)
2024-08-30,02:28:46 | INFO | Train Epoch: 45 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 497.609/s, 497.609/s/gpu LR: 5.746008e-09 Logit Scale: 100.000 Contrastive_loss: 0.22387 (0.21441) Loss: 0.22387 (0.21441)
2024-08-30,02:29:05 | INFO | Train Epoch: 45 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 506.525/s, 506.525/s/gpu LR: 5.735651e-09 Logit Scale: 100.000 Contrastive_loss: 0.26059 (0.21600) Loss: 0.26059 (0.21600)
2024-08-30,02:29:24 | INFO | Train Epoch: 45 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 527.785/s, 527.785/s/gpu LR: 5.725292e-09 Logit Scale: 100.000 Contrastive_loss: 0.17389 (0.21460) Loss: 0.17389 (0.21460)
2024-08-30,02:29:44 | INFO | Train Epoch: 45 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.193, 531.214/s, 531.214/s/gpu LR: 5.714929e-09 Logit Scale: 100.000 Contrastive_loss: 0.14744 (0.21243) Loss: 0.14744 (0.21243)
2024-08-30,02:30:03 | INFO | Train Epoch: 45 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 536.841/s, 536.841/s/gpu LR: 5.704563e-09 Logit Scale: 100.000 Contrastive_loss: 0.26892 (0.21420) Loss: 0.26892 (0.21420)
2024-08-30,02:30:22 | INFO | Train Epoch: 45 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 494.044/s, 494.044/s/gpu LR: 5.694194e-09 Logit Scale: 100.000 Contrastive_loss: 0.24747 (0.21521) Loss: 0.24747 (0.21521)
2024-08-30,02:30:42 | INFO | Train Epoch: 45 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 494.699/s, 494.699/s/gpu LR: 5.683821e-09 Logit Scale: 100.000 Contrastive_loss: 0.28890 (0.21737) Loss: 0.28890 (0.21737)
2024-08-30,02:31:01 | INFO | Train Epoch: 45 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 501.006/s, 501.006/s/gpu LR: 5.673446e-09 Logit Scale: 100.000 Contrastive_loss: 0.18919 (0.21657) Loss: 0.18919 (0.21657)
2024-08-30,02:31:20 | INFO | Train Epoch: 45 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 529.681/s, 529.681/s/gpu LR: 5.663068e-09 Logit Scale: 100.000 Contrastive_loss: 0.25876 (0.21774) Loss: 0.25876 (0.21774)
2024-08-30,02:31:39 | INFO | Train Epoch: 45 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 531.978/s, 531.978/s/gpu LR: 5.652687e-09 Logit Scale: 100.000 Contrastive_loss: 0.30664 (0.22014) Loss: 0.30664 (0.22014)
2024-08-30,02:31:59 | INFO | Train Epoch: 45 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 493.921/s, 493.921/s/gpu LR: 5.642303e-09 Logit Scale: 100.000 Contrastive_loss: 0.16032 (0.21857) Loss: 0.16032 (0.21857)
2024-08-30,02:32:18 | INFO | Train Epoch: 45 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 563.360/s, 563.360/s/gpu LR: 5.631917e-09 Logit Scale: 100.000 Contrastive_loss: 0.26436 (0.21974) Loss: 0.26436 (0.21974)
2024-08-30,02:32:37 | INFO | Train Epoch: 45 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 500.545/s, 500.545/s/gpu LR: 5.621527e-09 Logit Scale: 100.000 Contrastive_loss: 0.24082 (0.22027) Loss: 0.24082 (0.22027)
2024-08-30,02:32:57 | INFO | Train Epoch: 45 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 504.430/s, 504.430/s/gpu LR: 5.611135e-09 Logit Scale: 100.000 Contrastive_loss: 0.13713 (0.21824) Loss: 0.13713 (0.21824)
2024-08-30,02:33:16 | INFO | Train Epoch: 45 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 509.359/s, 509.359/s/gpu LR: 5.600740e-09 Logit Scale: 100.000 Contrastive_loss: 0.22941 (0.21851) Loss: 0.22941 (0.21851)
2024-08-30,02:33:35 | INFO | Train Epoch: 45 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 507.044/s, 507.044/s/gpu LR: 5.590343e-09 Logit Scale: 100.000 Contrastive_loss: 0.092302 (0.21557) Loss: 0.092302 (0.21557)
2024-08-30,02:33:43 | INFO | Eval Epoch: 46 [200 / 1000]	Clip Loss: 0.482021	
2024-08-30,02:33:43 | INFO | Eval Epoch: 46 image_to_text_mean_rank: 2.4290	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6650	image_to_text_R@5: 0.9120	image_to_text_R@10: 0.9620	text_to_image_mean_rank: 3.7910	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9670	clip_val_loss: 0.4394	epoch: 46.0000	num_samples: 1000.0000
2024-08-30,02:33:45 | INFO | Start epoch 46
2024-08-30,02:33:45 | INFO | Train Epoch: 46 [   100/145000.0 (0%)] Data (t): 0.035 Batch (t): 0.119, 842.665/s, 842.665/s/gpu LR: 5.864860e-09 Logit Scale: 100.000 Contrastive_loss: 0.17446 (0.17446) Loss: 0.17446 (0.17446)
2024-08-30,02:34:04 | INFO | Train Epoch: 46 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.195, 542.311/s, 542.311/s/gpu LR: 5.854544e-09 Logit Scale: 100.000 Contrastive_loss: 0.34385 (0.25915) Loss: 0.34385 (0.25915)
2024-08-30,02:34:24 | INFO | Train Epoch: 46 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.194, 498.971/s, 498.971/s/gpu LR: 5.844224e-09 Logit Scale: 100.000 Contrastive_loss: 0.35870 (0.29234) Loss: 0.35870 (0.29234)
2024-08-30,02:34:43 | INFO | Train Epoch: 46 [ 30100/145000.0 (21%)] Data (t): 0.110 Batch (t): 0.194, 529.919/s, 529.919/s/gpu LR: 5.833901e-09 Logit Scale: 100.000 Contrastive_loss: 0.18835 (0.26634) Loss: 0.18835 (0.26634)
2024-08-30,02:35:02 | INFO | Train Epoch: 46 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 519.143/s, 519.143/s/gpu LR: 5.823574e-09 Logit Scale: 100.000 Contrastive_loss: 0.13683 (0.24044) Loss: 0.13683 (0.24044)
2024-08-30,02:35:22 | INFO | Train Epoch: 46 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 501.284/s, 501.284/s/gpu LR: 5.813243e-09 Logit Scale: 100.000 Contrastive_loss: 0.16549 (0.22795) Loss: 0.16549 (0.22795)
2024-08-30,02:35:41 | INFO | Train Epoch: 46 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 499.432/s, 499.432/s/gpu LR: 5.802908e-09 Logit Scale: 100.000 Contrastive_loss: 0.12027 (0.21256) Loss: 0.12027 (0.21256)
2024-08-30,02:36:00 | INFO | Train Epoch: 46 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 504.288/s, 504.288/s/gpu LR: 5.792571e-09 Logit Scale: 100.000 Contrastive_loss: 0.22837 (0.21454) Loss: 0.22837 (0.21454)
2024-08-30,02:36:20 | INFO | Train Epoch: 46 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.194, 494.978/s, 494.978/s/gpu LR: 5.782229e-09 Logit Scale: 100.000 Contrastive_loss: 0.13611 (0.20582) Loss: 0.13611 (0.20582)
2024-08-30,02:36:39 | INFO | Train Epoch: 46 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 519.745/s, 519.745/s/gpu LR: 5.771884e-09 Logit Scale: 100.000 Contrastive_loss: 0.21856 (0.20710) Loss: 0.21856 (0.20710)
2024-08-30,02:36:58 | INFO | Train Epoch: 46 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 499.305/s, 499.305/s/gpu LR: 5.761536e-09 Logit Scale: 100.000 Contrastive_loss: 0.15601 (0.20245) Loss: 0.15601 (0.20245)
2024-08-30,02:37:18 | INFO | Train Epoch: 46 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.193, 496.419/s, 496.419/s/gpu LR: 5.751185e-09 Logit Scale: 100.000 Contrastive_loss: 0.17596 (0.20025) Loss: 0.17596 (0.20025)
2024-08-30,02:37:37 | INFO | Train Epoch: 46 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 531.642/s, 531.642/s/gpu LR: 5.740830e-09 Logit Scale: 100.000 Contrastive_loss: 0.27998 (0.20638) Loss: 0.27998 (0.20638)
2024-08-30,02:37:56 | INFO | Train Epoch: 46 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 506.760/s, 506.760/s/gpu LR: 5.730472e-09 Logit Scale: 100.000 Contrastive_loss: 0.15606 (0.20278) Loss: 0.15606 (0.20278)
2024-08-30,02:38:16 | INFO | Train Epoch: 46 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 497.618/s, 497.618/s/gpu LR: 5.720110e-09 Logit Scale: 100.000 Contrastive_loss: 0.18149 (0.20137) Loss: 0.18149 (0.20137)
2024-08-30,02:38:25 | INFO | Train Epoch: 46 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 534.791/s, 534.791/s/gpu LR: 5.715032e-09 Logit Scale: 100.000 Contrastive_loss: 0.22966 (0.20313) Loss: 0.22966 (0.20313)
2024-08-30,02:38:35 | INFO | Train Epoch: 46 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.194, 527.971/s, 527.971/s/gpu LR: 5.709746e-09 Logit Scale: 100.000 Contrastive_loss: 0.20296 (0.20312) Loss: 0.20296 (0.20312)
2024-08-30,02:38:54 | INFO | Train Epoch: 46 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.194, 529.445/s, 529.445/s/gpu LR: 5.699378e-09 Logit Scale: 100.000 Contrastive_loss: 0.23659 (0.20498) Loss: 0.23659 (0.20498)
2024-08-30,02:39:13 | INFO | Train Epoch: 46 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 502.655/s, 502.655/s/gpu LR: 5.689008e-09 Logit Scale: 100.000 Contrastive_loss: 0.17836 (0.20358) Loss: 0.17836 (0.20358)
2024-08-30,02:39:33 | INFO | Train Epoch: 46 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 504.358/s, 504.358/s/gpu LR: 5.678634e-09 Logit Scale: 100.000 Contrastive_loss: 0.20106 (0.20346) Loss: 0.20106 (0.20346)
2024-08-30,02:39:52 | INFO | Train Epoch: 46 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 524.314/s, 524.314/s/gpu LR: 5.668258e-09 Logit Scale: 100.000 Contrastive_loss: 0.24943 (0.20565) Loss: 0.24943 (0.20565)
2024-08-30,02:40:11 | INFO | Train Epoch: 46 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 539.612/s, 539.612/s/gpu LR: 5.657878e-09 Logit Scale: 100.000 Contrastive_loss: 0.16847 (0.20396) Loss: 0.16847 (0.20396)
2024-08-30,02:40:31 | INFO | Train Epoch: 46 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 493.365/s, 493.365/s/gpu LR: 5.647496e-09 Logit Scale: 100.000 Contrastive_loss: 0.25533 (0.20619) Loss: 0.25533 (0.20619)
2024-08-30,02:40:50 | INFO | Train Epoch: 46 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 499.502/s, 499.502/s/gpu LR: 5.637110e-09 Logit Scale: 100.000 Contrastive_loss: 0.31664 (0.21079) Loss: 0.31664 (0.21079)
2024-08-30,02:41:09 | INFO | Train Epoch: 46 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.194, 503.919/s, 503.919/s/gpu LR: 5.626722e-09 Logit Scale: 100.000 Contrastive_loss: 0.30476 (0.21455) Loss: 0.30476 (0.21455)
2024-08-30,02:41:29 | INFO | Train Epoch: 46 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 527.390/s, 527.390/s/gpu LR: 5.616331e-09 Logit Scale: 100.000 Contrastive_loss: 0.16993 (0.21283) Loss: 0.16993 (0.21283)
2024-08-30,02:41:48 | INFO | Train Epoch: 46 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 504.267/s, 504.267/s/gpu LR: 5.605938e-09 Logit Scale: 100.000 Contrastive_loss: 0.21982 (0.21309) Loss: 0.21982 (0.21309)
2024-08-30,02:42:07 | INFO | Train Epoch: 46 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 490.576/s, 490.576/s/gpu LR: 5.595542e-09 Logit Scale: 100.000 Contrastive_loss: 0.22297 (0.21344) Loss: 0.22297 (0.21344)
2024-08-30,02:42:27 | INFO | Train Epoch: 46 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 535.669/s, 535.669/s/gpu LR: 5.585143e-09 Logit Scale: 100.000 Contrastive_loss: 0.25996 (0.21505) Loss: 0.25996 (0.21505)
2024-08-30,02:42:46 | INFO | Train Epoch: 46 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 495.398/s, 495.398/s/gpu LR: 5.574742e-09 Logit Scale: 100.000 Contrastive_loss: 0.17306 (0.21365) Loss: 0.17306 (0.21365)
2024-08-30,02:43:05 | INFO | Train Epoch: 46 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 501.960/s, 501.960/s/gpu LR: 5.564338e-09 Logit Scale: 100.000 Contrastive_loss: 0.14706 (0.21150) Loss: 0.14706 (0.21150)
2024-08-30,02:43:24 | INFO | Train Epoch: 46 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 502.744/s, 502.744/s/gpu LR: 5.553931e-09 Logit Scale: 100.000 Contrastive_loss: 0.26785 (0.21326) Loss: 0.26785 (0.21326)
2024-08-30,02:43:44 | INFO | Train Epoch: 46 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 533.122/s, 533.122/s/gpu LR: 5.543523e-09 Logit Scale: 100.000 Contrastive_loss: 0.24695 (0.21428) Loss: 0.24695 (0.21428)
2024-08-30,02:44:03 | INFO | Train Epoch: 46 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 535.136/s, 535.136/s/gpu LR: 5.533111e-09 Logit Scale: 100.000 Contrastive_loss: 0.28755 (0.21644) Loss: 0.28755 (0.21644)
2024-08-30,02:44:22 | INFO | Train Epoch: 46 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 535.003/s, 535.003/s/gpu LR: 5.522698e-09 Logit Scale: 100.000 Contrastive_loss: 0.18859 (0.21564) Loss: 0.18859 (0.21564)
2024-08-30,02:44:42 | INFO | Train Epoch: 46 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.194, 505.315/s, 505.315/s/gpu LR: 5.512282e-09 Logit Scale: 100.000 Contrastive_loss: 0.25713 (0.21679) Loss: 0.25713 (0.21679)
2024-08-30,02:45:01 | INFO | Train Epoch: 46 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 498.286/s, 498.286/s/gpu LR: 5.501864e-09 Logit Scale: 100.000 Contrastive_loss: 0.30550 (0.21919) Loss: 0.30550 (0.21919)
2024-08-30,02:45:20 | INFO | Train Epoch: 46 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.192, 529.782/s, 529.782/s/gpu LR: 5.491444e-09 Logit Scale: 100.000 Contrastive_loss: 0.15930 (0.21762) Loss: 0.15930 (0.21762)
2024-08-30,02:45:39 | INFO | Train Epoch: 46 [370100/145000.0 (255%)] Data (t): 0.107 Batch (t): 0.192, 527.241/s, 527.241/s/gpu LR: 5.481021e-09 Logit Scale: 100.000 Contrastive_loss: 0.26347 (0.21879) Loss: 0.26347 (0.21879)
2024-08-30,02:45:59 | INFO | Train Epoch: 46 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 501.569/s, 501.569/s/gpu LR: 5.470597e-09 Logit Scale: 100.000 Contrastive_loss: 0.23990 (0.21932) Loss: 0.23990 (0.21932)
2024-08-30,02:46:18 | INFO | Train Epoch: 46 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 498.978/s, 498.978/s/gpu LR: 5.460170e-09 Logit Scale: 100.000 Contrastive_loss: 0.13623 (0.21729) Loss: 0.13623 (0.21729)
2024-08-30,02:46:37 | INFO | Train Epoch: 46 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 532.682/s, 532.682/s/gpu LR: 5.449742e-09 Logit Scale: 100.000 Contrastive_loss: 0.22836 (0.21756) Loss: 0.22836 (0.21756)
2024-08-30,02:46:57 | INFO | Train Epoch: 46 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.194, 497.916/s, 497.916/s/gpu LR: 5.439311e-09 Logit Scale: 100.000 Contrastive_loss: 0.091722 (0.21463) Loss: 0.091722 (0.21463)
2024-08-30,02:47:04 | INFO | Eval Epoch: 47 [200 / 1000]	Clip Loss: 0.482069	
2024-08-30,02:47:05 | INFO | Eval Epoch: 47 image_to_text_mean_rank: 2.4290	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6650	image_to_text_R@5: 0.9120	image_to_text_R@10: 0.9620	text_to_image_mean_rank: 3.8060	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9080	text_to_image_R@10: 0.9670	clip_val_loss: 0.4394	epoch: 47.0000	num_samples: 1000.0000
2024-08-30,02:47:06 | INFO | Start epoch 47
2024-08-30,02:47:06 | INFO | Train Epoch: 47 [   100/145000.0 (0%)] Data (t): 0.023 Batch (t): 0.109, 914.569/s, 914.569/s/gpu LR: 5.714929e-09 Logit Scale: 100.000 Contrastive_loss: 0.17367 (0.17367) Loss: 0.17367 (0.17367)
2024-08-30,02:47:26 | INFO | Train Epoch: 47 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 499.075/s, 499.075/s/gpu LR: 5.704563e-09 Logit Scale: 100.000 Contrastive_loss: 0.34276 (0.25822) Loss: 0.34276 (0.25822)
2024-08-30,02:47:45 | INFO | Train Epoch: 47 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 497.061/s, 497.061/s/gpu LR: 5.694194e-09 Logit Scale: 100.000 Contrastive_loss: 0.35738 (0.29127) Loss: 0.35738 (0.29127)
2024-08-30,02:48:04 | INFO | Train Epoch: 47 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 499.588/s, 499.588/s/gpu LR: 5.683821e-09 Logit Scale: 100.000 Contrastive_loss: 0.18721 (0.26526) Loss: 0.18721 (0.26526)
2024-08-30,02:48:23 | INFO | Train Epoch: 47 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 499.468/s, 499.468/s/gpu LR: 5.673446e-09 Logit Scale: 100.000 Contrastive_loss: 0.13629 (0.23946) Loss: 0.13629 (0.23946)
2024-08-30,02:48:43 | INFO | Train Epoch: 47 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 492.494/s, 492.494/s/gpu LR: 5.663068e-09 Logit Scale: 100.000 Contrastive_loss: 0.16453 (0.22697) Loss: 0.16453 (0.22697)
2024-08-30,02:49:02 | INFO | Train Epoch: 47 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 495.992/s, 495.992/s/gpu LR: 5.652687e-09 Logit Scale: 100.000 Contrastive_loss: 0.11972 (0.21165) Loss: 0.11972 (0.21165)
2024-08-30,02:49:21 | INFO | Train Epoch: 47 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.194, 518.836/s, 518.836/s/gpu LR: 5.642303e-09 Logit Scale: 100.000 Contrastive_loss: 0.22702 (0.21357) Loss: 0.22702 (0.21357)
2024-08-30,02:49:41 | INFO | Train Epoch: 47 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.192, 499.401/s, 499.401/s/gpu LR: 5.631917e-09 Logit Scale: 100.000 Contrastive_loss: 0.13518 (0.20486) Loss: 0.13518 (0.20486)
2024-08-30,02:50:00 | INFO | Train Epoch: 47 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.194, 522.513/s, 522.513/s/gpu LR: 5.621527e-09 Logit Scale: 100.000 Contrastive_loss: 0.21756 (0.20613) Loss: 0.21756 (0.20613)
2024-08-30,02:50:19 | INFO | Train Epoch: 47 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 500.042/s, 500.042/s/gpu LR: 5.611135e-09 Logit Scale: 100.000 Contrastive_loss: 0.15536 (0.20152) Loss: 0.15536 (0.20152)
2024-08-30,02:50:39 | INFO | Train Epoch: 47 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 503.544/s, 503.544/s/gpu LR: 5.600740e-09 Logit Scale: 100.000 Contrastive_loss: 0.17516 (0.19932) Loss: 0.17516 (0.19932)
2024-08-30,02:50:58 | INFO | Train Epoch: 47 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 541.609/s, 541.609/s/gpu LR: 5.590343e-09 Logit Scale: 100.000 Contrastive_loss: 0.27881 (0.20543) Loss: 0.27881 (0.20543)
2024-08-30,02:51:17 | INFO | Train Epoch: 47 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 503.986/s, 503.986/s/gpu LR: 5.579943e-09 Logit Scale: 100.000 Contrastive_loss: 0.15541 (0.20186) Loss: 0.15541 (0.20186)
2024-08-30,02:51:36 | INFO | Train Epoch: 47 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 537.791/s, 537.791/s/gpu LR: 5.569540e-09 Logit Scale: 100.000 Contrastive_loss: 0.18099 (0.20047) Loss: 0.18099 (0.20047)
2024-08-30,02:51:46 | INFO | Train Epoch: 47 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 534.752/s, 534.752/s/gpu LR: 5.564442e-09 Logit Scale: 100.000 Contrastive_loss: 0.22863 (0.20223) Loss: 0.22863 (0.20223)
2024-08-30,02:51:56 | INFO | Train Epoch: 47 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.192, 531.831/s, 531.831/s/gpu LR: 5.559135e-09 Logit Scale: 100.000 Contrastive_loss: 0.20203 (0.20222) Loss: 0.20203 (0.20222)
2024-08-30,02:52:15 | INFO | Train Epoch: 47 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 526.020/s, 526.020/s/gpu LR: 5.548727e-09 Logit Scale: 100.000 Contrastive_loss: 0.23522 (0.20405) Loss: 0.23522 (0.20405)
2024-08-30,02:52:34 | INFO | Train Epoch: 47 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 501.617/s, 501.617/s/gpu LR: 5.538317e-09 Logit Scale: 100.000 Contrastive_loss: 0.17745 (0.20265) Loss: 0.17745 (0.20265)
2024-08-30,02:52:53 | INFO | Train Epoch: 47 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 522.096/s, 522.096/s/gpu LR: 5.527905e-09 Logit Scale: 100.000 Contrastive_loss: 0.20041 (0.20254) Loss: 0.20041 (0.20254)
2024-08-30,02:53:13 | INFO | Train Epoch: 47 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 525.999/s, 525.999/s/gpu LR: 5.517490e-09 Logit Scale: 100.000 Contrastive_loss: 0.24845 (0.20473) Loss: 0.24845 (0.20473)
2024-08-30,02:53:32 | INFO | Train Epoch: 47 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.192, 495.593/s, 495.593/s/gpu LR: 5.507073e-09 Logit Scale: 100.000 Contrastive_loss: 0.16786 (0.20305) Loss: 0.16786 (0.20305)
2024-08-30,02:53:51 | INFO | Train Epoch: 47 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 532.551/s, 532.551/s/gpu LR: 5.496654e-09 Logit Scale: 100.000 Contrastive_loss: 0.25436 (0.20528) Loss: 0.25436 (0.20528)
2024-08-30,02:54:11 | INFO | Train Epoch: 47 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 502.701/s, 502.701/s/gpu LR: 5.486233e-09 Logit Scale: 100.000 Contrastive_loss: 0.31515 (0.20986) Loss: 0.31515 (0.20986)
2024-08-30,02:54:30 | INFO | Train Epoch: 47 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 531.899/s, 531.899/s/gpu LR: 5.475809e-09 Logit Scale: 100.000 Contrastive_loss: 0.30385 (0.21362) Loss: 0.30385 (0.21362)
2024-08-30,02:54:49 | INFO | Train Epoch: 47 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 527.696/s, 527.696/s/gpu LR: 5.465384e-09 Logit Scale: 100.000 Contrastive_loss: 0.16887 (0.21190) Loss: 0.16887 (0.21190)
2024-08-30,02:55:08 | INFO | Train Epoch: 47 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 491.326/s, 491.326/s/gpu LR: 5.454956e-09 Logit Scale: 100.000 Contrastive_loss: 0.21918 (0.21217) Loss: 0.21918 (0.21217)
2024-08-30,02:55:28 | INFO | Train Epoch: 47 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.192, 494.100/s, 494.100/s/gpu LR: 5.444527e-09 Logit Scale: 100.000 Contrastive_loss: 0.22210 (0.21252) Loss: 0.22210 (0.21252)
2024-08-30,02:55:47 | INFO | Train Epoch: 47 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.194, 525.780/s, 525.780/s/gpu LR: 5.434095e-09 Logit Scale: 100.000 Contrastive_loss: 0.25924 (0.21413) Loss: 0.25924 (0.21413)
2024-08-30,02:56:06 | INFO | Train Epoch: 47 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 530.223/s, 530.223/s/gpu LR: 5.423662e-09 Logit Scale: 100.000 Contrastive_loss: 0.17244 (0.21274) Loss: 0.17244 (0.21274)
2024-08-30,02:56:26 | INFO | Train Epoch: 47 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.193, 537.711/s, 537.711/s/gpu LR: 5.413226e-09 Logit Scale: 100.000 Contrastive_loss: 0.14668 (0.21061) Loss: 0.14668 (0.21061)
2024-08-30,02:56:45 | INFO | Train Epoch: 47 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.194, 528.765/s, 528.765/s/gpu LR: 5.402789e-09 Logit Scale: 100.000 Contrastive_loss: 0.26680 (0.21237) Loss: 0.26680 (0.21237)
2024-08-30,02:57:04 | INFO | Train Epoch: 47 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.194, 489.064/s, 489.064/s/gpu LR: 5.392350e-09 Logit Scale: 100.000 Contrastive_loss: 0.24626 (0.21340) Loss: 0.24626 (0.21340)
2024-08-30,02:57:24 | INFO | Train Epoch: 47 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 493.606/s, 493.606/s/gpu LR: 5.381910e-09 Logit Scale: 100.000 Contrastive_loss: 0.28645 (0.21554) Loss: 0.28645 (0.21554)
2024-08-30,02:57:43 | INFO | Train Epoch: 47 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 490.473/s, 490.473/s/gpu LR: 5.371467e-09 Logit Scale: 100.000 Contrastive_loss: 0.18811 (0.21476) Loss: 0.18811 (0.21476)
2024-08-30,02:58:02 | INFO | Train Epoch: 47 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 495.504/s, 495.504/s/gpu LR: 5.361023e-09 Logit Scale: 100.000 Contrastive_loss: 0.25564 (0.21590) Loss: 0.25564 (0.21590)
2024-08-30,02:58:21 | INFO | Train Epoch: 47 [350100/145000.0 (241%)] Data (t): 0.107 Batch (t): 0.192, 537.785/s, 537.785/s/gpu LR: 5.350578e-09 Logit Scale: 100.000 Contrastive_loss: 0.30441 (0.21829) Loss: 0.30441 (0.21829)
2024-08-30,02:58:41 | INFO | Train Epoch: 47 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 501.424/s, 501.424/s/gpu LR: 5.340131e-09 Logit Scale: 100.000 Contrastive_loss: 0.15853 (0.21672) Loss: 0.15853 (0.21672)
2024-08-30,02:59:00 | INFO | Train Epoch: 47 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 535.737/s, 535.737/s/gpu LR: 5.329682e-09 Logit Scale: 100.000 Contrastive_loss: 0.26263 (0.21789) Loss: 0.26263 (0.21789)
2024-08-30,02:59:19 | INFO | Train Epoch: 47 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 522.560/s, 522.560/s/gpu LR: 5.319232e-09 Logit Scale: 100.000 Contrastive_loss: 0.23897 (0.21842) Loss: 0.23897 (0.21842)
2024-08-30,02:59:39 | INFO | Train Epoch: 47 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 526.760/s, 526.760/s/gpu LR: 5.308781e-09 Logit Scale: 100.000 Contrastive_loss: 0.13550 (0.21640) Loss: 0.13550 (0.21640)
2024-08-30,02:59:58 | INFO | Train Epoch: 47 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.193, 526.403/s, 526.403/s/gpu LR: 5.298328e-09 Logit Scale: 100.000 Contrastive_loss: 0.22755 (0.21666) Loss: 0.22755 (0.21666)
2024-08-30,03:00:17 | INFO | Train Epoch: 47 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 535.706/s, 535.706/s/gpu LR: 5.287874e-09 Logit Scale: 100.000 Contrastive_loss: 0.091144 (0.21374) Loss: 0.091144 (0.21374)
2024-08-30,03:00:25 | INFO | Eval Epoch: 48 [200 / 1000]	Clip Loss: 0.481994	
2024-08-30,03:00:25 | INFO | Eval Epoch: 48 image_to_text_mean_rank: 2.4270	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6660	image_to_text_R@5: 0.9100	image_to_text_R@10: 0.9620	text_to_image_mean_rank: 3.8100	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6700	text_to_image_R@5: 0.9080	text_to_image_R@10: 0.9670	clip_val_loss: 0.4393	epoch: 48.0000	num_samples: 1000.0000
2024-08-30,03:00:27 | INFO | Start epoch 48
2024-08-30,03:00:27 | INFO | Train Epoch: 48 [   100/145000.0 (0%)] Data (t): 0.028 Batch (t): 0.115, 871.229/s, 871.229/s/gpu LR: 5.564338e-09 Logit Scale: 100.000 Contrastive_loss: 0.17284 (0.17284) Loss: 0.17284 (0.17284)
2024-08-30,03:00:46 | INFO | Train Epoch: 48 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 500.389/s, 500.389/s/gpu LR: 5.553931e-09 Logit Scale: 100.000 Contrastive_loss: 0.34181 (0.25732) Loss: 0.34181 (0.25732)
2024-08-30,03:01:05 | INFO | Train Epoch: 48 [ 20100/145000.0 (14%)] Data (t): 0.110 Batch (t): 0.194, 522.971/s, 522.971/s/gpu LR: 5.543523e-09 Logit Scale: 100.000 Contrastive_loss: 0.35567 (0.29011) Loss: 0.35567 (0.29011)
2024-08-30,03:01:25 | INFO | Train Epoch: 48 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 492.992/s, 492.992/s/gpu LR: 5.533111e-09 Logit Scale: 100.000 Contrastive_loss: 0.18641 (0.26418) Loss: 0.18641 (0.26418)
2024-08-30,03:01:44 | INFO | Train Epoch: 48 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 490.538/s, 490.538/s/gpu LR: 5.522698e-09 Logit Scale: 100.000 Contrastive_loss: 0.13584 (0.23851) Loss: 0.13584 (0.23851)
2024-08-30,03:02:03 | INFO | Train Epoch: 48 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 518.734/s, 518.734/s/gpu LR: 5.512282e-09 Logit Scale: 100.000 Contrastive_loss: 0.16350 (0.22601) Loss: 0.16350 (0.22601)
2024-08-30,03:02:23 | INFO | Train Epoch: 48 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 504.409/s, 504.409/s/gpu LR: 5.501864e-09 Logit Scale: 100.000 Contrastive_loss: 0.11919 (0.21075) Loss: 0.11919 (0.21075)
2024-08-30,03:02:42 | INFO | Train Epoch: 48 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 533.445/s, 533.445/s/gpu LR: 5.491444e-09 Logit Scale: 100.000 Contrastive_loss: 0.22569 (0.21262) Loss: 0.22569 (0.21262)
2024-08-30,03:03:01 | INFO | Train Epoch: 48 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 506.879/s, 506.879/s/gpu LR: 5.481021e-09 Logit Scale: 100.000 Contrastive_loss: 0.13431 (0.20392) Loss: 0.13431 (0.20392)
2024-08-30,03:03:21 | INFO | Train Epoch: 48 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 537.169/s, 537.169/s/gpu LR: 5.470597e-09 Logit Scale: 100.000 Contrastive_loss: 0.21671 (0.20520) Loss: 0.21671 (0.20520)
2024-08-30,03:03:40 | INFO | Train Epoch: 48 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 507.043/s, 507.043/s/gpu LR: 5.460170e-09 Logit Scale: 100.000 Contrastive_loss: 0.15479 (0.20062) Loss: 0.15479 (0.20062)
2024-08-30,03:03:59 | INFO | Train Epoch: 48 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 521.090/s, 521.090/s/gpu LR: 5.449742e-09 Logit Scale: 100.000 Contrastive_loss: 0.17435 (0.19843) Loss: 0.17435 (0.19843)
2024-08-30,03:04:18 | INFO | Train Epoch: 48 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 527.468/s, 527.468/s/gpu LR: 5.439311e-09 Logit Scale: 100.000 Contrastive_loss: 0.27764 (0.20452) Loss: 0.27764 (0.20452)
2024-08-30,03:04:38 | INFO | Train Epoch: 48 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 531.473/s, 531.473/s/gpu LR: 5.428879e-09 Logit Scale: 100.000 Contrastive_loss: 0.15482 (0.20097) Loss: 0.15482 (0.20097)
2024-08-30,03:04:57 | INFO | Train Epoch: 48 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 501.446/s, 501.446/s/gpu LR: 5.418444e-09 Logit Scale: 100.000 Contrastive_loss: 0.18054 (0.19961) Loss: 0.18054 (0.19961)
2024-08-30,03:05:06 | INFO | Train Epoch: 48 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 528.843/s, 528.843/s/gpu LR: 5.413331e-09 Logit Scale: 100.000 Contrastive_loss: 0.22743 (0.20135) Loss: 0.22743 (0.20135)
2024-08-30,03:05:16 | INFO | Train Epoch: 48 [150100/145000.0 (104%)] Data (t): 0.110 Batch (t): 0.194, 495.896/s, 495.896/s/gpu LR: 5.408008e-09 Logit Scale: 100.000 Contrastive_loss: 0.20094 (0.20132) Loss: 0.20094 (0.20132)
2024-08-30,03:05:36 | INFO | Train Epoch: 48 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 525.177/s, 525.177/s/gpu LR: 5.397570e-09 Logit Scale: 100.000 Contrastive_loss: 0.23403 (0.20314) Loss: 0.23403 (0.20314)
2024-08-30,03:05:55 | INFO | Train Epoch: 48 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 534.259/s, 534.259/s/gpu LR: 5.387130e-09 Logit Scale: 100.000 Contrastive_loss: 0.17664 (0.20175) Loss: 0.17664 (0.20175)
2024-08-30,03:06:14 | INFO | Train Epoch: 48 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 526.770/s, 526.770/s/gpu LR: 5.376689e-09 Logit Scale: 100.000 Contrastive_loss: 0.19984 (0.20165) Loss: 0.19984 (0.20165)
2024-08-30,03:06:34 | INFO | Train Epoch: 48 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 492.275/s, 492.275/s/gpu LR: 5.366246e-09 Logit Scale: 100.000 Contrastive_loss: 0.24745 (0.20383) Loss: 0.24745 (0.20383)
2024-08-30,03:06:53 | INFO | Train Epoch: 48 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.192, 525.966/s, 525.966/s/gpu LR: 5.355801e-09 Logit Scale: 100.000 Contrastive_loss: 0.16727 (0.20217) Loss: 0.16727 (0.20217)
2024-08-30,03:07:12 | INFO | Train Epoch: 48 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 529.769/s, 529.769/s/gpu LR: 5.345355e-09 Logit Scale: 100.000 Contrastive_loss: 0.25363 (0.20441) Loss: 0.25363 (0.20441)
2024-08-30,03:07:31 | INFO | Train Epoch: 48 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 536.885/s, 536.885/s/gpu LR: 5.334907e-09 Logit Scale: 100.000 Contrastive_loss: 0.31363 (0.20896) Loss: 0.31363 (0.20896)
2024-08-30,03:07:51 | INFO | Train Epoch: 48 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 531.669/s, 531.669/s/gpu LR: 5.324458e-09 Logit Scale: 100.000 Contrastive_loss: 0.30282 (0.21271) Loss: 0.30282 (0.21271)
2024-08-30,03:08:10 | INFO | Train Epoch: 48 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 497.150/s, 497.150/s/gpu LR: 5.314007e-09 Logit Scale: 100.000 Contrastive_loss: 0.16789 (0.21099) Loss: 0.16789 (0.21099)
2024-08-30,03:08:29 | INFO | Train Epoch: 48 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 493.789/s, 493.789/s/gpu LR: 5.303555e-09 Logit Scale: 100.000 Contrastive_loss: 0.21862 (0.21127) Loss: 0.21862 (0.21127)
2024-08-30,03:08:48 | INFO | Train Epoch: 48 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 495.307/s, 495.307/s/gpu LR: 5.293102e-09 Logit Scale: 100.000 Contrastive_loss: 0.22114 (0.21162) Loss: 0.22114 (0.21162)
2024-08-30,03:09:08 | INFO | Train Epoch: 48 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 496.622/s, 496.622/s/gpu LR: 5.282647e-09 Logit Scale: 100.000 Contrastive_loss: 0.25856 (0.21324) Loss: 0.25856 (0.21324)
2024-08-30,03:09:27 | INFO | Train Epoch: 48 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 522.563/s, 522.563/s/gpu LR: 5.272191e-09 Logit Scale: 100.000 Contrastive_loss: 0.17180 (0.21186) Loss: 0.17180 (0.21186)
2024-08-30,03:09:46 | INFO | Train Epoch: 48 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 537.701/s, 537.701/s/gpu LR: 5.261734e-09 Logit Scale: 100.000 Contrastive_loss: 0.14628 (0.20975) Loss: 0.14628 (0.20975)
2024-08-30,03:10:06 | INFO | Train Epoch: 48 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 509.964/s, 509.964/s/gpu LR: 5.251276e-09 Logit Scale: 100.000 Contrastive_loss: 0.26598 (0.21150) Loss: 0.26598 (0.21150)
2024-08-30,03:10:25 | INFO | Train Epoch: 48 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 508.118/s, 508.118/s/gpu LR: 5.240816e-09 Logit Scale: 100.000 Contrastive_loss: 0.24563 (0.21254) Loss: 0.24563 (0.21254)
2024-08-30,03:10:44 | INFO | Train Epoch: 48 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.194, 501.642/s, 501.642/s/gpu LR: 5.230356e-09 Logit Scale: 100.000 Contrastive_loss: 0.28539 (0.21468) Loss: 0.28539 (0.21468)
2024-08-30,03:11:04 | INFO | Train Epoch: 48 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.194, 523.607/s, 523.607/s/gpu LR: 5.219894e-09 Logit Scale: 100.000 Contrastive_loss: 0.18747 (0.21390) Loss: 0.18747 (0.21390)
2024-08-30,03:11:23 | INFO | Train Epoch: 48 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 511.146/s, 511.146/s/gpu LR: 5.209432e-09 Logit Scale: 100.000 Contrastive_loss: 0.25415 (0.21502) Loss: 0.25415 (0.21502)
2024-08-30,03:11:42 | INFO | Train Epoch: 48 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 536.777/s, 536.777/s/gpu LR: 5.198969e-09 Logit Scale: 100.000 Contrastive_loss: 0.30328 (0.21741) Loss: 0.30328 (0.21741)
2024-08-30,03:12:02 | INFO | Train Epoch: 48 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 505.744/s, 505.744/s/gpu LR: 5.188504e-09 Logit Scale: 100.000 Contrastive_loss: 0.15774 (0.21584) Loss: 0.15774 (0.21584)
2024-08-30,03:12:21 | INFO | Train Epoch: 48 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.194, 534.363/s, 534.363/s/gpu LR: 5.178039e-09 Logit Scale: 100.000 Contrastive_loss: 0.26160 (0.21701) Loss: 0.26160 (0.21701)
2024-08-30,03:12:40 | INFO | Train Epoch: 48 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 539.098/s, 539.098/s/gpu LR: 5.167574e-09 Logit Scale: 100.000 Contrastive_loss: 0.23808 (0.21754) Loss: 0.23808 (0.21754)
2024-08-30,03:12:59 | INFO | Train Epoch: 48 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 504.472/s, 504.472/s/gpu LR: 5.157107e-09 Logit Scale: 100.000 Contrastive_loss: 0.13461 (0.21551) Loss: 0.13461 (0.21551)
2024-08-30,03:13:19 | INFO | Train Epoch: 48 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 535.089/s, 535.089/s/gpu LR: 5.146640e-09 Logit Scale: 100.000 Contrastive_loss: 0.22660 (0.21578) Loss: 0.22660 (0.21578)
2024-08-30,03:13:38 | INFO | Train Epoch: 48 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 503.074/s, 503.074/s/gpu LR: 5.136172e-09 Logit Scale: 100.000 Contrastive_loss: 0.090629 (0.21287) Loss: 0.090629 (0.21287)
2024-08-30,03:13:46 | INFO | Eval Epoch: 49 [200 / 1000]	Clip Loss: 0.482055	
2024-08-30,03:13:46 | INFO | Eval Epoch: 49 image_to_text_mean_rank: 2.4280	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6660	image_to_text_R@5: 0.9100	image_to_text_R@10: 0.9620	text_to_image_mean_rank: 3.8230	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6700	text_to_image_R@5: 0.9080	text_to_image_R@10: 0.9670	clip_val_loss: 0.4393	epoch: 49.0000	num_samples: 1000.0000
2024-08-30,03:13:47 | INFO | Start epoch 49
2024-08-30,03:13:48 | INFO | Train Epoch: 49 [   100/145000.0 (0%)] Data (t): 0.021 Batch (t): 0.105, 956.531/s, 956.531/s/gpu LR: 5.413226e-09 Logit Scale: 100.000 Contrastive_loss: 0.17216 (0.17216) Loss: 0.17216 (0.17216)
2024-08-30,03:14:07 | INFO | Train Epoch: 49 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 503.737/s, 503.737/s/gpu LR: 5.402789e-09 Logit Scale: 100.000 Contrastive_loss: 0.34073 (0.25645) Loss: 0.34073 (0.25645)
2024-08-30,03:14:26 | INFO | Train Epoch: 49 [ 20100/145000.0 (14%)] Data (t): 0.110 Batch (t): 0.194, 507.671/s, 507.671/s/gpu LR: 5.392350e-09 Logit Scale: 100.000 Contrastive_loss: 0.35433 (0.28907) Loss: 0.35433 (0.28907)
2024-08-30,03:14:46 | INFO | Train Epoch: 49 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 523.205/s, 523.205/s/gpu LR: 5.381910e-09 Logit Scale: 100.000 Contrastive_loss: 0.18526 (0.26312) Loss: 0.18526 (0.26312)
2024-08-30,03:15:05 | INFO | Train Epoch: 49 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.194, 526.951/s, 526.951/s/gpu LR: 5.371467e-09 Logit Scale: 100.000 Contrastive_loss: 0.13532 (0.23756) Loss: 0.13532 (0.23756)
2024-08-30,03:15:24 | INFO | Train Epoch: 49 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 503.801/s, 503.801/s/gpu LR: 5.361023e-09 Logit Scale: 100.000 Contrastive_loss: 0.16275 (0.22509) Loss: 0.16275 (0.22509)
2024-08-30,03:15:44 | INFO | Train Epoch: 49 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 490.709/s, 490.709/s/gpu LR: 5.350578e-09 Logit Scale: 100.000 Contrastive_loss: 0.11872 (0.20990) Loss: 0.11872 (0.20990)
2024-08-30,03:16:03 | INFO | Train Epoch: 49 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 508.990/s, 508.990/s/gpu LR: 5.340131e-09 Logit Scale: 100.000 Contrastive_loss: 0.22420 (0.21168) Loss: 0.22420 (0.21168)
2024-08-30,03:16:22 | INFO | Train Epoch: 49 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 499.461/s, 499.461/s/gpu LR: 5.329682e-09 Logit Scale: 100.000 Contrastive_loss: 0.13353 (0.20300) Loss: 0.13353 (0.20300)
2024-08-30,03:16:42 | INFO | Train Epoch: 49 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 567.745/s, 567.745/s/gpu LR: 5.319232e-09 Logit Scale: 100.000 Contrastive_loss: 0.21589 (0.20429) Loss: 0.21589 (0.20429)
2024-08-30,03:17:01 | INFO | Train Epoch: 49 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 505.929/s, 505.929/s/gpu LR: 5.308781e-09 Logit Scale: 100.000 Contrastive_loss: 0.15410 (0.19972) Loss: 0.15410 (0.19972)
2024-08-30,03:17:20 | INFO | Train Epoch: 49 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.193, 498.975/s, 498.975/s/gpu LR: 5.298328e-09 Logit Scale: 100.000 Contrastive_loss: 0.17366 (0.19755) Loss: 0.17366 (0.19755)
2024-08-30,03:17:40 | INFO | Train Epoch: 49 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 501.444/s, 501.444/s/gpu LR: 5.287874e-09 Logit Scale: 100.000 Contrastive_loss: 0.27676 (0.20365) Loss: 0.27676 (0.20365)
2024-08-30,03:17:59 | INFO | Train Epoch: 49 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 537.606/s, 537.606/s/gpu LR: 5.277419e-09 Logit Scale: 100.000 Contrastive_loss: 0.15414 (0.20011) Loss: 0.15414 (0.20011)
2024-08-30,03:18:18 | INFO | Train Epoch: 49 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 519.923/s, 519.923/s/gpu LR: 5.266963e-09 Logit Scale: 100.000 Contrastive_loss: 0.17998 (0.19877) Loss: 0.17998 (0.19877)
2024-08-30,03:18:28 | INFO | Train Epoch: 49 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.194, 518.665/s, 518.665/s/gpu LR: 5.261838e-09 Logit Scale: 100.000 Contrastive_loss: 0.22614 (0.20048) Loss: 0.22614 (0.20048)
2024-08-30,03:18:38 | INFO | Train Epoch: 49 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.192, 537.028/s, 537.028/s/gpu LR: 5.256505e-09 Logit Scale: 100.000 Contrastive_loss: 0.20007 (0.20045) Loss: 0.20007 (0.20045)
2024-08-30,03:18:57 | INFO | Train Epoch: 49 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 535.745/s, 535.745/s/gpu LR: 5.246046e-09 Logit Scale: 100.000 Contrastive_loss: 0.23261 (0.20224) Loss: 0.23261 (0.20224)
2024-08-30,03:19:16 | INFO | Train Epoch: 49 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 502.011/s, 502.011/s/gpu LR: 5.235586e-09 Logit Scale: 100.000 Contrastive_loss: 0.17571 (0.20084) Loss: 0.17571 (0.20084)
2024-08-30,03:19:35 | INFO | Train Epoch: 49 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 520.034/s, 520.034/s/gpu LR: 5.225125e-09 Logit Scale: 100.000 Contrastive_loss: 0.19926 (0.20076) Loss: 0.19926 (0.20076)
2024-08-30,03:19:55 | INFO | Train Epoch: 49 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 528.438/s, 528.438/s/gpu LR: 5.214663e-09 Logit Scale: 100.000 Contrastive_loss: 0.24649 (0.20294) Loss: 0.24649 (0.20294)
2024-08-30,03:20:14 | INFO | Train Epoch: 49 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 523.357/s, 523.357/s/gpu LR: 5.204200e-09 Logit Scale: 100.000 Contrastive_loss: 0.16669 (0.20129) Loss: 0.16669 (0.20129)
2024-08-30,03:20:33 | INFO | Train Epoch: 49 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 532.526/s, 532.526/s/gpu LR: 5.193737e-09 Logit Scale: 100.000 Contrastive_loss: 0.25268 (0.20353) Loss: 0.25268 (0.20353)
2024-08-30,03:20:53 | INFO | Train Epoch: 49 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 518.723/s, 518.723/s/gpu LR: 5.183272e-09 Logit Scale: 100.000 Contrastive_loss: 0.31235 (0.20806) Loss: 0.31235 (0.20806)
2024-08-30,03:21:12 | INFO | Train Epoch: 49 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 526.115/s, 526.115/s/gpu LR: 5.172807e-09 Logit Scale: 100.000 Contrastive_loss: 0.30214 (0.21183) Loss: 0.30214 (0.21183)
2024-08-30,03:21:31 | INFO | Train Epoch: 49 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 527.771/s, 527.771/s/gpu LR: 5.162341e-09 Logit Scale: 100.000 Contrastive_loss: 0.16703 (0.21010) Loss: 0.16703 (0.21010)
2024-08-30,03:21:50 | INFO | Train Epoch: 49 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 500.224/s, 500.224/s/gpu LR: 5.151874e-09 Logit Scale: 100.000 Contrastive_loss: 0.21795 (0.21039) Loss: 0.21795 (0.21039)
2024-08-30,03:22:10 | INFO | Train Epoch: 49 [260100/145000.0 (179%)] Data (t): 0.107 Batch (t): 0.192, 503.922/s, 503.922/s/gpu LR: 5.141406e-09 Logit Scale: 100.000 Contrastive_loss: 0.22024 (0.21075) Loss: 0.22024 (0.21075)
2024-08-30,03:22:29 | INFO | Train Epoch: 49 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 497.523/s, 497.523/s/gpu LR: 5.130938e-09 Logit Scale: 100.000 Contrastive_loss: 0.25786 (0.21237) Loss: 0.25786 (0.21237)
2024-08-30,03:22:48 | INFO | Train Epoch: 49 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 499.970/s, 499.970/s/gpu LR: 5.120469e-09 Logit Scale: 100.000 Contrastive_loss: 0.17109 (0.21099) Loss: 0.17109 (0.21099)
2024-08-30,03:23:07 | INFO | Train Epoch: 49 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 508.195/s, 508.195/s/gpu LR: 5.110000e-09 Logit Scale: 100.000 Contrastive_loss: 0.14591 (0.20889) Loss: 0.14591 (0.20889)
2024-08-30,03:23:27 | INFO | Train Epoch: 49 [300100/145000.0 (207%)] Data (t): 0.107 Batch (t): 0.192, 534.726/s, 534.726/s/gpu LR: 5.099530e-09 Logit Scale: 100.000 Contrastive_loss: 0.26506 (0.21065) Loss: 0.26506 (0.21065)
2024-08-30,03:23:46 | INFO | Train Epoch: 49 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 498.808/s, 498.808/s/gpu LR: 5.089060e-09 Logit Scale: 100.000 Contrastive_loss: 0.24524 (0.21170) Loss: 0.24524 (0.21170)
2024-08-30,03:24:05 | INFO | Train Epoch: 49 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.192, 500.953/s, 500.953/s/gpu LR: 5.078589e-09 Logit Scale: 100.000 Contrastive_loss: 0.28435 (0.21383) Loss: 0.28435 (0.21383)
2024-08-30,03:24:24 | INFO | Train Epoch: 49 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 497.792/s, 497.792/s/gpu LR: 5.068119e-09 Logit Scale: 100.000 Contrastive_loss: 0.18699 (0.21307) Loss: 0.18699 (0.21307)
2024-08-30,03:24:44 | INFO | Train Epoch: 49 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.193, 534.444/s, 534.444/s/gpu LR: 5.057647e-09 Logit Scale: 100.000 Contrastive_loss: 0.25275 (0.21417) Loss: 0.25275 (0.21417)
2024-08-30,03:25:03 | INFO | Train Epoch: 49 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 529.250/s, 529.250/s/gpu LR: 5.047176e-09 Logit Scale: 100.000 Contrastive_loss: 0.30215 (0.21655) Loss: 0.30215 (0.21655)
2024-08-30,03:25:22 | INFO | Train Epoch: 49 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 529.663/s, 529.663/s/gpu LR: 5.036704e-09 Logit Scale: 100.000 Contrastive_loss: 0.15701 (0.21498) Loss: 0.15701 (0.21498)
2024-08-30,03:25:42 | INFO | Train Epoch: 49 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 499.333/s, 499.333/s/gpu LR: 5.026232e-09 Logit Scale: 100.000 Contrastive_loss: 0.26081 (0.21616) Loss: 0.26081 (0.21616)
2024-08-30,03:26:01 | INFO | Train Epoch: 49 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 532.236/s, 532.236/s/gpu LR: 5.015760e-09 Logit Scale: 100.000 Contrastive_loss: 0.23727 (0.21668) Loss: 0.23727 (0.21668)
2024-08-30,03:26:20 | INFO | Train Epoch: 49 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.193, 497.696/s, 497.696/s/gpu LR: 5.005288e-09 Logit Scale: 100.000 Contrastive_loss: 0.13386 (0.21466) Loss: 0.13386 (0.21466)
2024-08-30,03:26:39 | INFO | Train Epoch: 49 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 518.377/s, 518.377/s/gpu LR: 4.994816e-09 Logit Scale: 100.000 Contrastive_loss: 0.22580 (0.21493) Loss: 0.22580 (0.21493)
2024-08-30,03:26:59 | INFO | Train Epoch: 49 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 497.524/s, 497.524/s/gpu LR: 4.984344e-09 Logit Scale: 100.000 Contrastive_loss: 0.090091 (0.21203) Loss: 0.090091 (0.21203)
2024-08-30,03:27:06 | INFO | Eval Epoch: 50 [200 / 1000]	Clip Loss: 0.482050	
2024-08-30,03:27:07 | INFO | Eval Epoch: 50 image_to_text_mean_rank: 2.4270	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6660	image_to_text_R@5: 0.9100	image_to_text_R@10: 0.9620	text_to_image_mean_rank: 3.8350	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6700	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9670	clip_val_loss: 0.4393	epoch: 50.0000	num_samples: 1000.0000
2024-08-30,03:27:08 | INFO | Start epoch 50
2024-08-30,03:27:08 | INFO | Train Epoch: 50 [   100/145000.0 (0%)] Data (t): 0.019 Batch (t): 0.107, 938.653/s, 938.653/s/gpu LR: 5.261734e-09 Logit Scale: 100.000 Contrastive_loss: 0.17129 (0.17129) Loss: 0.17129 (0.17129)
2024-08-30,03:27:28 | INFO | Train Epoch: 50 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 529.323/s, 529.323/s/gpu LR: 5.251276e-09 Logit Scale: 100.000 Contrastive_loss: 0.33977 (0.25553) Loss: 0.33977 (0.25553)
2024-08-30,03:27:47 | INFO | Train Epoch: 50 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.193, 490.404/s, 490.404/s/gpu LR: 5.240816e-09 Logit Scale: 100.000 Contrastive_loss: 0.35277 (0.28794) Loss: 0.35277 (0.28794)
2024-08-30,03:28:06 | INFO | Train Epoch: 50 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 495.782/s, 495.782/s/gpu LR: 5.230356e-09 Logit Scale: 100.000 Contrastive_loss: 0.18436 (0.26205) Loss: 0.18436 (0.26205)
2024-08-30,03:28:26 | INFO | Train Epoch: 50 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 534.902/s, 534.902/s/gpu LR: 5.219894e-09 Logit Scale: 100.000 Contrastive_loss: 0.13480 (0.23660) Loss: 0.13480 (0.23660)
2024-08-30,03:28:45 | INFO | Train Epoch: 50 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.194, 496.471/s, 496.471/s/gpu LR: 5.209432e-09 Logit Scale: 100.000 Contrastive_loss: 0.16186 (0.22414) Loss: 0.16186 (0.22414)
2024-08-30,03:29:04 | INFO | Train Epoch: 50 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.192, 505.170/s, 505.170/s/gpu LR: 5.198969e-09 Logit Scale: 100.000 Contrastive_loss: 0.11818 (0.20901) Loss: 0.11818 (0.20901)
2024-08-30,03:29:24 | INFO | Train Epoch: 50 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 528.910/s, 528.910/s/gpu LR: 5.188504e-09 Logit Scale: 100.000 Contrastive_loss: 0.22294 (0.21075) Loss: 0.22294 (0.21075)
2024-08-30,03:29:43 | INFO | Train Epoch: 50 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 527.474/s, 527.474/s/gpu LR: 5.178039e-09 Logit Scale: 100.000 Contrastive_loss: 0.13271 (0.20208) Loss: 0.13271 (0.20208)
2024-08-30,03:30:02 | INFO | Train Epoch: 50 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 512.276/s, 512.276/s/gpu LR: 5.167574e-09 Logit Scale: 100.000 Contrastive_loss: 0.21512 (0.20338) Loss: 0.21512 (0.20338)
2024-08-30,03:30:22 | INFO | Train Epoch: 50 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 496.217/s, 496.217/s/gpu LR: 5.157107e-09 Logit Scale: 100.000 Contrastive_loss: 0.15360 (0.19886) Loss: 0.15360 (0.19886)
2024-08-30,03:30:41 | INFO | Train Epoch: 50 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.193, 519.872/s, 519.872/s/gpu LR: 5.146640e-09 Logit Scale: 100.000 Contrastive_loss: 0.17291 (0.19669) Loss: 0.17291 (0.19669)
2024-08-30,03:31:00 | INFO | Train Epoch: 50 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 499.803/s, 499.803/s/gpu LR: 5.136172e-09 Logit Scale: 100.000 Contrastive_loss: 0.27565 (0.20277) Loss: 0.27565 (0.20277)
2024-08-30,03:31:19 | INFO | Train Epoch: 50 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 501.206/s, 501.206/s/gpu LR: 5.125704e-09 Logit Scale: 100.000 Contrastive_loss: 0.15371 (0.19926) Loss: 0.15371 (0.19926)
2024-08-30,03:31:39 | INFO | Train Epoch: 50 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 497.995/s, 497.995/s/gpu LR: 5.115235e-09 Logit Scale: 100.000 Contrastive_loss: 0.17944 (0.19794) Loss: 0.17944 (0.19794)
2024-08-30,03:31:48 | INFO | Train Epoch: 50 [145000/145000.0 (100%)] Data (t): 0.110 Batch (t): 0.195, 526.684/s, 526.684/s/gpu LR: 5.110105e-09 Logit Scale: 100.000 Contrastive_loss: 0.22520 (0.19964) Loss: 0.22520 (0.19964)
2024-08-30,03:31:58 | INFO | Train Epoch: 50 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 528.771/s, 528.771/s/gpu LR: 5.104765e-09 Logit Scale: 100.000 Contrastive_loss: 0.19927 (0.19962) Loss: 0.19927 (0.19962)
2024-08-30,03:32:17 | INFO | Train Epoch: 50 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 523.547/s, 523.547/s/gpu LR: 5.094295e-09 Logit Scale: 100.000 Contrastive_loss: 0.23145 (0.20139) Loss: 0.23145 (0.20139)
2024-08-30,03:32:37 | INFO | Train Epoch: 50 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 510.352/s, 510.352/s/gpu LR: 5.083825e-09 Logit Scale: 100.000 Contrastive_loss: 0.17486 (0.19999) Loss: 0.17486 (0.19999)
2024-08-30,03:32:56 | INFO | Train Epoch: 50 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 519.726/s, 519.726/s/gpu LR: 5.073354e-09 Logit Scale: 100.000 Contrastive_loss: 0.19865 (0.19993) Loss: 0.19865 (0.19993)
2024-08-30,03:33:15 | INFO | Train Epoch: 50 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 530.635/s, 530.635/s/gpu LR: 5.062883e-09 Logit Scale: 100.000 Contrastive_loss: 0.24561 (0.20210) Loss: 0.24561 (0.20210)
2024-08-30,03:33:34 | INFO | Train Epoch: 50 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.192, 513.138/s, 513.138/s/gpu LR: 5.052412e-09 Logit Scale: 100.000 Contrastive_loss: 0.16613 (0.20047) Loss: 0.16613 (0.20047)
2024-08-30,03:33:54 | INFO | Train Epoch: 50 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 528.993/s, 528.993/s/gpu LR: 5.041940e-09 Logit Scale: 100.000 Contrastive_loss: 0.25186 (0.20270) Loss: 0.25186 (0.20270)
2024-08-30,03:34:13 | INFO | Train Epoch: 50 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.192, 527.107/s, 527.107/s/gpu LR: 5.031468e-09 Logit Scale: 100.000 Contrastive_loss: 0.31073 (0.20720) Loss: 0.31073 (0.20720)
2024-08-30,03:34:32 | INFO | Train Epoch: 50 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 497.985/s, 497.985/s/gpu LR: 5.020996e-09 Logit Scale: 100.000 Contrastive_loss: 0.30126 (0.21097) Loss: 0.30126 (0.21097)
2024-08-30,03:34:51 | INFO | Train Epoch: 50 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 503.923/s, 503.923/s/gpu LR: 5.010524e-09 Logit Scale: 100.000 Contrastive_loss: 0.16598 (0.20924) Loss: 0.16598 (0.20924)
2024-08-30,03:35:11 | INFO | Train Epoch: 50 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 505.311/s, 505.311/s/gpu LR: 5.000052e-09 Logit Scale: 100.000 Contrastive_loss: 0.21730 (0.20953) Loss: 0.21730 (0.20953)
2024-08-30,03:35:30 | INFO | Train Epoch: 50 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 492.119/s, 492.119/s/gpu LR: 4.989580e-09 Logit Scale: 100.000 Contrastive_loss: 0.21940 (0.20989) Loss: 0.21940 (0.20989)
2024-08-30,03:35:49 | INFO | Train Epoch: 50 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 529.964/s, 529.964/s/gpu LR: 4.979108e-09 Logit Scale: 100.000 Contrastive_loss: 0.25716 (0.21152) Loss: 0.25716 (0.21152)
2024-08-30,03:36:08 | INFO | Train Epoch: 50 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 532.568/s, 532.568/s/gpu LR: 4.968636e-09 Logit Scale: 100.000 Contrastive_loss: 0.17047 (0.21015) Loss: 0.17047 (0.21015)
2024-08-30,03:36:28 | INFO | Train Epoch: 50 [290100/145000.0 (200%)] Data (t): 0.107 Batch (t): 0.193, 494.759/s, 494.759/s/gpu LR: 4.958165e-09 Logit Scale: 100.000 Contrastive_loss: 0.14558 (0.20807) Loss: 0.14558 (0.20807)
2024-08-30,03:36:47 | INFO | Train Epoch: 50 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 527.921/s, 527.921/s/gpu LR: 4.947693e-09 Logit Scale: 100.000 Contrastive_loss: 0.26402 (0.20981) Loss: 0.26402 (0.20981)
2024-08-30,03:37:06 | INFO | Train Epoch: 50 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 501.728/s, 501.728/s/gpu LR: 4.937222e-09 Logit Scale: 100.000 Contrastive_loss: 0.24464 (0.21087) Loss: 0.24464 (0.21087)
2024-08-30,03:37:25 | INFO | Train Epoch: 50 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 533.301/s, 533.301/s/gpu LR: 4.926751e-09 Logit Scale: 100.000 Contrastive_loss: 0.28317 (0.21300) Loss: 0.28317 (0.21300)
2024-08-30,03:37:45 | INFO | Train Epoch: 50 [330100/145000.0 (228%)] Data (t): 0.107 Batch (t): 0.193, 534.455/s, 534.455/s/gpu LR: 4.916280e-09 Logit Scale: 100.000 Contrastive_loss: 0.18657 (0.21224) Loss: 0.18657 (0.21224)
2024-08-30,03:38:04 | INFO | Train Epoch: 50 [340100/145000.0 (235%)] Data (t): 0.107 Batch (t): 0.192, 503.151/s, 503.151/s/gpu LR: 4.905810e-09 Logit Scale: 100.000 Contrastive_loss: 0.25149 (0.21333) Loss: 0.25149 (0.21333)
2024-08-30,03:38:23 | INFO | Train Epoch: 50 [350100/145000.0 (241%)] Data (t): 0.107 Batch (t): 0.192, 521.472/s, 521.472/s/gpu LR: 4.895340e-09 Logit Scale: 100.000 Contrastive_loss: 0.30125 (0.21571) Loss: 0.30125 (0.21571)
2024-08-30,03:38:42 | INFO | Train Epoch: 50 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 529.977/s, 529.977/s/gpu LR: 4.884870e-09 Logit Scale: 100.000 Contrastive_loss: 0.15627 (0.21414) Loss: 0.15627 (0.21414)
2024-08-30,03:39:02 | INFO | Train Epoch: 50 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.192, 532.309/s, 532.309/s/gpu LR: 4.874401e-09 Logit Scale: 100.000 Contrastive_loss: 0.26005 (0.21532) Loss: 0.26005 (0.21532)
2024-08-30,03:39:21 | INFO | Train Epoch: 50 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 498.925/s, 498.925/s/gpu LR: 4.863933e-09 Logit Scale: 100.000 Contrastive_loss: 0.23646 (0.21585) Loss: 0.23646 (0.21585)
2024-08-30,03:39:40 | INFO | Train Epoch: 50 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 498.058/s, 498.058/s/gpu LR: 4.853465e-09 Logit Scale: 100.000 Contrastive_loss: 0.13312 (0.21383) Loss: 0.13312 (0.21383)
2024-08-30,03:39:59 | INFO | Train Epoch: 50 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.192, 496.846/s, 496.846/s/gpu LR: 4.842997e-09 Logit Scale: 100.000 Contrastive_loss: 0.22491 (0.21409) Loss: 0.22491 (0.21409)
2024-08-30,03:40:19 | INFO | Train Epoch: 50 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.192, 533.255/s, 533.255/s/gpu LR: 4.832531e-09 Logit Scale: 100.000 Contrastive_loss: 0.089585 (0.21120) Loss: 0.089585 (0.21120)
2024-08-30,03:40:26 | INFO | Eval Epoch: 51 [200 / 1000]	Clip Loss: 0.482089	
2024-08-30,03:40:27 | INFO | Eval Epoch: 51 image_to_text_mean_rank: 2.4230	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6660	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.8370	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9080	text_to_image_R@10: 0.9670	clip_val_loss: 0.4393	epoch: 51.0000	num_samples: 1000.0000
2024-08-30,03:40:28 | INFO | Start epoch 51
2024-08-30,03:40:28 | INFO | Train Epoch: 51 [   100/145000.0 (0%)] Data (t): 0.019 Batch (t): 0.110, 910.400/s, 910.400/s/gpu LR: 5.110000e-09 Logit Scale: 100.000 Contrastive_loss: 0.17061 (0.17061) Loss: 0.17061 (0.17061)
2024-08-30,03:40:48 | INFO | Train Epoch: 51 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 498.074/s, 498.074/s/gpu LR: 5.099530e-09 Logit Scale: 100.000 Contrastive_loss: 0.33879 (0.25470) Loss: 0.33879 (0.25470)
2024-08-30,03:41:07 | INFO | Train Epoch: 51 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.194, 527.182/s, 527.182/s/gpu LR: 5.089060e-09 Logit Scale: 100.000 Contrastive_loss: 0.35138 (0.28692) Loss: 0.35138 (0.28692)
2024-08-30,03:41:26 | INFO | Train Epoch: 51 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 518.576/s, 518.576/s/gpu LR: 5.078589e-09 Logit Scale: 100.000 Contrastive_loss: 0.18347 (0.26106) Loss: 0.18347 (0.26106)
2024-08-30,03:41:46 | INFO | Train Epoch: 51 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 494.978/s, 494.978/s/gpu LR: 5.068119e-09 Logit Scale: 100.000 Contrastive_loss: 0.13433 (0.23571) Loss: 0.13433 (0.23571)
2024-08-30,03:42:05 | INFO | Train Epoch: 51 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 526.477/s, 526.477/s/gpu LR: 5.057647e-09 Logit Scale: 100.000 Contrastive_loss: 0.16111 (0.22328) Loss: 0.16111 (0.22328)
2024-08-30,03:42:24 | INFO | Train Epoch: 51 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 526.020/s, 526.020/s/gpu LR: 5.047176e-09 Logit Scale: 100.000 Contrastive_loss: 0.11772 (0.20820) Loss: 0.11772 (0.20820)
2024-08-30,03:42:43 | INFO | Train Epoch: 51 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 527.131/s, 527.131/s/gpu LR: 5.036704e-09 Logit Scale: 100.000 Contrastive_loss: 0.22171 (0.20989) Loss: 0.22171 (0.20989)
2024-08-30,03:43:03 | INFO | Train Epoch: 51 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.194, 501.833/s, 501.833/s/gpu LR: 5.026232e-09 Logit Scale: 100.000 Contrastive_loss: 0.13200 (0.20123) Loss: 0.13200 (0.20123)
2024-08-30,03:43:22 | INFO | Train Epoch: 51 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 507.584/s, 507.584/s/gpu LR: 5.015760e-09 Logit Scale: 100.000 Contrastive_loss: 0.21420 (0.20253) Loss: 0.21420 (0.20253)
2024-08-30,03:43:41 | INFO | Train Epoch: 51 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 511.879/s, 511.879/s/gpu LR: 5.005288e-09 Logit Scale: 100.000 Contrastive_loss: 0.15297 (0.19803) Loss: 0.15297 (0.19803)
2024-08-30,03:44:01 | INFO | Train Epoch: 51 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 500.762/s, 500.762/s/gpu LR: 4.994816e-09 Logit Scale: 100.000 Contrastive_loss: 0.17204 (0.19586) Loss: 0.17204 (0.19586)
2024-08-30,03:44:20 | INFO | Train Epoch: 51 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 491.869/s, 491.869/s/gpu LR: 4.984344e-09 Logit Scale: 100.000 Contrastive_loss: 0.27486 (0.20194) Loss: 0.27486 (0.20194)
2024-08-30,03:44:39 | INFO | Train Epoch: 51 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 522.156/s, 522.156/s/gpu LR: 4.973872e-09 Logit Scale: 100.000 Contrastive_loss: 0.15310 (0.19845) Loss: 0.15310 (0.19845)
2024-08-30,03:44:59 | INFO | Train Epoch: 51 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 525.553/s, 525.553/s/gpu LR: 4.963401e-09 Logit Scale: 100.000 Contrastive_loss: 0.17903 (0.19715) Loss: 0.17903 (0.19715)
2024-08-30,03:45:08 | INFO | Train Epoch: 51 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 538.572/s, 538.572/s/gpu LR: 4.958269e-09 Logit Scale: 100.000 Contrastive_loss: 0.22397 (0.19883) Loss: 0.22397 (0.19883)
2024-08-30,03:45:18 | INFO | Train Epoch: 51 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.194, 499.458/s, 499.458/s/gpu LR: 4.952929e-09 Logit Scale: 100.000 Contrastive_loss: 0.19839 (0.19880) Loss: 0.19839 (0.19880)
2024-08-30,03:45:37 | INFO | Train Epoch: 51 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 499.162/s, 499.162/s/gpu LR: 4.942457e-09 Logit Scale: 100.000 Contrastive_loss: 0.23032 (0.20056) Loss: 0.23032 (0.20056)
2024-08-30,03:45:56 | INFO | Train Epoch: 51 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 498.449/s, 498.449/s/gpu LR: 4.931986e-09 Logit Scale: 100.000 Contrastive_loss: 0.17410 (0.19916) Loss: 0.17410 (0.19916)
2024-08-30,03:46:16 | INFO | Train Epoch: 51 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 533.118/s, 533.118/s/gpu LR: 4.921515e-09 Logit Scale: 100.000 Contrastive_loss: 0.19814 (0.19911) Loss: 0.19814 (0.19911)
2024-08-30,03:46:35 | INFO | Train Epoch: 51 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 501.734/s, 501.734/s/gpu LR: 4.911045e-09 Logit Scale: 100.000 Contrastive_loss: 0.24459 (0.20128) Loss: 0.24459 (0.20128)
2024-08-30,03:46:54 | INFO | Train Epoch: 51 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 502.373/s, 502.373/s/gpu LR: 4.900574e-09 Logit Scale: 100.000 Contrastive_loss: 0.16562 (0.19966) Loss: 0.16562 (0.19966)
2024-08-30,03:47:13 | INFO | Train Epoch: 51 [210100/145000.0 (145%)] Data (t): 0.107 Batch (t): 0.192, 501.050/s, 501.050/s/gpu LR: 4.890105e-09 Logit Scale: 100.000 Contrastive_loss: 0.25116 (0.20190) Loss: 0.25116 (0.20190)
2024-08-30,03:47:33 | INFO | Train Epoch: 51 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.192, 505.734/s, 505.734/s/gpu LR: 4.879635e-09 Logit Scale: 100.000 Contrastive_loss: 0.30961 (0.20638) Loss: 0.30961 (0.20638)
2024-08-30,03:47:52 | INFO | Train Epoch: 51 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 511.925/s, 511.925/s/gpu LR: 4.869167e-09 Logit Scale: 100.000 Contrastive_loss: 0.30046 (0.21015) Loss: 0.30046 (0.21015)
2024-08-30,03:48:11 | INFO | Train Epoch: 51 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 529.112/s, 529.112/s/gpu LR: 4.858699e-09 Logit Scale: 100.000 Contrastive_loss: 0.16502 (0.20841) Loss: 0.16502 (0.20841)
2024-08-30,03:48:30 | INFO | Train Epoch: 51 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 518.780/s, 518.780/s/gpu LR: 4.848231e-09 Logit Scale: 100.000 Contrastive_loss: 0.21675 (0.20872) Loss: 0.21675 (0.20872)
2024-08-30,03:48:50 | INFO | Train Epoch: 51 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 506.260/s, 506.260/s/gpu LR: 4.837764e-09 Logit Scale: 100.000 Contrastive_loss: 0.21870 (0.20908) Loss: 0.21870 (0.20908)
2024-08-30,03:49:09 | INFO | Train Epoch: 51 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 523.796/s, 523.796/s/gpu LR: 4.827298e-09 Logit Scale: 100.000 Contrastive_loss: 0.25651 (0.21071) Loss: 0.25651 (0.21071)
2024-08-30,03:49:28 | INFO | Train Epoch: 51 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 495.588/s, 495.588/s/gpu LR: 4.816833e-09 Logit Scale: 100.000 Contrastive_loss: 0.16989 (0.20935) Loss: 0.16989 (0.20935)
2024-08-30,03:49:48 | INFO | Train Epoch: 51 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 533.777/s, 533.777/s/gpu LR: 4.806368e-09 Logit Scale: 100.000 Contrastive_loss: 0.14526 (0.20728) Loss: 0.14526 (0.20728)
2024-08-30,03:50:07 | INFO | Train Epoch: 51 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 527.471/s, 527.471/s/gpu LR: 4.795904e-09 Logit Scale: 100.000 Contrastive_loss: 0.26311 (0.20903) Loss: 0.26311 (0.20903)
2024-08-30,03:50:26 | INFO | Train Epoch: 51 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 536.635/s, 536.635/s/gpu LR: 4.785441e-09 Logit Scale: 100.000 Contrastive_loss: 0.24419 (0.21009) Loss: 0.24419 (0.21009)
2024-08-30,03:50:45 | INFO | Train Epoch: 51 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 498.480/s, 498.480/s/gpu LR: 4.774979e-09 Logit Scale: 100.000 Contrastive_loss: 0.28214 (0.21221) Loss: 0.28214 (0.21221)
2024-08-30,03:51:05 | INFO | Train Epoch: 51 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 561.890/s, 561.890/s/gpu LR: 4.764518e-09 Logit Scale: 100.000 Contrastive_loss: 0.18616 (0.21147) Loss: 0.18616 (0.21147)
2024-08-30,03:51:24 | INFO | Train Epoch: 51 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 536.937/s, 536.937/s/gpu LR: 4.754059e-09 Logit Scale: 100.000 Contrastive_loss: 0.25005 (0.21254) Loss: 0.25005 (0.21254)
2024-08-30,03:51:43 | INFO | Train Epoch: 51 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 509.017/s, 509.017/s/gpu LR: 4.743600e-09 Logit Scale: 100.000 Contrastive_loss: 0.30027 (0.21491) Loss: 0.30027 (0.21491)
2024-08-30,03:52:03 | INFO | Train Epoch: 51 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 528.678/s, 528.678/s/gpu LR: 4.733142e-09 Logit Scale: 100.000 Contrastive_loss: 0.15564 (0.21335) Loss: 0.15564 (0.21335)
2024-08-30,03:52:22 | INFO | Train Epoch: 51 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 541.511/s, 541.511/s/gpu LR: 4.722686e-09 Logit Scale: 100.000 Contrastive_loss: 0.25917 (0.21453) Loss: 0.25917 (0.21453)
2024-08-30,03:52:41 | INFO | Train Epoch: 51 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 528.483/s, 528.483/s/gpu LR: 4.712230e-09 Logit Scale: 100.000 Contrastive_loss: 0.23567 (0.21505) Loss: 0.23567 (0.21505)
2024-08-30,03:53:00 | INFO | Train Epoch: 51 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 518.693/s, 518.693/s/gpu LR: 4.701776e-09 Logit Scale: 100.000 Contrastive_loss: 0.13237 (0.21304) Loss: 0.13237 (0.21304)
2024-08-30,03:53:20 | INFO | Train Epoch: 51 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 522.561/s, 522.561/s/gpu LR: 4.691323e-09 Logit Scale: 100.000 Contrastive_loss: 0.22405 (0.21330) Loss: 0.22405 (0.21330)
2024-08-30,03:53:39 | INFO | Train Epoch: 51 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 535.685/s, 535.685/s/gpu LR: 4.680872e-09 Logit Scale: 100.000 Contrastive_loss: 0.089162 (0.21041) Loss: 0.089162 (0.21041)
2024-08-30,03:53:46 | INFO | Eval Epoch: 52 [200 / 1000]	Clip Loss: 0.482161	
2024-08-30,03:53:47 | INFO | Eval Epoch: 52 image_to_text_mean_rank: 2.4220	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6670	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.8480	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6690	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4394	epoch: 52.0000	num_samples: 1000.0000
2024-08-30,03:53:48 | INFO | Start epoch 52
2024-08-30,03:53:49 | INFO | Train Epoch: 52 [   100/145000.0 (0%)] Data (t): 0.019 Batch (t): 0.107, 934.576/s, 934.576/s/gpu LR: 4.958165e-09 Logit Scale: 100.000 Contrastive_loss: 0.16993 (0.16993) Loss: 0.16993 (0.16993)
2024-08-30,03:54:08 | INFO | Train Epoch: 52 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 498.821/s, 498.821/s/gpu LR: 4.947693e-09 Logit Scale: 100.000 Contrastive_loss: 0.33807 (0.25400) Loss: 0.33807 (0.25400)
2024-08-30,03:54:27 | INFO | Train Epoch: 52 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 533.406/s, 533.406/s/gpu LR: 4.937222e-09 Logit Scale: 100.000 Contrastive_loss: 0.35022 (0.28608) Loss: 0.35022 (0.28608)
2024-08-30,03:54:47 | INFO | Train Epoch: 52 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 500.661/s, 500.661/s/gpu LR: 4.926751e-09 Logit Scale: 100.000 Contrastive_loss: 0.18253 (0.26019) Loss: 0.18253 (0.26019)
2024-08-30,03:55:06 | INFO | Train Epoch: 52 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 500.080/s, 500.080/s/gpu LR: 4.916280e-09 Logit Scale: 100.000 Contrastive_loss: 0.13388 (0.23493) Loss: 0.13388 (0.23493)
2024-08-30,03:55:25 | INFO | Train Epoch: 52 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 521.322/s, 521.322/s/gpu LR: 4.905810e-09 Logit Scale: 100.000 Contrastive_loss: 0.16028 (0.22249) Loss: 0.16028 (0.22249)
2024-08-30,03:55:45 | INFO | Train Epoch: 52 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 507.523/s, 507.523/s/gpu LR: 4.895340e-09 Logit Scale: 100.000 Contrastive_loss: 0.11734 (0.20746) Loss: 0.11734 (0.20746)
2024-08-30,03:56:04 | INFO | Train Epoch: 52 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.192, 531.677/s, 531.677/s/gpu LR: 4.884870e-09 Logit Scale: 100.000 Contrastive_loss: 0.22052 (0.20910) Loss: 0.22052 (0.20910)
2024-08-30,03:56:23 | INFO | Train Epoch: 52 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.192, 530.063/s, 530.063/s/gpu LR: 4.874401e-09 Logit Scale: 100.000 Contrastive_loss: 0.13127 (0.20045) Loss: 0.13127 (0.20045)
2024-08-30,03:56:42 | INFO | Train Epoch: 52 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.192, 537.673/s, 537.673/s/gpu LR: 4.863933e-09 Logit Scale: 100.000 Contrastive_loss: 0.21335 (0.20174) Loss: 0.21335 (0.20174)
2024-08-30,03:57:01 | INFO | Train Epoch: 52 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.192, 538.154/s, 538.154/s/gpu LR: 4.853465e-09 Logit Scale: 100.000 Contrastive_loss: 0.15243 (0.19726) Loss: 0.15243 (0.19726)
2024-08-30,03:57:21 | INFO | Train Epoch: 52 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 523.652/s, 523.652/s/gpu LR: 4.842997e-09 Logit Scale: 100.000 Contrastive_loss: 0.17125 (0.19509) Loss: 0.17125 (0.19509)
2024-08-30,03:57:40 | INFO | Train Epoch: 52 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 504.887/s, 504.887/s/gpu LR: 4.832531e-09 Logit Scale: 100.000 Contrastive_loss: 0.27393 (0.20115) Loss: 0.27393 (0.20115)
2024-08-30,03:57:59 | INFO | Train Epoch: 52 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 496.228/s, 496.228/s/gpu LR: 4.822065e-09 Logit Scale: 100.000 Contrastive_loss: 0.15268 (0.19769) Loss: 0.15268 (0.19769)
2024-08-30,03:58:19 | INFO | Train Epoch: 52 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 500.877/s, 500.877/s/gpu LR: 4.811600e-09 Logit Scale: 100.000 Contrastive_loss: 0.17852 (0.19641) Loss: 0.17852 (0.19641)
2024-08-30,03:58:28 | INFO | Train Epoch: 52 [145000/145000.0 (100%)] Data (t): 0.110 Batch (t): 0.193, 542.901/s, 542.901/s/gpu LR: 4.806473e-09 Logit Scale: 100.000 Contrastive_loss: 0.22280 (0.19806) Loss: 0.22280 (0.19806)
2024-08-30,03:58:38 | INFO | Train Epoch: 52 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 535.160/s, 535.160/s/gpu LR: 4.801136e-09 Logit Scale: 100.000 Contrastive_loss: 0.19738 (0.19802) Loss: 0.19738 (0.19802)
2024-08-30,03:58:57 | INFO | Train Epoch: 52 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 515.417/s, 515.417/s/gpu LR: 4.790673e-09 Logit Scale: 100.000 Contrastive_loss: 0.22920 (0.19975) Loss: 0.22920 (0.19975)
2024-08-30,03:59:16 | INFO | Train Epoch: 52 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 495.752/s, 495.752/s/gpu LR: 4.780210e-09 Logit Scale: 100.000 Contrastive_loss: 0.17339 (0.19837) Loss: 0.17339 (0.19837)
2024-08-30,03:59:36 | INFO | Train Epoch: 52 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 521.713/s, 521.713/s/gpu LR: 4.769749e-09 Logit Scale: 100.000 Contrastive_loss: 0.19777 (0.19834) Loss: 0.19777 (0.19834)
2024-08-30,03:59:55 | INFO | Train Epoch: 52 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.194, 500.052/s, 500.052/s/gpu LR: 4.759288e-09 Logit Scale: 100.000 Contrastive_loss: 0.24393 (0.20051) Loss: 0.24393 (0.20051)
2024-08-30,04:00:14 | INFO | Train Epoch: 52 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 531.670/s, 531.670/s/gpu LR: 4.748829e-09 Logit Scale: 100.000 Contrastive_loss: 0.16509 (0.19890) Loss: 0.16509 (0.19890)
2024-08-30,04:00:34 | INFO | Train Epoch: 52 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 520.336/s, 520.336/s/gpu LR: 4.738371e-09 Logit Scale: 100.000 Contrastive_loss: 0.25046 (0.20114) Loss: 0.25046 (0.20114)
2024-08-30,04:00:53 | INFO | Train Epoch: 52 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.194, 496.536/s, 496.536/s/gpu LR: 4.727914e-09 Logit Scale: 100.000 Contrastive_loss: 0.30847 (0.20561) Loss: 0.30847 (0.20561)
2024-08-30,04:01:12 | INFO | Train Epoch: 52 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 527.862/s, 527.862/s/gpu LR: 4.717458e-09 Logit Scale: 100.000 Contrastive_loss: 0.29961 (0.20937) Loss: 0.29961 (0.20937)
2024-08-30,04:01:31 | INFO | Train Epoch: 52 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 532.152/s, 532.152/s/gpu LR: 4.707003e-09 Logit Scale: 100.000 Contrastive_loss: 0.16410 (0.20763) Loss: 0.16410 (0.20763)
2024-08-30,04:01:51 | INFO | Train Epoch: 52 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 519.005/s, 519.005/s/gpu LR: 4.696550e-09 Logit Scale: 100.000 Contrastive_loss: 0.21620 (0.20795) Loss: 0.21620 (0.20795)
2024-08-30,04:02:10 | INFO | Train Epoch: 52 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 535.458/s, 535.458/s/gpu LR: 4.686098e-09 Logit Scale: 100.000 Contrastive_loss: 0.21783 (0.20830) Loss: 0.21783 (0.20830)
2024-08-30,04:02:29 | INFO | Train Epoch: 52 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 499.119/s, 499.119/s/gpu LR: 4.675647e-09 Logit Scale: 100.000 Contrastive_loss: 0.25592 (0.20994) Loss: 0.25592 (0.20994)
2024-08-30,04:02:49 | INFO | Train Epoch: 52 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 532.187/s, 532.187/s/gpu LR: 4.665198e-09 Logit Scale: 100.000 Contrastive_loss: 0.16930 (0.20859) Loss: 0.16930 (0.20859)
2024-08-30,04:03:08 | INFO | Train Epoch: 52 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 524.793/s, 524.793/s/gpu LR: 4.654750e-09 Logit Scale: 100.000 Contrastive_loss: 0.14484 (0.20653) Loss: 0.14484 (0.20653)
2024-08-30,04:03:27 | INFO | Train Epoch: 52 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 494.032/s, 494.032/s/gpu LR: 4.644304e-09 Logit Scale: 100.000 Contrastive_loss: 0.26233 (0.20828) Loss: 0.26233 (0.20828)
2024-08-30,04:03:47 | INFO | Train Epoch: 52 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 533.455/s, 533.455/s/gpu LR: 4.633859e-09 Logit Scale: 100.000 Contrastive_loss: 0.24392 (0.20936) Loss: 0.24392 (0.20936)
2024-08-30,04:04:06 | INFO | Train Epoch: 52 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 499.053/s, 499.053/s/gpu LR: 4.623416e-09 Logit Scale: 100.000 Contrastive_loss: 0.28113 (0.21147) Loss: 0.28113 (0.21147)
2024-08-30,04:04:25 | INFO | Train Epoch: 52 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 536.294/s, 536.294/s/gpu LR: 4.612974e-09 Logit Scale: 100.000 Contrastive_loss: 0.18567 (0.21073) Loss: 0.18567 (0.21073)
2024-08-30,04:04:44 | INFO | Train Epoch: 52 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.193, 528.708/s, 528.708/s/gpu LR: 4.602534e-09 Logit Scale: 100.000 Contrastive_loss: 0.24880 (0.21179) Loss: 0.24880 (0.21179)
2024-08-30,04:05:04 | INFO | Train Epoch: 52 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.194, 491.887/s, 491.887/s/gpu LR: 4.592096e-09 Logit Scale: 100.000 Contrastive_loss: 0.29924 (0.21415) Loss: 0.29924 (0.21415)
2024-08-30,04:05:23 | INFO | Train Epoch: 52 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.194, 540.335/s, 540.335/s/gpu LR: 4.581660e-09 Logit Scale: 100.000 Contrastive_loss: 0.15500 (0.21259) Loss: 0.15500 (0.21259)
2024-08-30,04:05:42 | INFO | Train Epoch: 52 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.192, 498.834/s, 498.834/s/gpu LR: 4.571226e-09 Logit Scale: 100.000 Contrastive_loss: 0.25838 (0.21377) Loss: 0.25838 (0.21377)
2024-08-30,04:06:02 | INFO | Train Epoch: 52 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 532.082/s, 532.082/s/gpu LR: 4.560793e-09 Logit Scale: 100.000 Contrastive_loss: 0.23499 (0.21430) Loss: 0.23499 (0.21430)
2024-08-30,04:06:21 | INFO | Train Epoch: 52 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 534.750/s, 534.750/s/gpu LR: 4.550363e-09 Logit Scale: 100.000 Contrastive_loss: 0.13165 (0.21228) Loss: 0.13165 (0.21228)
2024-08-30,04:06:40 | INFO | Train Epoch: 52 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 500.940/s, 500.940/s/gpu LR: 4.539934e-09 Logit Scale: 100.000 Contrastive_loss: 0.22332 (0.21255) Loss: 0.22332 (0.21255)
2024-08-30,04:06:59 | INFO | Train Epoch: 52 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 498.982/s, 498.982/s/gpu LR: 4.529507e-09 Logit Scale: 100.000 Contrastive_loss: 0.088743 (0.20967) Loss: 0.088743 (0.20967)
2024-08-30,04:07:07 | INFO | Eval Epoch: 53 [200 / 1000]	Clip Loss: 0.482291	
2024-08-30,04:07:08 | INFO | Eval Epoch: 53 image_to_text_mean_rank: 2.4180	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6680	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.8590	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6690	text_to_image_R@5: 0.9080	text_to_image_R@10: 0.9660	clip_val_loss: 0.4394	epoch: 53.0000	num_samples: 1000.0000
2024-08-30,04:07:09 | INFO | Start epoch 53
2024-08-30,04:07:09 | INFO | Train Epoch: 53 [   100/145000.0 (0%)] Data (t): 0.024 Batch (t): 0.105, 956.542/s, 956.542/s/gpu LR: 4.806368e-09 Logit Scale: 100.000 Contrastive_loss: 0.16936 (0.16936) Loss: 0.16936 (0.16936)
2024-08-30,04:07:29 | INFO | Train Epoch: 53 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 527.249/s, 527.249/s/gpu LR: 4.795904e-09 Logit Scale: 100.000 Contrastive_loss: 0.33720 (0.25328) Loss: 0.33720 (0.25328)
2024-08-30,04:07:48 | INFO | Train Epoch: 53 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 532.376/s, 532.376/s/gpu LR: 4.785441e-09 Logit Scale: 100.000 Contrastive_loss: 0.34886 (0.28514) Loss: 0.34886 (0.28514)
2024-08-30,04:08:07 | INFO | Train Epoch: 53 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 504.995/s, 504.995/s/gpu LR: 4.774979e-09 Logit Scale: 100.000 Contrastive_loss: 0.18152 (0.25924) Loss: 0.18152 (0.25924)
2024-08-30,04:08:27 | INFO | Train Epoch: 53 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 491.105/s, 491.105/s/gpu LR: 4.764518e-09 Logit Scale: 100.000 Contrastive_loss: 0.13354 (0.23410) Loss: 0.13354 (0.23410)
2024-08-30,04:08:46 | INFO | Train Epoch: 53 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 529.444/s, 529.444/s/gpu LR: 4.754059e-09 Logit Scale: 100.000 Contrastive_loss: 0.15946 (0.22166) Loss: 0.15946 (0.22166)
2024-08-30,04:09:05 | INFO | Train Epoch: 53 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 528.545/s, 528.545/s/gpu LR: 4.743600e-09 Logit Scale: 100.000 Contrastive_loss: 0.11686 (0.20669) Loss: 0.11686 (0.20669)
2024-08-30,04:09:24 | INFO | Train Epoch: 53 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 521.739/s, 521.739/s/gpu LR: 4.733142e-09 Logit Scale: 100.000 Contrastive_loss: 0.21932 (0.20826) Loss: 0.21932 (0.20826)
2024-08-30,04:09:44 | INFO | Train Epoch: 53 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 490.654/s, 490.654/s/gpu LR: 4.722686e-09 Logit Scale: 100.000 Contrastive_loss: 0.13065 (0.19964) Loss: 0.13065 (0.19964)
2024-08-30,04:10:03 | INFO | Train Epoch: 53 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.192, 495.729/s, 495.729/s/gpu LR: 4.712230e-09 Logit Scale: 100.000 Contrastive_loss: 0.21270 (0.20095) Loss: 0.21270 (0.20095)
2024-08-30,04:10:22 | INFO | Train Epoch: 53 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.192, 534.410/s, 534.410/s/gpu LR: 4.701776e-09 Logit Scale: 100.000 Contrastive_loss: 0.15190 (0.19649) Loss: 0.15190 (0.19649)
2024-08-30,04:10:42 | INFO | Train Epoch: 53 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.193, 527.995/s, 527.995/s/gpu LR: 4.691323e-09 Logit Scale: 100.000 Contrastive_loss: 0.17066 (0.19434) Loss: 0.17066 (0.19434)
2024-08-30,04:11:01 | INFO | Train Epoch: 53 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 500.545/s, 500.545/s/gpu LR: 4.680872e-09 Logit Scale: 100.000 Contrastive_loss: 0.27318 (0.20040) Loss: 0.27318 (0.20040)
2024-08-30,04:11:20 | INFO | Train Epoch: 53 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 531.059/s, 531.059/s/gpu LR: 4.670422e-09 Logit Scale: 100.000 Contrastive_loss: 0.15207 (0.19695) Loss: 0.15207 (0.19695)
2024-08-30,04:11:39 | INFO | Train Epoch: 53 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 504.383/s, 504.383/s/gpu LR: 4.659974e-09 Logit Scale: 100.000 Contrastive_loss: 0.17822 (0.19570) Loss: 0.17822 (0.19570)
2024-08-30,04:11:49 | INFO | Train Epoch: 53 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 540.511/s, 540.511/s/gpu LR: 4.654854e-09 Logit Scale: 100.000 Contrastive_loss: 0.22173 (0.19733) Loss: 0.22173 (0.19733)
2024-08-30,04:11:59 | INFO | Train Epoch: 53 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 506.879/s, 506.879/s/gpu LR: 4.649526e-09 Logit Scale: 100.000 Contrastive_loss: 0.19660 (0.19728) Loss: 0.19660 (0.19728)
2024-08-30,04:12:18 | INFO | Train Epoch: 53 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.192, 499.531/s, 499.531/s/gpu LR: 4.639081e-09 Logit Scale: 100.000 Contrastive_loss: 0.22815 (0.19900) Loss: 0.22815 (0.19900)
2024-08-30,04:12:37 | INFO | Train Epoch: 53 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 497.359/s, 497.359/s/gpu LR: 4.628637e-09 Logit Scale: 100.000 Contrastive_loss: 0.17267 (0.19761) Loss: 0.17267 (0.19761)
2024-08-30,04:12:56 | INFO | Train Epoch: 53 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.192, 498.161/s, 498.161/s/gpu LR: 4.618195e-09 Logit Scale: 100.000 Contrastive_loss: 0.19724 (0.19759) Loss: 0.19724 (0.19759)
2024-08-30,04:13:16 | INFO | Train Epoch: 53 [190100/145000.0 (131%)] Data (t): 0.107 Batch (t): 0.192, 546.592/s, 546.592/s/gpu LR: 4.607754e-09 Logit Scale: 100.000 Contrastive_loss: 0.24314 (0.19976) Loss: 0.24314 (0.19976)
2024-08-30,04:13:35 | INFO | Train Epoch: 53 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.192, 499.575/s, 499.575/s/gpu LR: 4.597315e-09 Logit Scale: 100.000 Contrastive_loss: 0.16467 (0.19817) Loss: 0.16467 (0.19817)
2024-08-30,04:13:54 | INFO | Train Epoch: 53 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 533.394/s, 533.394/s/gpu LR: 4.586878e-09 Logit Scale: 100.000 Contrastive_loss: 0.24977 (0.20041) Loss: 0.24977 (0.20041)
2024-08-30,04:14:13 | INFO | Train Epoch: 53 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.192, 501.373/s, 501.373/s/gpu LR: 4.576443e-09 Logit Scale: 100.000 Contrastive_loss: 0.30721 (0.20486) Loss: 0.30721 (0.20486)
2024-08-30,04:14:33 | INFO | Train Epoch: 53 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 503.483/s, 503.483/s/gpu LR: 4.566009e-09 Logit Scale: 100.000 Contrastive_loss: 0.29884 (0.20862) Loss: 0.29884 (0.20862)
2024-08-30,04:14:52 | INFO | Train Epoch: 53 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 505.552/s, 505.552/s/gpu LR: 4.555578e-09 Logit Scale: 100.000 Contrastive_loss: 0.16324 (0.20688) Loss: 0.16324 (0.20688)
2024-08-30,04:15:11 | INFO | Train Epoch: 53 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 497.089/s, 497.089/s/gpu LR: 4.545148e-09 Logit Scale: 100.000 Contrastive_loss: 0.21555 (0.20720) Loss: 0.21555 (0.20720)
2024-08-30,04:15:30 | INFO | Train Epoch: 53 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 505.801/s, 505.801/s/gpu LR: 4.534720e-09 Logit Scale: 100.000 Contrastive_loss: 0.21712 (0.20755) Loss: 0.21712 (0.20755)
2024-08-30,04:15:50 | INFO | Train Epoch: 53 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 499.106/s, 499.106/s/gpu LR: 4.524295e-09 Logit Scale: 100.000 Contrastive_loss: 0.25527 (0.20920) Loss: 0.25527 (0.20920)
2024-08-30,04:16:09 | INFO | Train Epoch: 53 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.192, 524.718/s, 524.718/s/gpu LR: 4.513871e-09 Logit Scale: 100.000 Contrastive_loss: 0.16877 (0.20785) Loss: 0.16877 (0.20785)
2024-08-30,04:16:28 | INFO | Train Epoch: 53 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.192, 499.366/s, 499.366/s/gpu LR: 4.503450e-09 Logit Scale: 100.000 Contrastive_loss: 0.14460 (0.20581) Loss: 0.14460 (0.20581)
2024-08-30,04:16:47 | INFO | Train Epoch: 53 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 522.721/s, 522.721/s/gpu LR: 4.493031e-09 Logit Scale: 100.000 Contrastive_loss: 0.26138 (0.20755) Loss: 0.26138 (0.20755)
2024-08-30,04:17:07 | INFO | Train Epoch: 53 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 526.854/s, 526.854/s/gpu LR: 4.482614e-09 Logit Scale: 100.000 Contrastive_loss: 0.24322 (0.20863) Loss: 0.24322 (0.20863)
2024-08-30,04:17:26 | INFO | Train Epoch: 53 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.192, 497.977/s, 497.977/s/gpu LR: 4.472199e-09 Logit Scale: 100.000 Contrastive_loss: 0.28011 (0.21073) Loss: 0.28011 (0.21073)
2024-08-30,04:17:45 | INFO | Train Epoch: 53 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 537.414/s, 537.414/s/gpu LR: 4.461787e-09 Logit Scale: 100.000 Contrastive_loss: 0.18519 (0.21000) Loss: 0.18519 (0.21000)
2024-08-30,04:18:04 | INFO | Train Epoch: 53 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 516.335/s, 516.335/s/gpu LR: 4.451377e-09 Logit Scale: 100.000 Contrastive_loss: 0.24747 (0.21104) Loss: 0.24747 (0.21104)
2024-08-30,04:18:24 | INFO | Train Epoch: 53 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 541.050/s, 541.050/s/gpu LR: 4.440969e-09 Logit Scale: 100.000 Contrastive_loss: 0.29849 (0.21340) Loss: 0.29849 (0.21340)
2024-08-30,04:18:43 | INFO | Train Epoch: 53 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 496.349/s, 496.349/s/gpu LR: 4.430564e-09 Logit Scale: 100.000 Contrastive_loss: 0.15428 (0.21185) Loss: 0.15428 (0.21185)
2024-08-30,04:19:02 | INFO | Train Epoch: 53 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 529.382/s, 529.382/s/gpu LR: 4.420161e-09 Logit Scale: 100.000 Contrastive_loss: 0.25767 (0.21302) Loss: 0.25767 (0.21302)
2024-08-30,04:19:22 | INFO | Train Epoch: 53 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 524.778/s, 524.778/s/gpu LR: 4.409761e-09 Logit Scale: 100.000 Contrastive_loss: 0.23416 (0.21355) Loss: 0.23416 (0.21355)
2024-08-30,04:19:41 | INFO | Train Epoch: 53 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.193, 540.283/s, 540.283/s/gpu LR: 4.399364e-09 Logit Scale: 100.000 Contrastive_loss: 0.13095 (0.21154) Loss: 0.13095 (0.21154)
2024-08-30,04:20:00 | INFO | Train Epoch: 53 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.192, 512.169/s, 512.169/s/gpu LR: 4.388969e-09 Logit Scale: 100.000 Contrastive_loss: 0.22251 (0.21180) Loss: 0.22251 (0.21180)
2024-08-30,04:20:19 | INFO | Train Epoch: 53 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.192, 492.456/s, 492.456/s/gpu LR: 4.378577e-09 Logit Scale: 100.000 Contrastive_loss: 0.088368 (0.20893) Loss: 0.088368 (0.20893)
2024-08-30,04:20:27 | INFO | Eval Epoch: 54 [200 / 1000]	Clip Loss: 0.482241	
2024-08-30,04:20:28 | INFO | Eval Epoch: 54 image_to_text_mean_rank: 2.4170	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6680	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.8700	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6690	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4394	epoch: 54.0000	num_samples: 1000.0000
2024-08-30,04:20:29 | INFO | Start epoch 54
2024-08-30,04:20:29 | INFO | Train Epoch: 54 [   100/145000.0 (0%)] Data (t): 0.020 Batch (t): 0.098, 1016.90/s, 1016.90/s/gpu LR: 4.654750e-09 Logit Scale: 100.000 Contrastive_loss: 0.16868 (0.16868) Loss: 0.16868 (0.16868)
2024-08-30,04:20:48 | INFO | Train Epoch: 54 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 525.913/s, 525.913/s/gpu LR: 4.644304e-09 Logit Scale: 100.000 Contrastive_loss: 0.33644 (0.25256) Loss: 0.33644 (0.25256)
2024-08-30,04:21:08 | INFO | Train Epoch: 54 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.194, 499.410/s, 499.410/s/gpu LR: 4.633859e-09 Logit Scale: 100.000 Contrastive_loss: 0.34755 (0.28422) Loss: 0.34755 (0.28422)
2024-08-30,04:21:27 | INFO | Train Epoch: 54 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 513.479/s, 513.479/s/gpu LR: 4.623416e-09 Logit Scale: 100.000 Contrastive_loss: 0.18064 (0.25833) Loss: 0.18064 (0.25833)
2024-08-30,04:21:46 | INFO | Train Epoch: 54 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 533.535/s, 533.535/s/gpu LR: 4.612974e-09 Logit Scale: 100.000 Contrastive_loss: 0.13311 (0.23328) Loss: 0.13311 (0.23328)
2024-08-30,04:22:06 | INFO | Train Epoch: 54 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 531.366/s, 531.366/s/gpu LR: 4.602534e-09 Logit Scale: 100.000 Contrastive_loss: 0.15873 (0.22086) Loss: 0.15873 (0.22086)
2024-08-30,04:22:25 | INFO | Train Epoch: 54 [ 60100/145000.0 (41%)] Data (t): 0.107 Batch (t): 0.192, 506.942/s, 506.942/s/gpu LR: 4.592096e-09 Logit Scale: 100.000 Contrastive_loss: 0.11646 (0.20594) Loss: 0.11646 (0.20594)
2024-08-30,04:22:44 | INFO | Train Epoch: 54 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.192, 502.934/s, 502.934/s/gpu LR: 4.581660e-09 Logit Scale: 100.000 Contrastive_loss: 0.21804 (0.20745) Loss: 0.21804 (0.20745)
2024-08-30,04:23:03 | INFO | Train Epoch: 54 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.192, 534.383/s, 534.383/s/gpu LR: 4.571226e-09 Logit Scale: 100.000 Contrastive_loss: 0.13002 (0.19885) Loss: 0.13002 (0.19885)
2024-08-30,04:23:23 | INFO | Train Epoch: 54 [ 90100/145000.0 (62%)] Data (t): 0.107 Batch (t): 0.192, 528.495/s, 528.495/s/gpu LR: 4.560793e-09 Logit Scale: 100.000 Contrastive_loss: 0.21203 (0.20017) Loss: 0.21203 (0.20017)
2024-08-30,04:23:42 | INFO | Train Epoch: 54 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 485.473/s, 485.473/s/gpu LR: 4.550363e-09 Logit Scale: 100.000 Contrastive_loss: 0.15130 (0.19573) Loss: 0.15130 (0.19573)
2024-08-30,04:24:01 | INFO | Train Epoch: 54 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 500.181/s, 500.181/s/gpu LR: 4.539934e-09 Logit Scale: 100.000 Contrastive_loss: 0.16991 (0.19357) Loss: 0.16991 (0.19357)
2024-08-30,04:24:21 | INFO | Train Epoch: 54 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 529.547/s, 529.547/s/gpu LR: 4.529507e-09 Logit Scale: 100.000 Contrastive_loss: 0.27232 (0.19963) Loss: 0.27232 (0.19963)
2024-08-30,04:24:40 | INFO | Train Epoch: 54 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 529.196/s, 529.196/s/gpu LR: 4.519083e-09 Logit Scale: 100.000 Contrastive_loss: 0.15171 (0.19621) Loss: 0.15171 (0.19621)
2024-08-30,04:24:59 | INFO | Train Epoch: 54 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 533.311/s, 533.311/s/gpu LR: 4.508660e-09 Logit Scale: 100.000 Contrastive_loss: 0.17775 (0.19498) Loss: 0.17775 (0.19498)
2024-08-30,04:25:09 | INFO | Train Epoch: 54 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 534.331/s, 534.331/s/gpu LR: 4.503554e-09 Logit Scale: 100.000 Contrastive_loss: 0.22053 (0.19658) Loss: 0.22053 (0.19658)
2024-08-30,04:25:18 | INFO | Train Epoch: 54 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 504.226/s, 504.226/s/gpu LR: 4.498240e-09 Logit Scale: 100.000 Contrastive_loss: 0.19587 (0.19653) Loss: 0.19587 (0.19653)
2024-08-30,04:25:38 | INFO | Train Epoch: 54 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 506.895/s, 506.895/s/gpu LR: 4.487822e-09 Logit Scale: 100.000 Contrastive_loss: 0.22711 (0.19823) Loss: 0.22711 (0.19823)
2024-08-30,04:25:57 | INFO | Train Epoch: 54 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 524.781/s, 524.781/s/gpu LR: 4.477406e-09 Logit Scale: 100.000 Contrastive_loss: 0.17203 (0.19685) Loss: 0.17203 (0.19685)
2024-08-30,04:26:16 | INFO | Train Epoch: 54 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.194, 524.898/s, 524.898/s/gpu LR: 4.466993e-09 Logit Scale: 100.000 Contrastive_loss: 0.19679 (0.19685) Loss: 0.19679 (0.19685)
2024-08-30,04:26:36 | INFO | Train Epoch: 54 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 508.203/s, 508.203/s/gpu LR: 4.456582e-09 Logit Scale: 100.000 Contrastive_loss: 0.24240 (0.19902) Loss: 0.24240 (0.19902)
2024-08-30,04:26:55 | INFO | Train Epoch: 54 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 529.707/s, 529.707/s/gpu LR: 4.446173e-09 Logit Scale: 100.000 Contrastive_loss: 0.16409 (0.19743) Loss: 0.16409 (0.19743)
2024-08-30,04:27:14 | INFO | Train Epoch: 54 [210100/145000.0 (145%)] Data (t): 0.107 Batch (t): 0.192, 524.430/s, 524.430/s/gpu LR: 4.435766e-09 Logit Scale: 100.000 Contrastive_loss: 0.24912 (0.19968) Loss: 0.24912 (0.19968)
2024-08-30,04:27:33 | INFO | Train Epoch: 54 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 501.716/s, 501.716/s/gpu LR: 4.425362e-09 Logit Scale: 100.000 Contrastive_loss: 0.30617 (0.20412) Loss: 0.30617 (0.20412)
2024-08-30,04:27:53 | INFO | Train Epoch: 54 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 495.988/s, 495.988/s/gpu LR: 4.414961e-09 Logit Scale: 100.000 Contrastive_loss: 0.29795 (0.20787) Loss: 0.29795 (0.20787)
2024-08-30,04:28:12 | INFO | Train Epoch: 54 [240100/145000.0 (166%)] Data (t): 0.107 Batch (t): 0.192, 498.123/s, 498.123/s/gpu LR: 4.404562e-09 Logit Scale: 100.000 Contrastive_loss: 0.16252 (0.20613) Loss: 0.16252 (0.20613)
2024-08-30,04:28:31 | INFO | Train Epoch: 54 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 527.011/s, 527.011/s/gpu LR: 4.394166e-09 Logit Scale: 100.000 Contrastive_loss: 0.21491 (0.20645) Loss: 0.21491 (0.20645)
2024-08-30,04:28:50 | INFO | Train Epoch: 54 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 522.171/s, 522.171/s/gpu LR: 4.383772e-09 Logit Scale: 100.000 Contrastive_loss: 0.21660 (0.20681) Loss: 0.21660 (0.20681)
2024-08-30,04:29:10 | INFO | Train Epoch: 54 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 515.465/s, 515.465/s/gpu LR: 4.373382e-09 Logit Scale: 100.000 Contrastive_loss: 0.25472 (0.20847) Loss: 0.25472 (0.20847)
2024-08-30,04:29:29 | INFO | Train Epoch: 54 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 493.884/s, 493.884/s/gpu LR: 4.362994e-09 Logit Scale: 100.000 Contrastive_loss: 0.16817 (0.20712) Loss: 0.16817 (0.20712)
2024-08-30,04:29:48 | INFO | Train Epoch: 54 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.193, 532.250/s, 532.250/s/gpu LR: 4.352608e-09 Logit Scale: 100.000 Contrastive_loss: 0.14432 (0.20510) Loss: 0.14432 (0.20510)
2024-08-30,04:30:08 | INFO | Train Epoch: 54 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 507.256/s, 507.256/s/gpu LR: 4.342226e-09 Logit Scale: 100.000 Contrastive_loss: 0.26071 (0.20683) Loss: 0.26071 (0.20683)
2024-08-30,04:30:27 | INFO | Train Epoch: 54 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 524.207/s, 524.207/s/gpu LR: 4.331846e-09 Logit Scale: 100.000 Contrastive_loss: 0.24301 (0.20793) Loss: 0.24301 (0.20793)
2024-08-30,04:30:46 | INFO | Train Epoch: 54 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 536.671/s, 536.671/s/gpu LR: 4.321470e-09 Logit Scale: 100.000 Contrastive_loss: 0.27927 (0.21003) Loss: 0.27927 (0.21003)
2024-08-30,04:31:05 | INFO | Train Epoch: 54 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 498.913/s, 498.913/s/gpu LR: 4.311096e-09 Logit Scale: 100.000 Contrastive_loss: 0.18477 (0.20931) Loss: 0.18477 (0.20931)
2024-08-30,04:31:25 | INFO | Train Epoch: 54 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 532.697/s, 532.697/s/gpu LR: 4.300725e-09 Logit Scale: 100.000 Contrastive_loss: 0.24630 (0.21033) Loss: 0.24630 (0.21033)
2024-08-30,04:31:44 | INFO | Train Epoch: 54 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 501.360/s, 501.360/s/gpu LR: 4.290358e-09 Logit Scale: 100.000 Contrastive_loss: 0.29782 (0.21270) Loss: 0.29782 (0.21270)
2024-08-30,04:32:03 | INFO | Train Epoch: 54 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 505.544/s, 505.544/s/gpu LR: 4.279993e-09 Logit Scale: 100.000 Contrastive_loss: 0.15372 (0.21115) Loss: 0.15372 (0.21115)
2024-08-30,04:32:23 | INFO | Train Epoch: 54 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.192, 498.250/s, 498.250/s/gpu LR: 4.269632e-09 Logit Scale: 100.000 Contrastive_loss: 0.25692 (0.21232) Loss: 0.25692 (0.21232)
2024-08-30,04:32:42 | INFO | Train Epoch: 54 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 522.498/s, 522.498/s/gpu LR: 4.259274e-09 Logit Scale: 100.000 Contrastive_loss: 0.23359 (0.21285) Loss: 0.23359 (0.21285)
2024-08-30,04:33:01 | INFO | Train Epoch: 54 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 499.291/s, 499.291/s/gpu LR: 4.248919e-09 Logit Scale: 100.000 Contrastive_loss: 0.13042 (0.21084) Loss: 0.13042 (0.21084)
2024-08-30,04:33:20 | INFO | Train Epoch: 54 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 503.319/s, 503.319/s/gpu LR: 4.238567e-09 Logit Scale: 100.000 Contrastive_loss: 0.22191 (0.21111) Loss: 0.22191 (0.21111)
2024-08-30,04:33:40 | INFO | Train Epoch: 54 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.192, 526.249/s, 526.249/s/gpu LR: 4.228219e-09 Logit Scale: 100.000 Contrastive_loss: 0.087881 (0.20824) Loss: 0.087881 (0.20824)
2024-08-30,04:33:47 | INFO | Eval Epoch: 55 [200 / 1000]	Clip Loss: 0.482341	
2024-08-30,04:33:48 | INFO | Eval Epoch: 55 image_to_text_mean_rank: 2.4140	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6690	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.8730	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6690	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4395	epoch: 55.0000	num_samples: 1000.0000
2024-08-30,04:33:49 | INFO | Start epoch 55
2024-08-30,04:33:49 | INFO | Train Epoch: 55 [   100/145000.0 (0%)] Data (t): 0.025 Batch (t): 0.111, 903.300/s, 903.300/s/gpu LR: 4.503450e-09 Logit Scale: 100.000 Contrastive_loss: 0.16810 (0.16810) Loss: 0.16810 (0.16810)
2024-08-30,04:34:09 | INFO | Train Epoch: 55 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 527.686/s, 527.686/s/gpu LR: 4.493031e-09 Logit Scale: 100.000 Contrastive_loss: 0.33562 (0.25186) Loss: 0.33562 (0.25186)
2024-08-30,04:34:28 | INFO | Train Epoch: 55 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.193, 485.559/s, 485.559/s/gpu LR: 4.482614e-09 Logit Scale: 100.000 Contrastive_loss: 0.34637 (0.28336) Loss: 0.34637 (0.28336)
2024-08-30,04:34:47 | INFO | Train Epoch: 55 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 502.577/s, 502.577/s/gpu LR: 4.472199e-09 Logit Scale: 100.000 Contrastive_loss: 0.17983 (0.25748) Loss: 0.17983 (0.25748)
2024-08-30,04:35:07 | INFO | Train Epoch: 55 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 528.314/s, 528.314/s/gpu LR: 4.461787e-09 Logit Scale: 100.000 Contrastive_loss: 0.13262 (0.23251) Loss: 0.13262 (0.23251)
2024-08-30,04:35:26 | INFO | Train Epoch: 55 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 499.211/s, 499.211/s/gpu LR: 4.451377e-09 Logit Scale: 100.000 Contrastive_loss: 0.15807 (0.22010) Loss: 0.15807 (0.22010)
2024-08-30,04:35:45 | INFO | Train Epoch: 55 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.192, 534.250/s, 534.250/s/gpu LR: 4.440969e-09 Logit Scale: 100.000 Contrastive_loss: 0.11603 (0.20524) Loss: 0.11603 (0.20524)
2024-08-30,04:36:04 | INFO | Train Epoch: 55 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 527.084/s, 527.084/s/gpu LR: 4.430564e-09 Logit Scale: 100.000 Contrastive_loss: 0.21703 (0.20671) Loss: 0.21703 (0.20671)
2024-08-30,04:36:24 | INFO | Train Epoch: 55 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 495.134/s, 495.134/s/gpu LR: 4.420161e-09 Logit Scale: 100.000 Contrastive_loss: 0.12938 (0.19812) Loss: 0.12938 (0.19812)
2024-08-30,04:36:43 | INFO | Train Epoch: 55 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 499.264/s, 499.264/s/gpu LR: 4.409761e-09 Logit Scale: 100.000 Contrastive_loss: 0.21122 (0.19943) Loss: 0.21122 (0.19943)
2024-08-30,04:37:02 | INFO | Train Epoch: 55 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 529.048/s, 529.048/s/gpu LR: 4.399364e-09 Logit Scale: 100.000 Contrastive_loss: 0.15098 (0.19502) Loss: 0.15098 (0.19502)
2024-08-30,04:37:22 | INFO | Train Epoch: 55 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 540.636/s, 540.636/s/gpu LR: 4.388969e-09 Logit Scale: 100.000 Contrastive_loss: 0.16942 (0.19289) Loss: 0.16942 (0.19289)
2024-08-30,04:37:41 | INFO | Train Epoch: 55 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 522.751/s, 522.751/s/gpu LR: 4.378577e-09 Logit Scale: 100.000 Contrastive_loss: 0.27149 (0.19894) Loss: 0.27149 (0.19894)
2024-08-30,04:38:00 | INFO | Train Epoch: 55 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 529.530/s, 529.530/s/gpu LR: 4.368187e-09 Logit Scale: 100.000 Contrastive_loss: 0.15122 (0.19553) Loss: 0.15122 (0.19553)
2024-08-30,04:38:19 | INFO | Train Epoch: 55 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 524.604/s, 524.604/s/gpu LR: 4.357801e-09 Logit Scale: 100.000 Contrastive_loss: 0.17741 (0.19432) Loss: 0.17741 (0.19432)
2024-08-30,04:38:29 | INFO | Train Epoch: 55 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.192, 526.918/s, 526.918/s/gpu LR: 4.352712e-09 Logit Scale: 100.000 Contrastive_loss: 0.21944 (0.19589) Loss: 0.21944 (0.19589)
2024-08-30,04:38:39 | INFO | Train Epoch: 55 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.194, 524.155/s, 524.155/s/gpu LR: 4.347417e-09 Logit Scale: 100.000 Contrastive_loss: 0.19522 (0.19585) Loss: 0.19522 (0.19585)
2024-08-30,04:38:58 | INFO | Train Epoch: 55 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 523.264/s, 523.264/s/gpu LR: 4.337036e-09 Logit Scale: 100.000 Contrastive_loss: 0.22604 (0.19753) Loss: 0.22604 (0.19753)
2024-08-30,04:39:17 | INFO | Train Epoch: 55 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 521.475/s, 521.475/s/gpu LR: 4.326657e-09 Logit Scale: 100.000 Contrastive_loss: 0.17127 (0.19615) Loss: 0.17127 (0.19615)
2024-08-30,04:39:37 | INFO | Train Epoch: 55 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 527.467/s, 527.467/s/gpu LR: 4.316282e-09 Logit Scale: 100.000 Contrastive_loss: 0.19635 (0.19616) Loss: 0.19635 (0.19616)
2024-08-30,04:39:56 | INFO | Train Epoch: 55 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 509.919/s, 509.919/s/gpu LR: 4.305910e-09 Logit Scale: 100.000 Contrastive_loss: 0.24162 (0.19832) Loss: 0.24162 (0.19832)
2024-08-30,04:40:15 | INFO | Train Epoch: 55 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.192, 496.089/s, 496.089/s/gpu LR: 4.295541e-09 Logit Scale: 100.000 Contrastive_loss: 0.16370 (0.19675) Loss: 0.16370 (0.19675)
2024-08-30,04:40:34 | INFO | Train Epoch: 55 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 500.268/s, 500.268/s/gpu LR: 4.285175e-09 Logit Scale: 100.000 Contrastive_loss: 0.24838 (0.19899) Loss: 0.24838 (0.19899)
2024-08-30,04:40:53 | INFO | Train Epoch: 55 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 496.247/s, 496.247/s/gpu LR: 4.274812e-09 Logit Scale: 100.000 Contrastive_loss: 0.30506 (0.20341) Loss: 0.30506 (0.20341)
2024-08-30,04:41:13 | INFO | Train Epoch: 55 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 487.703/s, 487.703/s/gpu LR: 4.264452e-09 Logit Scale: 100.000 Contrastive_loss: 0.29735 (0.20717) Loss: 0.29735 (0.20717)
2024-08-30,04:41:32 | INFO | Train Epoch: 55 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 489.051/s, 489.051/s/gpu LR: 4.254096e-09 Logit Scale: 100.000 Contrastive_loss: 0.16171 (0.20542) Loss: 0.16171 (0.20542)
2024-08-30,04:41:51 | INFO | Train Epoch: 55 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 496.348/s, 496.348/s/gpu LR: 4.243743e-09 Logit Scale: 100.000 Contrastive_loss: 0.21460 (0.20576) Loss: 0.21460 (0.20576)
2024-08-30,04:42:11 | INFO | Train Epoch: 55 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 527.215/s, 527.215/s/gpu LR: 4.233393e-09 Logit Scale: 100.000 Contrastive_loss: 0.21582 (0.20612) Loss: 0.21582 (0.20612)
2024-08-30,04:42:30 | INFO | Train Epoch: 55 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 504.826/s, 504.826/s/gpu LR: 4.223046e-09 Logit Scale: 100.000 Contrastive_loss: 0.25415 (0.20778) Loss: 0.25415 (0.20778)
2024-08-30,04:42:49 | INFO | Train Epoch: 55 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 496.018/s, 496.018/s/gpu LR: 4.212703e-09 Logit Scale: 100.000 Contrastive_loss: 0.16777 (0.20644) Loss: 0.16777 (0.20644)
2024-08-30,04:43:08 | INFO | Train Epoch: 55 [290100/145000.0 (200%)] Data (t): 0.107 Batch (t): 0.193, 530.712/s, 530.712/s/gpu LR: 4.202363e-09 Logit Scale: 100.000 Contrastive_loss: 0.14413 (0.20443) Loss: 0.14413 (0.20443)
2024-08-30,04:43:28 | INFO | Train Epoch: 55 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 485.087/s, 485.087/s/gpu LR: 4.192027e-09 Logit Scale: 100.000 Contrastive_loss: 0.25989 (0.20617) Loss: 0.25989 (0.20617)
2024-08-30,04:43:47 | INFO | Train Epoch: 55 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 534.654/s, 534.654/s/gpu LR: 4.181695e-09 Logit Scale: 100.000 Contrastive_loss: 0.24258 (0.20727) Loss: 0.24258 (0.20727)
2024-08-30,04:44:06 | INFO | Train Epoch: 55 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 498.611/s, 498.611/s/gpu LR: 4.171366e-09 Logit Scale: 100.000 Contrastive_loss: 0.27848 (0.20936) Loss: 0.27848 (0.20936)
2024-08-30,04:44:26 | INFO | Train Epoch: 55 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.193, 525.199/s, 525.199/s/gpu LR: 4.161040e-09 Logit Scale: 100.000 Contrastive_loss: 0.18430 (0.20865) Loss: 0.18430 (0.20865)
2024-08-30,04:44:45 | INFO | Train Epoch: 55 [340100/145000.0 (235%)] Data (t): 0.107 Batch (t): 0.192, 530.176/s, 530.176/s/gpu LR: 4.150718e-09 Logit Scale: 100.000 Contrastive_loss: 0.24519 (0.20966) Loss: 0.24519 (0.20966)
2024-08-30,04:45:04 | INFO | Train Epoch: 55 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.193, 491.720/s, 491.720/s/gpu LR: 4.140400e-09 Logit Scale: 100.000 Contrastive_loss: 0.29690 (0.21202) Loss: 0.29690 (0.21202)
2024-08-30,04:45:23 | INFO | Train Epoch: 55 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 497.225/s, 497.225/s/gpu LR: 4.130086e-09 Logit Scale: 100.000 Contrastive_loss: 0.15308 (0.21047) Loss: 0.15308 (0.21047)
2024-08-30,04:45:43 | INFO | Train Epoch: 55 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 497.205/s, 497.205/s/gpu LR: 4.119776e-09 Logit Scale: 100.000 Contrastive_loss: 0.25621 (0.21164) Loss: 0.25621 (0.21164)
2024-08-30,04:46:02 | INFO | Train Epoch: 55 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 500.467/s, 500.467/s/gpu LR: 4.109469e-09 Logit Scale: 100.000 Contrastive_loss: 0.23300 (0.21218) Loss: 0.23300 (0.21218)
2024-08-30,04:46:21 | INFO | Train Epoch: 55 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 536.207/s, 536.207/s/gpu LR: 4.099167e-09 Logit Scale: 100.000 Contrastive_loss: 0.12984 (0.21017) Loss: 0.12984 (0.21017)
2024-08-30,04:46:40 | INFO | Train Epoch: 55 [400100/145000.0 (276%)] Data (t): 0.107 Batch (t): 0.192, 538.691/s, 538.691/s/gpu LR: 4.088868e-09 Logit Scale: 100.000 Contrastive_loss: 0.22131 (0.21043) Loss: 0.22131 (0.21043)
2024-08-30,04:47:00 | INFO | Train Epoch: 55 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 498.954/s, 498.954/s/gpu LR: 4.078573e-09 Logit Scale: 100.000 Contrastive_loss: 0.087506 (0.20757) Loss: 0.087506 (0.20757)
2024-08-30,04:47:07 | INFO | Eval Epoch: 56 [200 / 1000]	Clip Loss: 0.482450	
2024-08-30,04:47:08 | INFO | Eval Epoch: 56 image_to_text_mean_rank: 2.4120	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6690	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.8830	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6700	text_to_image_R@5: 0.9060	text_to_image_R@10: 0.9660	clip_val_loss: 0.4395	epoch: 56.0000	num_samples: 1000.0000
2024-08-30,04:47:09 | INFO | Start epoch 56
2024-08-30,04:47:09 | INFO | Train Epoch: 56 [   100/145000.0 (0%)] Data (t): 0.025 Batch (t): 0.110, 909.062/s, 909.062/s/gpu LR: 4.352608e-09 Logit Scale: 100.000 Contrastive_loss: 0.16747 (0.16747) Loss: 0.16747 (0.16747)
2024-08-30,04:47:29 | INFO | Train Epoch: 56 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 497.192/s, 497.192/s/gpu LR: 4.342226e-09 Logit Scale: 100.000 Contrastive_loss: 0.33486 (0.25116) Loss: 0.33486 (0.25116)
2024-08-30,04:47:48 | INFO | Train Epoch: 56 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.193, 529.480/s, 529.480/s/gpu LR: 4.331846e-09 Logit Scale: 100.000 Contrastive_loss: 0.34524 (0.28252) Loss: 0.34524 (0.28252)
2024-08-30,04:48:07 | INFO | Train Epoch: 56 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 539.921/s, 539.921/s/gpu LR: 4.321470e-09 Logit Scale: 100.000 Contrastive_loss: 0.17912 (0.25667) Loss: 0.17912 (0.25667)
2024-08-30,04:48:27 | INFO | Train Epoch: 56 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 497.659/s, 497.659/s/gpu LR: 4.311096e-09 Logit Scale: 100.000 Contrastive_loss: 0.13224 (0.23178) Loss: 0.13224 (0.23178)
2024-08-30,04:48:46 | INFO | Train Epoch: 56 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 493.041/s, 493.041/s/gpu LR: 4.300725e-09 Logit Scale: 100.000 Contrastive_loss: 0.15731 (0.21937) Loss: 0.15731 (0.21937)
2024-08-30,04:49:05 | INFO | Train Epoch: 56 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 533.362/s, 533.362/s/gpu LR: 4.290358e-09 Logit Scale: 100.000 Contrastive_loss: 0.11563 (0.20455) Loss: 0.11563 (0.20455)
2024-08-30,04:49:25 | INFO | Train Epoch: 56 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 503.386/s, 503.386/s/gpu LR: 4.279993e-09 Logit Scale: 100.000 Contrastive_loss: 0.21597 (0.20598) Loss: 0.21597 (0.20598)
2024-08-30,04:49:44 | INFO | Train Epoch: 56 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 538.824/s, 538.824/s/gpu LR: 4.269632e-09 Logit Scale: 100.000 Contrastive_loss: 0.12873 (0.19740) Loss: 0.12873 (0.19740)
2024-08-30,04:50:03 | INFO | Train Epoch: 56 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 499.524/s, 499.524/s/gpu LR: 4.259274e-09 Logit Scale: 100.000 Contrastive_loss: 0.21055 (0.19871) Loss: 0.21055 (0.19871)
2024-08-30,04:50:22 | INFO | Train Epoch: 56 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 520.151/s, 520.151/s/gpu LR: 4.248919e-09 Logit Scale: 100.000 Contrastive_loss: 0.15044 (0.19432) Loss: 0.15044 (0.19432)
2024-08-30,04:50:42 | INFO | Train Epoch: 56 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 527.498/s, 527.498/s/gpu LR: 4.238567e-09 Logit Scale: 100.000 Contrastive_loss: 0.16882 (0.19220) Loss: 0.16882 (0.19220)
2024-08-30,04:51:01 | INFO | Train Epoch: 56 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 524.052/s, 524.052/s/gpu LR: 4.228219e-09 Logit Scale: 100.000 Contrastive_loss: 0.27082 (0.19825) Loss: 0.27082 (0.19825)
2024-08-30,04:51:20 | INFO | Train Epoch: 56 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.194, 532.253/s, 532.253/s/gpu LR: 4.217874e-09 Logit Scale: 100.000 Contrastive_loss: 0.15072 (0.19485) Loss: 0.15072 (0.19485)
2024-08-30,04:51:40 | INFO | Train Epoch: 56 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 503.153/s, 503.153/s/gpu LR: 4.207533e-09 Logit Scale: 100.000 Contrastive_loss: 0.17689 (0.19365) Loss: 0.17689 (0.19365)
2024-08-30,04:51:49 | INFO | Train Epoch: 56 [145000/145000.0 (100%)] Data (t): 0.107 Batch (t): 0.193, 528.513/s, 528.513/s/gpu LR: 4.202467e-09 Logit Scale: 100.000 Contrastive_loss: 0.21825 (0.19519) Loss: 0.21825 (0.19519)
2024-08-30,04:51:59 | INFO | Train Epoch: 56 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.194, 527.122/s, 527.122/s/gpu LR: 4.197195e-09 Logit Scale: 100.000 Contrastive_loss: 0.19433 (0.19514) Loss: 0.19433 (0.19514)
2024-08-30,04:52:18 | INFO | Train Epoch: 56 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 496.955/s, 496.955/s/gpu LR: 4.186860e-09 Logit Scale: 100.000 Contrastive_loss: 0.22513 (0.19681) Loss: 0.22513 (0.19681)
2024-08-30,04:52:37 | INFO | Train Epoch: 56 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 518.325/s, 518.325/s/gpu LR: 4.176530e-09 Logit Scale: 100.000 Contrastive_loss: 0.17057 (0.19542) Loss: 0.17057 (0.19542)
2024-08-30,04:52:57 | INFO | Train Epoch: 56 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 498.659/s, 498.659/s/gpu LR: 4.166202e-09 Logit Scale: 100.000 Contrastive_loss: 0.19591 (0.19545) Loss: 0.19591 (0.19545)
2024-08-30,04:53:16 | INFO | Train Epoch: 56 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 498.500/s, 498.500/s/gpu LR: 4.155879e-09 Logit Scale: 100.000 Contrastive_loss: 0.24091 (0.19761) Loss: 0.24091 (0.19761)
2024-08-30,04:53:35 | INFO | Train Epoch: 56 [200100/145000.0 (138%)] Data (t): 0.107 Batch (t): 0.192, 509.812/s, 509.812/s/gpu LR: 4.145559e-09 Logit Scale: 100.000 Contrastive_loss: 0.16318 (0.19605) Loss: 0.16318 (0.19605)
2024-08-30,04:53:55 | INFO | Train Epoch: 56 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.194, 497.854/s, 497.854/s/gpu LR: 4.135243e-09 Logit Scale: 100.000 Contrastive_loss: 0.24778 (0.19830) Loss: 0.24778 (0.19830)
2024-08-30,04:54:14 | INFO | Train Epoch: 56 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.192, 525.131/s, 525.131/s/gpu LR: 4.124930e-09 Logit Scale: 100.000 Contrastive_loss: 0.30400 (0.20270) Loss: 0.30400 (0.20270)
2024-08-30,04:54:33 | INFO | Train Epoch: 56 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 533.087/s, 533.087/s/gpu LR: 4.114622e-09 Logit Scale: 100.000 Contrastive_loss: 0.29672 (0.20646) Loss: 0.29672 (0.20646)
2024-08-30,04:54:52 | INFO | Train Epoch: 56 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 529.062/s, 529.062/s/gpu LR: 4.104317e-09 Logit Scale: 100.000 Contrastive_loss: 0.16091 (0.20471) Loss: 0.16091 (0.20471)
2024-08-30,04:55:12 | INFO | Train Epoch: 56 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 484.140/s, 484.140/s/gpu LR: 4.094017e-09 Logit Scale: 100.000 Contrastive_loss: 0.21424 (0.20506) Loss: 0.21424 (0.20506)
2024-08-30,04:55:31 | INFO | Train Epoch: 56 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 522.761/s, 522.761/s/gpu LR: 4.083720e-09 Logit Scale: 100.000 Contrastive_loss: 0.21528 (0.20543) Loss: 0.21528 (0.20543)
2024-08-30,04:55:50 | INFO | Train Epoch: 56 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 521.782/s, 521.782/s/gpu LR: 4.073427e-09 Logit Scale: 100.000 Contrastive_loss: 0.25351 (0.20709) Loss: 0.25351 (0.20709)
2024-08-30,04:56:10 | INFO | Train Epoch: 56 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 501.889/s, 501.889/s/gpu LR: 4.063139e-09 Logit Scale: 100.000 Contrastive_loss: 0.16727 (0.20576) Loss: 0.16727 (0.20576)
2024-08-30,04:56:29 | INFO | Train Epoch: 56 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 502.967/s, 502.967/s/gpu LR: 4.052854e-09 Logit Scale: 100.000 Contrastive_loss: 0.14378 (0.20376) Loss: 0.14378 (0.20376)
2024-08-30,04:56:48 | INFO | Train Epoch: 56 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 495.177/s, 495.177/s/gpu LR: 4.042574e-09 Logit Scale: 100.000 Contrastive_loss: 0.25929 (0.20550) Loss: 0.25929 (0.20550)
2024-08-30,04:57:07 | INFO | Train Epoch: 56 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 539.012/s, 539.012/s/gpu LR: 4.032298e-09 Logit Scale: 100.000 Contrastive_loss: 0.24210 (0.20660) Loss: 0.24210 (0.20660)
2024-08-30,04:57:27 | INFO | Train Epoch: 56 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.192, 497.550/s, 497.550/s/gpu LR: 4.022026e-09 Logit Scale: 100.000 Contrastive_loss: 0.27755 (0.20869) Loss: 0.27755 (0.20869)
2024-08-30,04:57:46 | INFO | Train Epoch: 56 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 531.834/s, 531.834/s/gpu LR: 4.011758e-09 Logit Scale: 100.000 Contrastive_loss: 0.18390 (0.20798) Loss: 0.18390 (0.20798)
2024-08-30,04:58:05 | INFO | Train Epoch: 56 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 525.508/s, 525.508/s/gpu LR: 4.001495e-09 Logit Scale: 100.000 Contrastive_loss: 0.24424 (0.20899) Loss: 0.24424 (0.20899)
2024-08-30,04:58:25 | INFO | Train Epoch: 56 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 497.823/s, 497.823/s/gpu LR: 3.991236e-09 Logit Scale: 100.000 Contrastive_loss: 0.29617 (0.21135) Loss: 0.29617 (0.21135)
2024-08-30,04:58:44 | INFO | Train Epoch: 56 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 493.059/s, 493.059/s/gpu LR: 3.980981e-09 Logit Scale: 100.000 Contrastive_loss: 0.15253 (0.20980) Loss: 0.15253 (0.20980)
2024-08-30,04:59:03 | INFO | Train Epoch: 56 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 525.564/s, 525.564/s/gpu LR: 3.970731e-09 Logit Scale: 100.000 Contrastive_loss: 0.25553 (0.21097) Loss: 0.25553 (0.21097)
2024-08-30,04:59:22 | INFO | Train Epoch: 56 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 500.986/s, 500.986/s/gpu LR: 3.960486e-09 Logit Scale: 100.000 Contrastive_loss: 0.23238 (0.21151) Loss: 0.23238 (0.21151)
2024-08-30,04:59:42 | INFO | Train Epoch: 56 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 537.849/s, 537.849/s/gpu LR: 3.950245e-09 Logit Scale: 100.000 Contrastive_loss: 0.12928 (0.20950) Loss: 0.12928 (0.20950)
2024-08-30,05:00:01 | INFO | Train Epoch: 56 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 499.310/s, 499.310/s/gpu LR: 3.940009e-09 Logit Scale: 100.000 Contrastive_loss: 0.22057 (0.20976) Loss: 0.22057 (0.20976)
2024-08-30,05:00:20 | INFO | Train Epoch: 56 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.194, 504.116/s, 504.116/s/gpu LR: 3.929777e-09 Logit Scale: 100.000 Contrastive_loss: 0.087161 (0.20691) Loss: 0.087161 (0.20691)
2024-08-30,05:00:28 | INFO | Eval Epoch: 57 [200 / 1000]	Clip Loss: 0.482537	
2024-08-30,05:00:28 | INFO | Eval Epoch: 57 image_to_text_mean_rank: 2.4090	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6690	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.8890	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6700	text_to_image_R@5: 0.9060	text_to_image_R@10: 0.9660	clip_val_loss: 0.4395	epoch: 57.0000	num_samples: 1000.0000
2024-08-30,05:00:30 | INFO | Start epoch 57
2024-08-30,05:00:30 | INFO | Train Epoch: 57 [   100/145000.0 (0%)] Data (t): 0.035 Batch (t): 0.123, 809.883/s, 809.883/s/gpu LR: 4.202363e-09 Logit Scale: 100.000 Contrastive_loss: 0.16687 (0.16687) Loss: 0.16687 (0.16687)
2024-08-30,05:00:49 | INFO | Train Epoch: 57 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 528.283/s, 528.283/s/gpu LR: 4.192027e-09 Logit Scale: 100.000 Contrastive_loss: 0.33416 (0.25051) Loss: 0.33416 (0.25051)
2024-08-30,05:01:09 | INFO | Train Epoch: 57 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.194, 494.521/s, 494.521/s/gpu LR: 4.181695e-09 Logit Scale: 100.000 Contrastive_loss: 0.34408 (0.28170) Loss: 0.34408 (0.28170)
2024-08-30,05:01:28 | INFO | Train Epoch: 57 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 526.610/s, 526.610/s/gpu LR: 4.171366e-09 Logit Scale: 100.000 Contrastive_loss: 0.17838 (0.25587) Loss: 0.17838 (0.25587)
2024-08-30,05:01:47 | INFO | Train Epoch: 57 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 507.920/s, 507.920/s/gpu LR: 4.161040e-09 Logit Scale: 100.000 Contrastive_loss: 0.13198 (0.23109) Loss: 0.13198 (0.23109)
2024-08-30,05:02:07 | INFO | Train Epoch: 57 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.192, 492.387/s, 492.387/s/gpu LR: 4.150718e-09 Logit Scale: 100.000 Contrastive_loss: 0.15667 (0.21869) Loss: 0.15667 (0.21869)
2024-08-30,05:02:26 | INFO | Train Epoch: 57 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 526.134/s, 526.134/s/gpu LR: 4.140400e-09 Logit Scale: 100.000 Contrastive_loss: 0.11517 (0.20390) Loss: 0.11517 (0.20390)
2024-08-30,05:02:45 | INFO | Train Epoch: 57 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.194, 494.412/s, 494.412/s/gpu LR: 4.130086e-09 Logit Scale: 100.000 Contrastive_loss: 0.21484 (0.20527) Loss: 0.21484 (0.20527)
2024-08-30,05:03:05 | INFO | Train Epoch: 57 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 525.324/s, 525.324/s/gpu LR: 4.119776e-09 Logit Scale: 100.000 Contrastive_loss: 0.12826 (0.19671) Loss: 0.12826 (0.19671)
2024-08-30,05:03:24 | INFO | Train Epoch: 57 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 508.066/s, 508.066/s/gpu LR: 4.109469e-09 Logit Scale: 100.000 Contrastive_loss: 0.20997 (0.19804) Loss: 0.20997 (0.19804)
2024-08-30,05:03:43 | INFO | Train Epoch: 57 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.192, 529.715/s, 529.715/s/gpu LR: 4.099167e-09 Logit Scale: 100.000 Contrastive_loss: 0.14989 (0.19366) Loss: 0.14989 (0.19366)
2024-08-30,05:04:02 | INFO | Train Epoch: 57 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 535.848/s, 535.848/s/gpu LR: 4.088868e-09 Logit Scale: 100.000 Contrastive_loss: 0.16821 (0.19154) Loss: 0.16821 (0.19154)
2024-08-30,05:04:22 | INFO | Train Epoch: 57 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 529.676/s, 529.676/s/gpu LR: 4.078573e-09 Logit Scale: 100.000 Contrastive_loss: 0.27008 (0.19758) Loss: 0.27008 (0.19758)
2024-08-30,05:04:41 | INFO | Train Epoch: 57 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 532.844/s, 532.844/s/gpu LR: 4.068282e-09 Logit Scale: 100.000 Contrastive_loss: 0.15035 (0.19421) Loss: 0.15035 (0.19421)
2024-08-30,05:05:00 | INFO | Train Epoch: 57 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 534.935/s, 534.935/s/gpu LR: 4.057996e-09 Logit Scale: 100.000 Contrastive_loss: 0.17646 (0.19302) Loss: 0.17646 (0.19302)
2024-08-30,05:05:10 | INFO | Train Epoch: 57 [145000/145000.0 (100%)] Data (t): 0.107 Batch (t): 0.193, 521.888/s, 521.888/s/gpu LR: 4.052957e-09 Logit Scale: 100.000 Contrastive_loss: 0.21722 (0.19454) Loss: 0.21722 (0.19454)
2024-08-30,05:05:19 | INFO | Train Epoch: 57 [150100/145000.0 (104%)] Data (t): 0.107 Batch (t): 0.193, 536.423/s, 536.423/s/gpu LR: 4.047713e-09 Logit Scale: 100.000 Contrastive_loss: 0.19381 (0.19449) Loss: 0.19381 (0.19449)
2024-08-30,05:05:39 | INFO | Train Epoch: 57 [160100/145000.0 (110%)] Data (t): 0.107 Batch (t): 0.193, 495.612/s, 495.612/s/gpu LR: 4.037435e-09 Logit Scale: 100.000 Contrastive_loss: 0.22414 (0.19614) Loss: 0.22414 (0.19614)
2024-08-30,05:05:58 | INFO | Train Epoch: 57 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 528.449/s, 528.449/s/gpu LR: 4.027161e-09 Logit Scale: 100.000 Contrastive_loss: 0.17001 (0.19477) Loss: 0.17001 (0.19477)
2024-08-30,05:06:17 | INFO | Train Epoch: 57 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 495.120/s, 495.120/s/gpu LR: 4.016891e-09 Logit Scale: 100.000 Contrastive_loss: 0.19550 (0.19480) Loss: 0.19550 (0.19480)
2024-08-30,05:06:37 | INFO | Train Epoch: 57 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 530.691/s, 530.691/s/gpu LR: 4.006626e-09 Logit Scale: 100.000 Contrastive_loss: 0.24027 (0.19697) Loss: 0.24027 (0.19697)
2024-08-30,05:06:56 | INFO | Train Epoch: 57 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.194, 499.919/s, 499.919/s/gpu LR: 3.996365e-09 Logit Scale: 100.000 Contrastive_loss: 0.16273 (0.19541) Loss: 0.16273 (0.19541)
2024-08-30,05:07:15 | INFO | Train Epoch: 57 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 522.037/s, 522.037/s/gpu LR: 3.986108e-09 Logit Scale: 100.000 Contrastive_loss: 0.24718 (0.19766) Loss: 0.24718 (0.19766)
2024-08-30,05:07:35 | INFO | Train Epoch: 57 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 492.174/s, 492.174/s/gpu LR: 3.975856e-09 Logit Scale: 100.000 Contrastive_loss: 0.30307 (0.20205) Loss: 0.30307 (0.20205)
2024-08-30,05:07:54 | INFO | Train Epoch: 57 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 527.141/s, 527.141/s/gpu LR: 3.965608e-09 Logit Scale: 100.000 Contrastive_loss: 0.29593 (0.20581) Loss: 0.29593 (0.20581)
2024-08-30,05:08:13 | INFO | Train Epoch: 57 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 494.343/s, 494.343/s/gpu LR: 3.955365e-09 Logit Scale: 100.000 Contrastive_loss: 0.16031 (0.20406) Loss: 0.16031 (0.20406)
2024-08-30,05:08:32 | INFO | Train Epoch: 57 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 508.123/s, 508.123/s/gpu LR: 3.945126e-09 Logit Scale: 100.000 Contrastive_loss: 0.21362 (0.20441) Loss: 0.21362 (0.20441)
2024-08-30,05:08:52 | INFO | Train Epoch: 57 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.192, 526.021/s, 526.021/s/gpu LR: 3.934892e-09 Logit Scale: 100.000 Contrastive_loss: 0.21464 (0.20478) Loss: 0.21464 (0.20478)
2024-08-30,05:09:11 | INFO | Train Epoch: 57 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 533.673/s, 533.673/s/gpu LR: 3.924663e-09 Logit Scale: 100.000 Contrastive_loss: 0.25294 (0.20644) Loss: 0.25294 (0.20644)
2024-08-30,05:09:30 | INFO | Train Epoch: 57 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 522.817/s, 522.817/s/gpu LR: 3.914438e-09 Logit Scale: 100.000 Contrastive_loss: 0.16677 (0.20512) Loss: 0.16677 (0.20512)
2024-08-30,05:09:49 | INFO | Train Epoch: 57 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.193, 505.223/s, 505.223/s/gpu LR: 3.904218e-09 Logit Scale: 100.000 Contrastive_loss: 0.14357 (0.20313) Loss: 0.14357 (0.20313)
2024-08-30,05:10:09 | INFO | Train Epoch: 57 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 526.772/s, 526.772/s/gpu LR: 3.894003e-09 Logit Scale: 100.000 Contrastive_loss: 0.25854 (0.20486) Loss: 0.25854 (0.20486)
2024-08-30,05:10:28 | INFO | Train Epoch: 57 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 524.458/s, 524.458/s/gpu LR: 3.883793e-09 Logit Scale: 100.000 Contrastive_loss: 0.24170 (0.20598) Loss: 0.24170 (0.20598)
2024-08-30,05:10:47 | INFO | Train Epoch: 57 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 498.076/s, 498.076/s/gpu LR: 3.873588e-09 Logit Scale: 100.000 Contrastive_loss: 0.27687 (0.20806) Loss: 0.27687 (0.20806)
2024-08-30,05:11:06 | INFO | Train Epoch: 57 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 526.423/s, 526.423/s/gpu LR: 3.863387e-09 Logit Scale: 100.000 Contrastive_loss: 0.18356 (0.20736) Loss: 0.18356 (0.20736)
2024-08-30,05:11:26 | INFO | Train Epoch: 57 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.193, 499.169/s, 499.169/s/gpu LR: 3.853192e-09 Logit Scale: 100.000 Contrastive_loss: 0.24310 (0.20836) Loss: 0.24310 (0.20836)
2024-08-30,05:11:45 | INFO | Train Epoch: 57 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 492.155/s, 492.155/s/gpu LR: 3.843002e-09 Logit Scale: 100.000 Contrastive_loss: 0.29553 (0.21071) Loss: 0.29553 (0.21071)
2024-08-30,05:12:04 | INFO | Train Epoch: 57 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 534.451/s, 534.451/s/gpu LR: 3.832816e-09 Logit Scale: 100.000 Contrastive_loss: 0.15208 (0.20917) Loss: 0.15208 (0.20917)
2024-08-30,05:12:24 | INFO | Train Epoch: 57 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.192, 531.568/s, 531.568/s/gpu LR: 3.822636e-09 Logit Scale: 100.000 Contrastive_loss: 0.25496 (0.21034) Loss: 0.25496 (0.21034)
2024-08-30,05:12:43 | INFO | Train Epoch: 57 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 532.657/s, 532.657/s/gpu LR: 3.812461e-09 Logit Scale: 100.000 Contrastive_loss: 0.23179 (0.21088) Loss: 0.23179 (0.21088)
2024-08-30,05:13:02 | INFO | Train Epoch: 57 [390100/145000.0 (269%)] Data (t): 0.107 Batch (t): 0.192, 537.298/s, 537.298/s/gpu LR: 3.802292e-09 Logit Scale: 100.000 Contrastive_loss: 0.12865 (0.20887) Loss: 0.12865 (0.20887)
2024-08-30,05:13:21 | INFO | Train Epoch: 57 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.193, 528.263/s, 528.263/s/gpu LR: 3.792127e-09 Logit Scale: 100.000 Contrastive_loss: 0.21985 (0.20914) Loss: 0.21985 (0.20914)
2024-08-30,05:13:41 | INFO | Train Epoch: 57 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 498.200/s, 498.200/s/gpu LR: 3.781968e-09 Logit Scale: 100.000 Contrastive_loss: 0.086750 (0.20629) Loss: 0.086750 (0.20629)
2024-08-30,05:13:48 | INFO | Eval Epoch: 58 [200 / 1000]	Clip Loss: 0.482626	
2024-08-30,05:13:49 | INFO | Eval Epoch: 58 image_to_text_mean_rank: 2.4080	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6690	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.8890	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9060	text_to_image_R@10: 0.9660	clip_val_loss: 0.4396	epoch: 58.0000	num_samples: 1000.0000
2024-08-30,05:13:50 | INFO | Start epoch 58
2024-08-30,05:13:50 | INFO | Train Epoch: 58 [   100/145000.0 (0%)] Data (t): 0.024 Batch (t): 0.110, 908.411/s, 908.411/s/gpu LR: 4.052854e-09 Logit Scale: 100.000 Contrastive_loss: 0.16657 (0.16657) Loss: 0.16657 (0.16657)
2024-08-30,05:14:10 | INFO | Train Epoch: 58 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 526.715/s, 526.715/s/gpu LR: 4.042574e-09 Logit Scale: 100.000 Contrastive_loss: 0.33332 (0.24994) Loss: 0.33332 (0.24994)
2024-08-30,05:14:29 | INFO | Train Epoch: 58 [ 20100/145000.0 (14%)] Data (t): 0.110 Batch (t): 0.194, 494.772/s, 494.772/s/gpu LR: 4.032298e-09 Logit Scale: 100.000 Contrastive_loss: 0.34314 (0.28101) Loss: 0.34314 (0.28101)
2024-08-30,05:14:48 | INFO | Train Epoch: 58 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 497.919/s, 497.919/s/gpu LR: 4.022026e-09 Logit Scale: 100.000 Contrastive_loss: 0.17762 (0.25516) Loss: 0.17762 (0.25516)
2024-08-30,05:15:08 | INFO | Train Epoch: 58 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 507.612/s, 507.612/s/gpu LR: 4.011758e-09 Logit Scale: 100.000 Contrastive_loss: 0.13154 (0.23044) Loss: 0.13154 (0.23044)
2024-08-30,05:15:27 | INFO | Train Epoch: 58 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 522.784/s, 522.784/s/gpu LR: 4.001495e-09 Logit Scale: 100.000 Contrastive_loss: 0.15602 (0.21803) Loss: 0.15602 (0.21803)
2024-08-30,05:15:46 | INFO | Train Epoch: 58 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 501.098/s, 501.098/s/gpu LR: 3.991236e-09 Logit Scale: 100.000 Contrastive_loss: 0.11485 (0.20329) Loss: 0.11485 (0.20329)
2024-08-30,05:16:05 | INFO | Train Epoch: 58 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 505.494/s, 505.494/s/gpu LR: 3.980981e-09 Logit Scale: 100.000 Contrastive_loss: 0.21387 (0.20462) Loss: 0.21387 (0.20462)
2024-08-30,05:16:25 | INFO | Train Epoch: 58 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 500.502/s, 500.502/s/gpu LR: 3.970731e-09 Logit Scale: 100.000 Contrastive_loss: 0.12757 (0.19605) Loss: 0.12757 (0.19605)
2024-08-30,05:16:44 | INFO | Train Epoch: 58 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 522.558/s, 522.558/s/gpu LR: 3.960486e-09 Logit Scale: 100.000 Contrastive_loss: 0.20923 (0.19737) Loss: 0.20923 (0.19737)
2024-08-30,05:17:03 | INFO | Train Epoch: 58 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 531.105/s, 531.105/s/gpu LR: 3.950245e-09 Logit Scale: 100.000 Contrastive_loss: 0.14953 (0.19302) Loss: 0.14953 (0.19302)
2024-08-30,05:17:23 | INFO | Train Epoch: 58 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 508.173/s, 508.173/s/gpu LR: 3.940009e-09 Logit Scale: 100.000 Contrastive_loss: 0.16765 (0.19091) Loss: 0.16765 (0.19091)
2024-08-30,05:17:42 | INFO | Train Epoch: 58 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 494.695/s, 494.695/s/gpu LR: 3.929777e-09 Logit Scale: 100.000 Contrastive_loss: 0.26943 (0.19695) Loss: 0.26943 (0.19695)
2024-08-30,05:18:01 | INFO | Train Epoch: 58 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 529.821/s, 529.821/s/gpu LR: 3.919550e-09 Logit Scale: 100.000 Contrastive_loss: 0.14997 (0.19359) Loss: 0.14997 (0.19359)
2024-08-30,05:18:21 | INFO | Train Epoch: 58 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 530.429/s, 530.429/s/gpu LR: 3.909328e-09 Logit Scale: 100.000 Contrastive_loss: 0.17608 (0.19243) Loss: 0.17608 (0.19243)
2024-08-30,05:18:30 | INFO | Train Epoch: 58 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 533.926/s, 533.926/s/gpu LR: 3.904321e-09 Logit Scale: 100.000 Contrastive_loss: 0.21630 (0.19392) Loss: 0.21630 (0.19392)
2024-08-30,05:18:40 | INFO | Train Epoch: 58 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 534.776/s, 534.776/s/gpu LR: 3.899110e-09 Logit Scale: 100.000 Contrastive_loss: 0.19320 (0.19388) Loss: 0.19320 (0.19388)
2024-08-30,05:18:59 | INFO | Train Epoch: 58 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 496.904/s, 496.904/s/gpu LR: 3.888898e-09 Logit Scale: 100.000 Contrastive_loss: 0.22331 (0.19551) Loss: 0.22331 (0.19551)
2024-08-30,05:19:18 | INFO | Train Epoch: 58 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 522.461/s, 522.461/s/gpu LR: 3.878690e-09 Logit Scale: 100.000 Contrastive_loss: 0.16942 (0.19414) Loss: 0.16942 (0.19414)
2024-08-30,05:19:38 | INFO | Train Epoch: 58 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 500.423/s, 500.423/s/gpu LR: 3.868487e-09 Logit Scale: 100.000 Contrastive_loss: 0.19508 (0.19419) Loss: 0.19508 (0.19419)
2024-08-30,05:19:57 | INFO | Train Epoch: 58 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 505.328/s, 505.328/s/gpu LR: 3.858289e-09 Logit Scale: 100.000 Contrastive_loss: 0.23961 (0.19635) Loss: 0.23961 (0.19635)
2024-08-30,05:20:16 | INFO | Train Epoch: 58 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 529.562/s, 529.562/s/gpu LR: 3.848096e-09 Logit Scale: 100.000 Contrastive_loss: 0.16233 (0.19480) Loss: 0.16233 (0.19480)
2024-08-30,05:20:36 | INFO | Train Epoch: 58 [210100/145000.0 (145%)] Data (t): 0.107 Batch (t): 0.193, 508.175/s, 508.175/s/gpu LR: 3.837908e-09 Logit Scale: 100.000 Contrastive_loss: 0.24653 (0.19705) Loss: 0.24653 (0.19705)
2024-08-30,05:20:55 | INFO | Train Epoch: 58 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 500.116/s, 500.116/s/gpu LR: 3.827726e-09 Logit Scale: 100.000 Contrastive_loss: 0.30214 (0.20143) Loss: 0.30214 (0.20143)
2024-08-30,05:21:14 | INFO | Train Epoch: 58 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 497.211/s, 497.211/s/gpu LR: 3.817548e-09 Logit Scale: 100.000 Contrastive_loss: 0.29535 (0.20519) Loss: 0.29535 (0.20519)
2024-08-30,05:21:33 | INFO | Train Epoch: 58 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 522.704/s, 522.704/s/gpu LR: 3.807376e-09 Logit Scale: 100.000 Contrastive_loss: 0.15950 (0.20343) Loss: 0.15950 (0.20343)
2024-08-30,05:21:53 | INFO | Train Epoch: 58 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 503.740/s, 503.740/s/gpu LR: 3.797209e-09 Logit Scale: 100.000 Contrastive_loss: 0.21302 (0.20379) Loss: 0.21302 (0.20379)
2024-08-30,05:22:12 | INFO | Train Epoch: 58 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.192, 528.016/s, 528.016/s/gpu LR: 3.787047e-09 Logit Scale: 100.000 Contrastive_loss: 0.21411 (0.20415) Loss: 0.21411 (0.20415)
2024-08-30,05:22:31 | INFO | Train Epoch: 58 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 518.552/s, 518.552/s/gpu LR: 3.776890e-09 Logit Scale: 100.000 Contrastive_loss: 0.25249 (0.20582) Loss: 0.25249 (0.20582)
2024-08-30,05:22:51 | INFO | Train Epoch: 58 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 535.111/s, 535.111/s/gpu LR: 3.766739e-09 Logit Scale: 100.000 Contrastive_loss: 0.16646 (0.20451) Loss: 0.16646 (0.20451)
2024-08-30,05:23:10 | INFO | Train Epoch: 58 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.193, 533.181/s, 533.181/s/gpu LR: 3.756593e-09 Logit Scale: 100.000 Contrastive_loss: 0.14330 (0.20253) Loss: 0.14330 (0.20253)
2024-08-30,05:23:29 | INFO | Train Epoch: 58 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 530.227/s, 530.227/s/gpu LR: 3.746453e-09 Logit Scale: 100.000 Contrastive_loss: 0.25786 (0.20426) Loss: 0.25786 (0.20426)
2024-08-30,05:23:48 | INFO | Train Epoch: 58 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 506.064/s, 506.064/s/gpu LR: 3.736318e-09 Logit Scale: 100.000 Contrastive_loss: 0.24145 (0.20539) Loss: 0.24145 (0.20539)
2024-08-30,05:24:08 | INFO | Train Epoch: 58 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 503.483/s, 503.483/s/gpu LR: 3.726189e-09 Logit Scale: 100.000 Contrastive_loss: 0.27616 (0.20747) Loss: 0.27616 (0.20747)
2024-08-30,05:24:27 | INFO | Train Epoch: 58 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 526.275/s, 526.275/s/gpu LR: 3.716065e-09 Logit Scale: 100.000 Contrastive_loss: 0.18319 (0.20678) Loss: 0.18319 (0.20678)
2024-08-30,05:24:46 | INFO | Train Epoch: 58 [340100/145000.0 (235%)] Data (t): 0.107 Batch (t): 0.192, 498.569/s, 498.569/s/gpu LR: 3.705947e-09 Logit Scale: 100.000 Contrastive_loss: 0.24217 (0.20776) Loss: 0.24217 (0.20776)
2024-08-30,05:25:06 | INFO | Train Epoch: 58 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.193, 501.991/s, 501.991/s/gpu LR: 3.695835e-09 Logit Scale: 100.000 Contrastive_loss: 0.29483 (0.21011) Loss: 0.29483 (0.21011)
2024-08-30,05:25:25 | INFO | Train Epoch: 58 [360100/145000.0 (248%)] Data (t): 0.107 Batch (t): 0.193, 535.397/s, 535.397/s/gpu LR: 3.685728e-09 Logit Scale: 100.000 Contrastive_loss: 0.15164 (0.20858) Loss: 0.15164 (0.20858)
2024-08-30,05:25:44 | INFO | Train Epoch: 58 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 529.572/s, 529.572/s/gpu LR: 3.675627e-09 Logit Scale: 100.000 Contrastive_loss: 0.25426 (0.20975) Loss: 0.25426 (0.20975)
2024-08-30,05:26:03 | INFO | Train Epoch: 58 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 538.837/s, 538.837/s/gpu LR: 3.665532e-09 Logit Scale: 100.000 Contrastive_loss: 0.23117 (0.21028) Loss: 0.23117 (0.21028)
2024-08-30,05:26:23 | INFO | Train Epoch: 58 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 533.473/s, 533.473/s/gpu LR: 3.655443e-09 Logit Scale: 100.000 Contrastive_loss: 0.12815 (0.20828) Loss: 0.12815 (0.20828)
2024-08-30,05:26:42 | INFO | Train Epoch: 58 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.192, 524.312/s, 524.312/s/gpu LR: 3.645359e-09 Logit Scale: 100.000 Contrastive_loss: 0.21924 (0.20854) Loss: 0.21924 (0.20854)
2024-08-30,05:27:01 | INFO | Train Epoch: 58 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 522.469/s, 522.469/s/gpu LR: 3.635282e-09 Logit Scale: 100.000 Contrastive_loss: 0.086418 (0.20570) Loss: 0.086418 (0.20570)
2024-08-30,05:27:09 | INFO | Eval Epoch: 59 [200 / 1000]	Clip Loss: 0.482673	
2024-08-30,05:27:09 | INFO | Eval Epoch: 59 image_to_text_mean_rank: 2.4090	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6690	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.8900	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9060	text_to_image_R@10: 0.9660	clip_val_loss: 0.4396	epoch: 59.0000	num_samples: 1000.0000
2024-08-30,05:27:11 | INFO | Start epoch 59
2024-08-30,05:27:11 | INFO | Train Epoch: 59 [   100/145000.0 (0%)] Data (t): 0.034 Batch (t): 0.128, 778.629/s, 778.629/s/gpu LR: 3.904218e-09 Logit Scale: 100.000 Contrastive_loss: 0.16593 (0.16593) Loss: 0.16593 (0.16593)
2024-08-30,05:27:30 | INFO | Train Epoch: 59 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 526.217/s, 526.217/s/gpu LR: 3.894003e-09 Logit Scale: 100.000 Contrastive_loss: 0.33260 (0.24926) Loss: 0.33260 (0.24926)
2024-08-30,05:27:50 | INFO | Train Epoch: 59 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 524.310/s, 524.310/s/gpu LR: 3.883793e-09 Logit Scale: 100.000 Contrastive_loss: 0.34208 (0.28020) Loss: 0.34208 (0.28020)
2024-08-30,05:28:09 | INFO | Train Epoch: 59 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 505.943/s, 505.943/s/gpu LR: 3.873588e-09 Logit Scale: 100.000 Contrastive_loss: 0.17689 (0.25437) Loss: 0.17689 (0.25437)
2024-08-30,05:28:28 | INFO | Train Epoch: 59 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.192, 529.227/s, 529.227/s/gpu LR: 3.863387e-09 Logit Scale: 100.000 Contrastive_loss: 0.13125 (0.22975) Loss: 0.13125 (0.22975)
2024-08-30,05:28:47 | INFO | Train Epoch: 59 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.194, 498.299/s, 498.299/s/gpu LR: 3.853192e-09 Logit Scale: 100.000 Contrastive_loss: 0.15535 (0.21735) Loss: 0.15535 (0.21735)
2024-08-30,05:29:07 | INFO | Train Epoch: 59 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 491.565/s, 491.565/s/gpu LR: 3.843002e-09 Logit Scale: 100.000 Contrastive_loss: 0.11449 (0.20266) Loss: 0.11449 (0.20266)
2024-08-30,05:29:26 | INFO | Train Epoch: 59 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 500.465/s, 500.465/s/gpu LR: 3.832816e-09 Logit Scale: 100.000 Contrastive_loss: 0.21290 (0.20394) Loss: 0.21290 (0.20394)
2024-08-30,05:29:45 | INFO | Train Epoch: 59 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.192, 540.527/s, 540.527/s/gpu LR: 3.822636e-09 Logit Scale: 100.000 Contrastive_loss: 0.12700 (0.19539) Loss: 0.12700 (0.19539)
2024-08-30,05:30:05 | INFO | Train Epoch: 59 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.194, 475.264/s, 475.264/s/gpu LR: 3.812461e-09 Logit Scale: 100.000 Contrastive_loss: 0.20865 (0.19671) Loss: 0.20865 (0.19671)
2024-08-30,05:30:24 | INFO | Train Epoch: 59 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 526.770/s, 526.770/s/gpu LR: 3.802292e-09 Logit Scale: 100.000 Contrastive_loss: 0.14909 (0.19238) Loss: 0.14909 (0.19238)
2024-08-30,05:30:43 | INFO | Train Epoch: 59 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.193, 529.616/s, 529.616/s/gpu LR: 3.792127e-09 Logit Scale: 100.000 Contrastive_loss: 0.16722 (0.19029) Loss: 0.16722 (0.19029)
2024-08-30,05:31:03 | INFO | Train Epoch: 59 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 533.538/s, 533.538/s/gpu LR: 3.781968e-09 Logit Scale: 100.000 Contrastive_loss: 0.26881 (0.19633) Loss: 0.26881 (0.19633)
2024-08-30,05:31:22 | INFO | Train Epoch: 59 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 529.827/s, 529.827/s/gpu LR: 3.771814e-09 Logit Scale: 100.000 Contrastive_loss: 0.14966 (0.19299) Loss: 0.14966 (0.19299)
2024-08-30,05:31:41 | INFO | Train Epoch: 59 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 497.156/s, 497.156/s/gpu LR: 3.761665e-09 Logit Scale: 100.000 Contrastive_loss: 0.17576 (0.19185) Loss: 0.17576 (0.19185)
2024-08-30,05:31:50 | INFO | Train Epoch: 59 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 498.434/s, 498.434/s/gpu LR: 3.756695e-09 Logit Scale: 100.000 Contrastive_loss: 0.21518 (0.19330) Loss: 0.21518 (0.19330)
2024-08-30,05:32:00 | INFO | Train Epoch: 59 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.194, 500.395/s, 500.395/s/gpu LR: 3.751522e-09 Logit Scale: 100.000 Contrastive_loss: 0.19245 (0.19325) Loss: 0.19245 (0.19325)
2024-08-30,05:32:20 | INFO | Train Epoch: 59 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 521.903/s, 521.903/s/gpu LR: 3.741385e-09 Logit Scale: 100.000 Contrastive_loss: 0.22257 (0.19488) Loss: 0.22257 (0.19488)
2024-08-30,05:32:39 | INFO | Train Epoch: 59 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 522.524/s, 522.524/s/gpu LR: 3.731253e-09 Logit Scale: 100.000 Contrastive_loss: 0.16887 (0.19351) Loss: 0.16887 (0.19351)
2024-08-30,05:32:58 | INFO | Train Epoch: 59 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 532.211/s, 532.211/s/gpu LR: 3.721126e-09 Logit Scale: 100.000 Contrastive_loss: 0.19466 (0.19357) Loss: 0.19466 (0.19357)
2024-08-30,05:33:18 | INFO | Train Epoch: 59 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 537.800/s, 537.800/s/gpu LR: 3.711005e-09 Logit Scale: 100.000 Contrastive_loss: 0.23908 (0.19574) Loss: 0.23908 (0.19574)
2024-08-30,05:33:37 | INFO | Train Epoch: 59 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 511.987/s, 511.987/s/gpu LR: 3.700890e-09 Logit Scale: 100.000 Contrastive_loss: 0.16194 (0.19420) Loss: 0.16194 (0.19420)
2024-08-30,05:33:56 | INFO | Train Epoch: 59 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 506.538/s, 506.538/s/gpu LR: 3.690780e-09 Logit Scale: 100.000 Contrastive_loss: 0.24594 (0.19645) Loss: 0.24594 (0.19645)
2024-08-30,05:34:15 | INFO | Train Epoch: 59 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 499.526/s, 499.526/s/gpu LR: 3.680677e-09 Logit Scale: 100.000 Contrastive_loss: 0.30124 (0.20082) Loss: 0.30124 (0.20082)
2024-08-30,05:34:35 | INFO | Train Epoch: 59 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 523.166/s, 523.166/s/gpu LR: 3.670579e-09 Logit Scale: 100.000 Contrastive_loss: 0.29483 (0.20458) Loss: 0.29483 (0.20458)
2024-08-30,05:34:54 | INFO | Train Epoch: 59 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 495.886/s, 495.886/s/gpu LR: 3.660486e-09 Logit Scale: 100.000 Contrastive_loss: 0.15890 (0.20282) Loss: 0.15890 (0.20282)
2024-08-30,05:35:13 | INFO | Train Epoch: 59 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 523.481/s, 523.481/s/gpu LR: 3.650400e-09 Logit Scale: 100.000 Contrastive_loss: 0.21271 (0.20319) Loss: 0.21271 (0.20319)
2024-08-30,05:35:33 | INFO | Train Epoch: 59 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.192, 495.718/s, 495.718/s/gpu LR: 3.640320e-09 Logit Scale: 100.000 Contrastive_loss: 0.21355 (0.20356) Loss: 0.21355 (0.20356)
2024-08-30,05:35:52 | INFO | Train Epoch: 59 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 522.847/s, 522.847/s/gpu LR: 3.630245e-09 Logit Scale: 100.000 Contrastive_loss: 0.25207 (0.20523) Loss: 0.25207 (0.20523)
2024-08-30,05:36:11 | INFO | Train Epoch: 59 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.192, 535.146/s, 535.146/s/gpu LR: 3.620177e-09 Logit Scale: 100.000 Contrastive_loss: 0.16599 (0.20392) Loss: 0.16599 (0.20392)
2024-08-30,05:36:30 | INFO | Train Epoch: 59 [290100/145000.0 (200%)] Data (t): 0.107 Batch (t): 0.192, 522.186/s, 522.186/s/gpu LR: 3.610115e-09 Logit Scale: 100.000 Contrastive_loss: 0.14309 (0.20196) Loss: 0.14309 (0.20196)
2024-08-30,05:36:50 | INFO | Train Epoch: 59 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 499.147/s, 499.147/s/gpu LR: 3.600058e-09 Logit Scale: 100.000 Contrastive_loss: 0.25726 (0.20369) Loss: 0.25726 (0.20369)
2024-08-30,05:37:09 | INFO | Train Epoch: 59 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 533.835/s, 533.835/s/gpu LR: 3.590008e-09 Logit Scale: 100.000 Contrastive_loss: 0.24111 (0.20482) Loss: 0.24111 (0.20482)
2024-08-30,05:37:28 | INFO | Train Epoch: 59 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 505.566/s, 505.566/s/gpu LR: 3.579964e-09 Logit Scale: 100.000 Contrastive_loss: 0.27537 (0.20690) Loss: 0.27537 (0.20690)
2024-08-30,05:37:47 | INFO | Train Epoch: 59 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 536.294/s, 536.294/s/gpu LR: 3.569927e-09 Logit Scale: 100.000 Contrastive_loss: 0.18291 (0.20621) Loss: 0.18291 (0.20621)
2024-08-30,05:38:07 | INFO | Train Epoch: 59 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 526.169/s, 526.169/s/gpu LR: 3.559895e-09 Logit Scale: 100.000 Contrastive_loss: 0.24139 (0.20719) Loss: 0.24139 (0.20719)
2024-08-30,05:38:26 | INFO | Train Epoch: 59 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.194, 516.044/s, 516.044/s/gpu LR: 3.549870e-09 Logit Scale: 100.000 Contrastive_loss: 0.29413 (0.20954) Loss: 0.29413 (0.20954)
2024-08-30,05:38:45 | INFO | Train Epoch: 59 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 493.188/s, 493.188/s/gpu LR: 3.539851e-09 Logit Scale: 100.000 Contrastive_loss: 0.15103 (0.20800) Loss: 0.15103 (0.20800)
2024-08-30,05:39:05 | INFO | Train Epoch: 59 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 525.277/s, 525.277/s/gpu LR: 3.529839e-09 Logit Scale: 100.000 Contrastive_loss: 0.25371 (0.20917) Loss: 0.25371 (0.20917)
2024-08-30,05:39:24 | INFO | Train Epoch: 59 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 530.164/s, 530.164/s/gpu LR: 3.519833e-09 Logit Scale: 100.000 Contrastive_loss: 0.23057 (0.20971) Loss: 0.23057 (0.20971)
2024-08-30,05:39:43 | INFO | Train Epoch: 59 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 525.153/s, 525.153/s/gpu LR: 3.509834e-09 Logit Scale: 100.000 Contrastive_loss: 0.12771 (0.20771) Loss: 0.12771 (0.20771)
2024-08-30,05:40:02 | INFO | Train Epoch: 59 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 537.855/s, 537.855/s/gpu LR: 3.499841e-09 Logit Scale: 100.000 Contrastive_loss: 0.21880 (0.20797) Loss: 0.21880 (0.20797)
2024-08-30,05:40:22 | INFO | Train Epoch: 59 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.192, 540.878/s, 540.878/s/gpu LR: 3.489854e-09 Logit Scale: 100.000 Contrastive_loss: 0.086128 (0.20514) Loss: 0.086128 (0.20514)
2024-08-30,05:40:29 | INFO | Eval Epoch: 60 [200 / 1000]	Clip Loss: 0.482637	
2024-08-30,05:40:30 | INFO | Eval Epoch: 60 image_to_text_mean_rank: 2.4100	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6690	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.8970	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6720	text_to_image_R@5: 0.9060	text_to_image_R@10: 0.9660	clip_val_loss: 0.4396	epoch: 60.0000	num_samples: 1000.0000
2024-08-30,05:40:31 | INFO | Start epoch 60
2024-08-30,05:40:31 | INFO | Train Epoch: 60 [   100/145000.0 (0%)] Data (t): 0.033 Batch (t): 0.117, 857.373/s, 857.373/s/gpu LR: 3.756593e-09 Logit Scale: 100.000 Contrastive_loss: 0.16545 (0.16545) Loss: 0.16545 (0.16545)
2024-08-30,05:40:51 | INFO | Train Epoch: 60 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 529.387/s, 529.387/s/gpu LR: 3.746453e-09 Logit Scale: 100.000 Contrastive_loss: 0.33192 (0.24869) Loss: 0.33192 (0.24869)
2024-08-30,05:41:10 | INFO | Train Epoch: 60 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.194, 502.978/s, 502.978/s/gpu LR: 3.736318e-09 Logit Scale: 100.000 Contrastive_loss: 0.34106 (0.27948) Loss: 0.34106 (0.27948)
2024-08-30,05:41:29 | INFO | Train Epoch: 60 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 530.821/s, 530.821/s/gpu LR: 3.726189e-09 Logit Scale: 100.000 Contrastive_loss: 0.17628 (0.25368) Loss: 0.17628 (0.25368)
2024-08-30,05:41:49 | INFO | Train Epoch: 60 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.194, 504.721/s, 504.721/s/gpu LR: 3.716065e-09 Logit Scale: 100.000 Contrastive_loss: 0.13091 (0.22912) Loss: 0.13091 (0.22912)
2024-08-30,05:42:08 | INFO | Train Epoch: 60 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 494.584/s, 494.584/s/gpu LR: 3.705947e-09 Logit Scale: 100.000 Contrastive_loss: 0.15483 (0.21674) Loss: 0.15483 (0.21674)
2024-08-30,05:42:27 | INFO | Train Epoch: 60 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 536.674/s, 536.674/s/gpu LR: 3.695835e-09 Logit Scale: 100.000 Contrastive_loss: 0.11413 (0.20208) Loss: 0.11413 (0.20208)
2024-08-30,05:42:47 | INFO | Train Epoch: 60 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 528.482/s, 528.482/s/gpu LR: 3.685728e-09 Logit Scale: 100.000 Contrastive_loss: 0.21207 (0.20333) Loss: 0.21207 (0.20333)
2024-08-30,05:43:06 | INFO | Train Epoch: 60 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 494.875/s, 494.875/s/gpu LR: 3.675627e-09 Logit Scale: 100.000 Contrastive_loss: 0.12661 (0.19481) Loss: 0.12661 (0.19481)
2024-08-30,05:43:25 | INFO | Train Epoch: 60 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 519.859/s, 519.859/s/gpu LR: 3.665532e-09 Logit Scale: 100.000 Contrastive_loss: 0.20805 (0.19613) Loss: 0.20805 (0.19613)
2024-08-30,05:43:44 | INFO | Train Epoch: 60 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.192, 499.579/s, 499.579/s/gpu LR: 3.655443e-09 Logit Scale: 100.000 Contrastive_loss: 0.14876 (0.19183) Loss: 0.14876 (0.19183)
2024-08-30,05:44:04 | INFO | Train Epoch: 60 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 532.176/s, 532.176/s/gpu LR: 3.645359e-09 Logit Scale: 100.000 Contrastive_loss: 0.16667 (0.18973) Loss: 0.16667 (0.18973)
2024-08-30,05:44:23 | INFO | Train Epoch: 60 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 503.786/s, 503.786/s/gpu LR: 3.635282e-09 Logit Scale: 100.000 Contrastive_loss: 0.26821 (0.19577) Loss: 0.26821 (0.19577)
2024-08-30,05:44:42 | INFO | Train Epoch: 60 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 496.828/s, 496.828/s/gpu LR: 3.625210e-09 Logit Scale: 100.000 Contrastive_loss: 0.14925 (0.19244) Loss: 0.14925 (0.19244)
2024-08-30,05:45:02 | INFO | Train Epoch: 60 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.194, 490.177/s, 490.177/s/gpu LR: 3.615145e-09 Logit Scale: 100.000 Contrastive_loss: 0.17543 (0.19131) Loss: 0.17543 (0.19131)
2024-08-30,05:45:11 | INFO | Train Epoch: 60 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 531.782/s, 531.782/s/gpu LR: 3.610215e-09 Logit Scale: 100.000 Contrastive_loss: 0.21411 (0.19273) Loss: 0.21411 (0.19273)
2024-08-30,05:45:21 | INFO | Train Epoch: 60 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.194, 524.070/s, 524.070/s/gpu LR: 3.605086e-09 Logit Scale: 100.000 Contrastive_loss: 0.19185 (0.19268) Loss: 0.19185 (0.19268)
2024-08-30,05:45:40 | INFO | Train Epoch: 60 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.194, 537.778/s, 537.778/s/gpu LR: 3.595033e-09 Logit Scale: 100.000 Contrastive_loss: 0.22174 (0.19430) Loss: 0.22174 (0.19430)
2024-08-30,05:46:00 | INFO | Train Epoch: 60 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 528.931/s, 528.931/s/gpu LR: 3.584985e-09 Logit Scale: 100.000 Contrastive_loss: 0.16836 (0.19293) Loss: 0.16836 (0.19293)
2024-08-30,05:46:19 | INFO | Train Epoch: 60 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 529.815/s, 529.815/s/gpu LR: 3.574945e-09 Logit Scale: 100.000 Contrastive_loss: 0.19427 (0.19300) Loss: 0.19427 (0.19300)
2024-08-30,05:46:38 | INFO | Train Epoch: 60 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 533.257/s, 533.257/s/gpu LR: 3.564910e-09 Logit Scale: 100.000 Contrastive_loss: 0.23851 (0.19517) Loss: 0.23851 (0.19517)
2024-08-30,05:46:58 | INFO | Train Epoch: 60 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 501.165/s, 501.165/s/gpu LR: 3.554882e-09 Logit Scale: 100.000 Contrastive_loss: 0.16131 (0.19363) Loss: 0.16131 (0.19363)
2024-08-30,05:47:17 | INFO | Train Epoch: 60 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 523.507/s, 523.507/s/gpu LR: 3.544860e-09 Logit Scale: 100.000 Contrastive_loss: 0.24545 (0.19588) Loss: 0.24545 (0.19588)
2024-08-30,05:47:36 | INFO | Train Epoch: 60 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.192, 495.517/s, 495.517/s/gpu LR: 3.534844e-09 Logit Scale: 100.000 Contrastive_loss: 0.30031 (0.20023) Loss: 0.30031 (0.20023)
2024-08-30,05:47:55 | INFO | Train Epoch: 60 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.194, 504.136/s, 504.136/s/gpu LR: 3.524835e-09 Logit Scale: 100.000 Contrastive_loss: 0.29419 (0.20399) Loss: 0.29419 (0.20399)
2024-08-30,05:48:15 | INFO | Train Epoch: 60 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 491.245/s, 491.245/s/gpu LR: 3.514832e-09 Logit Scale: 100.000 Contrastive_loss: 0.15827 (0.20223) Loss: 0.15827 (0.20223)
2024-08-30,05:48:34 | INFO | Train Epoch: 60 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 493.430/s, 493.430/s/gpu LR: 3.504836e-09 Logit Scale: 100.000 Contrastive_loss: 0.21230 (0.20260) Loss: 0.21230 (0.20260)
2024-08-30,05:48:53 | INFO | Train Epoch: 60 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 526.518/s, 526.518/s/gpu LR: 3.494847e-09 Logit Scale: 100.000 Contrastive_loss: 0.21307 (0.20298) Loss: 0.21307 (0.20298)
2024-08-30,05:49:13 | INFO | Train Epoch: 60 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 497.438/s, 497.438/s/gpu LR: 3.484864e-09 Logit Scale: 100.000 Contrastive_loss: 0.25145 (0.20465) Loss: 0.25145 (0.20465)
2024-08-30,05:49:32 | INFO | Train Epoch: 60 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 526.974/s, 526.974/s/gpu LR: 3.474887e-09 Logit Scale: 100.000 Contrastive_loss: 0.16560 (0.20335) Loss: 0.16560 (0.20335)
2024-08-30,05:49:51 | INFO | Train Epoch: 60 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 492.171/s, 492.171/s/gpu LR: 3.464918e-09 Logit Scale: 100.000 Contrastive_loss: 0.14282 (0.20140) Loss: 0.14282 (0.20140)
2024-08-30,05:50:10 | INFO | Train Epoch: 60 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 522.966/s, 522.966/s/gpu LR: 3.454955e-09 Logit Scale: 100.000 Contrastive_loss: 0.25660 (0.20312) Loss: 0.25660 (0.20312)
2024-08-30,05:50:30 | INFO | Train Epoch: 60 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 525.588/s, 525.588/s/gpu LR: 3.444999e-09 Logit Scale: 100.000 Contrastive_loss: 0.24078 (0.20426) Loss: 0.24078 (0.20426)
2024-08-30,05:50:49 | INFO | Train Epoch: 60 [320100/145000.0 (221%)] Data (t): 0.107 Batch (t): 0.192, 508.301/s, 508.301/s/gpu LR: 3.435049e-09 Logit Scale: 100.000 Contrastive_loss: 0.27469 (0.20633) Loss: 0.27469 (0.20633)
2024-08-30,05:51:08 | INFO | Train Epoch: 60 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 502.067/s, 502.067/s/gpu LR: 3.425107e-09 Logit Scale: 100.000 Contrastive_loss: 0.18245 (0.20565) Loss: 0.18245 (0.20565)
2024-08-30,05:51:27 | INFO | Train Epoch: 60 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 491.209/s, 491.209/s/gpu LR: 3.415171e-09 Logit Scale: 100.000 Contrastive_loss: 0.24045 (0.20662) Loss: 0.24045 (0.20662)
2024-08-30,05:51:47 | INFO | Train Epoch: 60 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 507.564/s, 507.564/s/gpu LR: 3.405243e-09 Logit Scale: 100.000 Contrastive_loss: 0.29348 (0.20897) Loss: 0.29348 (0.20897)
2024-08-30,05:52:06 | INFO | Train Epoch: 60 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 500.150/s, 500.150/s/gpu LR: 3.395321e-09 Logit Scale: 100.000 Contrastive_loss: 0.15051 (0.20743) Loss: 0.15051 (0.20743)
2024-08-30,05:52:25 | INFO | Train Epoch: 60 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 502.782/s, 502.782/s/gpu LR: 3.385407e-09 Logit Scale: 100.000 Contrastive_loss: 0.25309 (0.20860) Loss: 0.25309 (0.20860)
2024-08-30,05:52:45 | INFO | Train Epoch: 60 [380100/145000.0 (262%)] Data (t): 0.107 Batch (t): 0.192, 531.261/s, 531.261/s/gpu LR: 3.375499e-09 Logit Scale: 100.000 Contrastive_loss: 0.23022 (0.20914) Loss: 0.23022 (0.20914)
2024-08-30,05:53:04 | INFO | Train Epoch: 60 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 525.151/s, 525.151/s/gpu LR: 3.365599e-09 Logit Scale: 100.000 Contrastive_loss: 0.12729 (0.20714) Loss: 0.12729 (0.20714)
2024-08-30,05:53:23 | INFO | Train Epoch: 60 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 518.871/s, 518.871/s/gpu LR: 3.355706e-09 Logit Scale: 100.000 Contrastive_loss: 0.21814 (0.20740) Loss: 0.21814 (0.20740)
2024-08-30,05:53:42 | INFO | Train Epoch: 60 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 525.358/s, 525.358/s/gpu LR: 3.345820e-09 Logit Scale: 100.000 Contrastive_loss: 0.085789 (0.20458) Loss: 0.085789 (0.20458)
2024-08-30,05:53:50 | INFO | Eval Epoch: 61 [200 / 1000]	Clip Loss: 0.482753	
2024-08-30,05:53:51 | INFO | Eval Epoch: 61 image_to_text_mean_rank: 2.4080	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6690	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9010	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9060	text_to_image_R@10: 0.9660	clip_val_loss: 0.4397	epoch: 61.0000	num_samples: 1000.0000
2024-08-30,05:53:52 | INFO | Start epoch 61
2024-08-30,05:53:52 | INFO | Train Epoch: 61 [   100/145000.0 (0%)] Data (t): 0.034 Batch (t): 0.118, 847.517/s, 847.517/s/gpu LR: 3.610115e-09 Logit Scale: 100.000 Contrastive_loss: 0.16494 (0.16494) Loss: 0.16494 (0.16494)
2024-08-30,05:54:11 | INFO | Train Epoch: 61 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 496.495/s, 496.495/s/gpu LR: 3.600058e-09 Logit Scale: 100.000 Contrastive_loss: 0.33133 (0.24813) Loss: 0.33133 (0.24813)
2024-08-30,05:54:31 | INFO | Train Epoch: 61 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.193, 525.522/s, 525.522/s/gpu LR: 3.590008e-09 Logit Scale: 100.000 Contrastive_loss: 0.34008 (0.27878) Loss: 0.34008 (0.27878)
2024-08-30,05:54:50 | INFO | Train Epoch: 61 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.194, 526.676/s, 526.676/s/gpu LR: 3.579964e-09 Logit Scale: 100.000 Contrastive_loss: 0.17556 (0.25298) Loss: 0.17556 (0.25298)
2024-08-30,05:55:09 | INFO | Train Epoch: 61 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.194, 494.281/s, 494.281/s/gpu LR: 3.569927e-09 Logit Scale: 100.000 Contrastive_loss: 0.13061 (0.22850) Loss: 0.13061 (0.22850)
2024-08-30,05:55:29 | INFO | Train Epoch: 61 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 507.167/s, 507.167/s/gpu LR: 3.559895e-09 Logit Scale: 100.000 Contrastive_loss: 0.15421 (0.21612) Loss: 0.15421 (0.21612)
2024-08-30,05:55:48 | INFO | Train Epoch: 61 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 530.851/s, 530.851/s/gpu LR: 3.549870e-09 Logit Scale: 100.000 Contrastive_loss: 0.11386 (0.20151) Loss: 0.11386 (0.20151)
2024-08-30,05:56:07 | INFO | Train Epoch: 61 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 522.180/s, 522.180/s/gpu LR: 3.539851e-09 Logit Scale: 100.000 Contrastive_loss: 0.21123 (0.20273) Loss: 0.21123 (0.20273)
2024-08-30,05:56:27 | INFO | Train Epoch: 61 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 502.393/s, 502.393/s/gpu LR: 3.529839e-09 Logit Scale: 100.000 Contrastive_loss: 0.12610 (0.19421) Loss: 0.12610 (0.19421)
2024-08-30,05:56:46 | INFO | Train Epoch: 61 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 530.624/s, 530.624/s/gpu LR: 3.519833e-09 Logit Scale: 100.000 Contrastive_loss: 0.20752 (0.19554) Loss: 0.20752 (0.19554)
2024-08-30,05:57:05 | INFO | Train Epoch: 61 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 524.233/s, 524.233/s/gpu LR: 3.509834e-09 Logit Scale: 100.000 Contrastive_loss: 0.14837 (0.19125) Loss: 0.14837 (0.19125)
2024-08-30,05:57:24 | INFO | Train Epoch: 61 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 533.424/s, 533.424/s/gpu LR: 3.499841e-09 Logit Scale: 100.000 Contrastive_loss: 0.16616 (0.18916) Loss: 0.16616 (0.18916)
2024-08-30,05:57:44 | INFO | Train Epoch: 61 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.192, 536.469/s, 536.469/s/gpu LR: 3.489854e-09 Logit Scale: 100.000 Contrastive_loss: 0.26766 (0.19520) Loss: 0.26766 (0.19520)
2024-08-30,05:58:03 | INFO | Train Epoch: 61 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 546.832/s, 546.832/s/gpu LR: 3.479875e-09 Logit Scale: 100.000 Contrastive_loss: 0.14897 (0.19190) Loss: 0.14897 (0.19190)
2024-08-30,05:58:22 | INFO | Train Epoch: 61 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 534.694/s, 534.694/s/gpu LR: 3.469902e-09 Logit Scale: 100.000 Contrastive_loss: 0.17513 (0.19078) Loss: 0.17513 (0.19078)
2024-08-30,05:58:32 | INFO | Train Epoch: 61 [145000/145000.0 (100%)] Data (t): 0.107 Batch (t): 0.192, 525.711/s, 525.711/s/gpu LR: 3.465017e-09 Logit Scale: 100.000 Contrastive_loss: 0.21323 (0.19218) Loss: 0.21323 (0.19218)
2024-08-30,05:58:42 | INFO | Train Epoch: 61 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 538.756/s, 538.756/s/gpu LR: 3.459935e-09 Logit Scale: 100.000 Contrastive_loss: 0.19152 (0.19215) Loss: 0.19152 (0.19215)
2024-08-30,05:59:01 | INFO | Train Epoch: 61 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.194, 511.685/s, 511.685/s/gpu LR: 3.449976e-09 Logit Scale: 100.000 Contrastive_loss: 0.22105 (0.19375) Loss: 0.22105 (0.19375)
2024-08-30,05:59:20 | INFO | Train Epoch: 61 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 532.059/s, 532.059/s/gpu LR: 3.440023e-09 Logit Scale: 100.000 Contrastive_loss: 0.16785 (0.19239) Loss: 0.16785 (0.19239)
2024-08-30,05:59:39 | INFO | Train Epoch: 61 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 501.958/s, 501.958/s/gpu LR: 3.430077e-09 Logit Scale: 100.000 Contrastive_loss: 0.19405 (0.19247) Loss: 0.19405 (0.19247)
2024-08-30,05:59:59 | INFO | Train Epoch: 61 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.192, 503.226/s, 503.226/s/gpu LR: 3.420138e-09 Logit Scale: 100.000 Contrastive_loss: 0.23795 (0.19464) Loss: 0.23795 (0.19464)
2024-08-30,06:00:18 | INFO | Train Epoch: 61 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 526.730/s, 526.730/s/gpu LR: 3.410206e-09 Logit Scale: 100.000 Contrastive_loss: 0.16104 (0.19311) Loss: 0.16104 (0.19311)
2024-08-30,06:00:37 | INFO | Train Epoch: 61 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 507.491/s, 507.491/s/gpu LR: 3.400281e-09 Logit Scale: 100.000 Contrastive_loss: 0.24497 (0.19536) Loss: 0.24497 (0.19536)
2024-08-30,06:00:56 | INFO | Train Epoch: 61 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 519.144/s, 519.144/s/gpu LR: 3.390363e-09 Logit Scale: 100.000 Contrastive_loss: 0.29955 (0.19971) Loss: 0.29955 (0.19971)
2024-08-30,06:01:16 | INFO | Train Epoch: 61 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 499.337/s, 499.337/s/gpu LR: 3.380452e-09 Logit Scale: 100.000 Contrastive_loss: 0.29362 (0.20346) Loss: 0.29362 (0.20346)
2024-08-30,06:01:35 | INFO | Train Epoch: 61 [240100/145000.0 (166%)] Data (t): 0.110 Batch (t): 0.194, 536.324/s, 536.324/s/gpu LR: 3.370548e-09 Logit Scale: 100.000 Contrastive_loss: 0.15768 (0.20170) Loss: 0.15768 (0.20170)
2024-08-30,06:01:55 | INFO | Train Epoch: 61 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 496.564/s, 496.564/s/gpu LR: 3.360651e-09 Logit Scale: 100.000 Contrastive_loss: 0.21186 (0.20208) Loss: 0.21186 (0.20208)
2024-08-30,06:02:14 | INFO | Train Epoch: 61 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 539.149/s, 539.149/s/gpu LR: 3.350762e-09 Logit Scale: 100.000 Contrastive_loss: 0.21244 (0.20245) Loss: 0.21244 (0.20245)
2024-08-30,06:02:33 | INFO | Train Epoch: 61 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.192, 502.988/s, 502.988/s/gpu LR: 3.340879e-09 Logit Scale: 100.000 Contrastive_loss: 0.25106 (0.20412) Loss: 0.25106 (0.20412)
2024-08-30,06:02:52 | INFO | Train Epoch: 61 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 498.983/s, 498.983/s/gpu LR: 3.331004e-09 Logit Scale: 100.000 Contrastive_loss: 0.16535 (0.20283) Loss: 0.16535 (0.20283)
2024-08-30,06:03:12 | INFO | Train Epoch: 61 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 496.768/s, 496.768/s/gpu LR: 3.321137e-09 Logit Scale: 100.000 Contrastive_loss: 0.14260 (0.20089) Loss: 0.14260 (0.20089)
2024-08-30,06:03:31 | INFO | Train Epoch: 61 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 509.508/s, 509.508/s/gpu LR: 3.311276e-09 Logit Scale: 100.000 Contrastive_loss: 0.25606 (0.20261) Loss: 0.25606 (0.20261)
2024-08-30,06:03:50 | INFO | Train Epoch: 61 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 534.823/s, 534.823/s/gpu LR: 3.301423e-09 Logit Scale: 100.000 Contrastive_loss: 0.24049 (0.20376) Loss: 0.24049 (0.20376)
2024-08-30,06:04:09 | INFO | Train Epoch: 61 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.192, 501.298/s, 501.298/s/gpu LR: 3.291578e-09 Logit Scale: 100.000 Contrastive_loss: 0.27408 (0.20583) Loss: 0.27408 (0.20583)
2024-08-30,06:04:29 | INFO | Train Epoch: 61 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 492.951/s, 492.951/s/gpu LR: 3.281740e-09 Logit Scale: 100.000 Contrastive_loss: 0.18212 (0.20515) Loss: 0.18212 (0.20515)
2024-08-30,06:04:48 | INFO | Train Epoch: 61 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.193, 499.666/s, 499.666/s/gpu LR: 3.271909e-09 Logit Scale: 100.000 Contrastive_loss: 0.23948 (0.20610) Loss: 0.23948 (0.20610)
2024-08-30,06:05:07 | INFO | Train Epoch: 61 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.193, 527.109/s, 527.109/s/gpu LR: 3.262086e-09 Logit Scale: 100.000 Contrastive_loss: 0.29291 (0.20845) Loss: 0.29291 (0.20845)
2024-08-30,06:05:26 | INFO | Train Epoch: 61 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 534.042/s, 534.042/s/gpu LR: 3.252271e-09 Logit Scale: 100.000 Contrastive_loss: 0.15009 (0.20691) Loss: 0.15009 (0.20691)
2024-08-30,06:05:46 | INFO | Train Epoch: 61 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 523.928/s, 523.928/s/gpu LR: 3.242463e-09 Logit Scale: 100.000 Contrastive_loss: 0.25254 (0.20808) Loss: 0.25254 (0.20808)
2024-08-30,06:06:05 | INFO | Train Epoch: 61 [380100/145000.0 (262%)] Data (t): 0.107 Batch (t): 0.192, 532.155/s, 532.155/s/gpu LR: 3.232663e-09 Logit Scale: 100.000 Contrastive_loss: 0.22971 (0.20863) Loss: 0.22971 (0.20863)
2024-08-30,06:06:24 | INFO | Train Epoch: 61 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.193, 528.103/s, 528.103/s/gpu LR: 3.222871e-09 Logit Scale: 100.000 Contrastive_loss: 0.12675 (0.20663) Loss: 0.12675 (0.20663)
2024-08-30,06:06:44 | INFO | Train Epoch: 61 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 496.290/s, 496.290/s/gpu LR: 3.213087e-09 Logit Scale: 100.000 Contrastive_loss: 0.21778 (0.20689) Loss: 0.21778 (0.20689)
2024-08-30,06:07:03 | INFO | Train Epoch: 61 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.194, 525.331/s, 525.331/s/gpu LR: 3.203310e-09 Logit Scale: 100.000 Contrastive_loss: 0.085518 (0.20407) Loss: 0.085518 (0.20407)
2024-08-30,06:07:10 | INFO | Eval Epoch: 62 [200 / 1000]	Clip Loss: 0.482857	
2024-08-30,06:07:11 | INFO | Eval Epoch: 62 image_to_text_mean_rank: 2.4090	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6690	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9000	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6730	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4398	epoch: 62.0000	num_samples: 1000.0000
2024-08-30,06:07:12 | INFO | Start epoch 62
2024-08-30,06:07:13 | INFO | Train Epoch: 62 [   100/145000.0 (0%)] Data (t): 0.026 Batch (t): 0.112, 892.861/s, 892.861/s/gpu LR: 3.464918e-09 Logit Scale: 100.000 Contrastive_loss: 0.16436 (0.16436) Loss: 0.16436 (0.16436)
2024-08-30,06:07:32 | INFO | Train Epoch: 62 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 525.791/s, 525.791/s/gpu LR: 3.454955e-09 Logit Scale: 100.000 Contrastive_loss: 0.33079 (0.24758) Loss: 0.33079 (0.24758)
2024-08-30,06:07:51 | INFO | Train Epoch: 62 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 501.157/s, 501.157/s/gpu LR: 3.444999e-09 Logit Scale: 100.000 Contrastive_loss: 0.33936 (0.27817) Loss: 0.33936 (0.27817)
2024-08-30,06:08:11 | INFO | Train Epoch: 62 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.194, 525.637/s, 525.637/s/gpu LR: 3.435049e-09 Logit Scale: 100.000 Contrastive_loss: 0.17497 (0.25237) Loss: 0.17497 (0.25237)
2024-08-30,06:08:30 | INFO | Train Epoch: 62 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.192, 495.142/s, 495.142/s/gpu LR: 3.425107e-09 Logit Scale: 100.000 Contrastive_loss: 0.13037 (0.22797) Loss: 0.13037 (0.22797)
2024-08-30,06:08:49 | INFO | Train Epoch: 62 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 507.294/s, 507.294/s/gpu LR: 3.415171e-09 Logit Scale: 100.000 Contrastive_loss: 0.15372 (0.21560) Loss: 0.15372 (0.21560)
2024-08-30,06:09:08 | INFO | Train Epoch: 62 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 526.280/s, 526.280/s/gpu LR: 3.405243e-09 Logit Scale: 100.000 Contrastive_loss: 0.11355 (0.20102) Loss: 0.11355 (0.20102)
2024-08-30,06:09:28 | INFO | Train Epoch: 62 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 524.402/s, 524.402/s/gpu LR: 3.395321e-09 Logit Scale: 100.000 Contrastive_loss: 0.21051 (0.20220) Loss: 0.21051 (0.20220)
2024-08-30,06:09:47 | INFO | Train Epoch: 62 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 531.273/s, 531.273/s/gpu LR: 3.385407e-09 Logit Scale: 100.000 Contrastive_loss: 0.12566 (0.19370) Loss: 0.12566 (0.19370)
2024-08-30,06:10:06 | INFO | Train Epoch: 62 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 497.309/s, 497.309/s/gpu LR: 3.375499e-09 Logit Scale: 100.000 Contrastive_loss: 0.20707 (0.19504) Loss: 0.20707 (0.19504)
2024-08-30,06:10:26 | INFO | Train Epoch: 62 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 530.735/s, 530.735/s/gpu LR: 3.365599e-09 Logit Scale: 100.000 Contrastive_loss: 0.14811 (0.19077) Loss: 0.14811 (0.19077)
2024-08-30,06:10:45 | INFO | Train Epoch: 62 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.192, 497.984/s, 497.984/s/gpu LR: 3.355706e-09 Logit Scale: 100.000 Contrastive_loss: 0.16571 (0.18868) Loss: 0.16571 (0.18868)
2024-08-30,06:11:04 | INFO | Train Epoch: 62 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 537.429/s, 537.429/s/gpu LR: 3.345820e-09 Logit Scale: 100.000 Contrastive_loss: 0.26713 (0.19472) Loss: 0.26713 (0.19472)
2024-08-30,06:11:23 | INFO | Train Epoch: 62 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 534.165/s, 534.165/s/gpu LR: 3.335941e-09 Logit Scale: 100.000 Contrastive_loss: 0.14855 (0.19142) Loss: 0.14855 (0.19142)
2024-08-30,06:11:43 | INFO | Train Epoch: 62 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 524.111/s, 524.111/s/gpu LR: 3.326070e-09 Logit Scale: 100.000 Contrastive_loss: 0.17470 (0.19030) Loss: 0.17470 (0.19030)
2024-08-30,06:11:52 | INFO | Train Epoch: 62 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 504.691/s, 504.691/s/gpu LR: 3.321235e-09 Logit Scale: 100.000 Contrastive_loss: 0.21232 (0.19168) Loss: 0.21232 (0.19168)
2024-08-30,06:12:02 | INFO | Train Epoch: 62 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.192, 507.544/s, 507.544/s/gpu LR: 3.316206e-09 Logit Scale: 100.000 Contrastive_loss: 0.19089 (0.19163) Loss: 0.19089 (0.19163)
2024-08-30,06:12:21 | INFO | Train Epoch: 62 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 500.532/s, 500.532/s/gpu LR: 3.306349e-09 Logit Scale: 100.000 Contrastive_loss: 0.22033 (0.19323) Loss: 0.22033 (0.19323)
2024-08-30,06:12:41 | INFO | Train Epoch: 62 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 503.063/s, 503.063/s/gpu LR: 3.296500e-09 Logit Scale: 100.000 Contrastive_loss: 0.16737 (0.19187) Loss: 0.16737 (0.19187)
2024-08-30,06:13:00 | INFO | Train Epoch: 62 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 499.154/s, 499.154/s/gpu LR: 3.286658e-09 Logit Scale: 100.000 Contrastive_loss: 0.19370 (0.19196) Loss: 0.19370 (0.19196)
2024-08-30,06:13:19 | INFO | Train Epoch: 62 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 499.047/s, 499.047/s/gpu LR: 3.276824e-09 Logit Scale: 100.000 Contrastive_loss: 0.23729 (0.19412) Loss: 0.23729 (0.19412)
2024-08-30,06:13:38 | INFO | Train Epoch: 62 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 524.519/s, 524.519/s/gpu LR: 3.266997e-09 Logit Scale: 100.000 Contrastive_loss: 0.16066 (0.19260) Loss: 0.16066 (0.19260)
2024-08-30,06:13:58 | INFO | Train Epoch: 62 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 499.031/s, 499.031/s/gpu LR: 3.257178e-09 Logit Scale: 100.000 Contrastive_loss: 0.24453 (0.19485) Loss: 0.24453 (0.19485)
2024-08-30,06:14:17 | INFO | Train Epoch: 62 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 504.408/s, 504.408/s/gpu LR: 3.247366e-09 Logit Scale: 100.000 Contrastive_loss: 0.29885 (0.19919) Loss: 0.29885 (0.19919)
2024-08-30,06:14:36 | INFO | Train Epoch: 62 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 527.656/s, 527.656/s/gpu LR: 3.237562e-09 Logit Scale: 100.000 Contrastive_loss: 0.29307 (0.20294) Loss: 0.29307 (0.20294)
2024-08-30,06:14:55 | INFO | Train Epoch: 62 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 494.021/s, 494.021/s/gpu LR: 3.227766e-09 Logit Scale: 100.000 Contrastive_loss: 0.15712 (0.20118) Loss: 0.15712 (0.20118)
2024-08-30,06:15:15 | INFO | Train Epoch: 62 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 494.807/s, 494.807/s/gpu LR: 3.217978e-09 Logit Scale: 100.000 Contrastive_loss: 0.21141 (0.20156) Loss: 0.21141 (0.20156)
2024-08-30,06:15:34 | INFO | Train Epoch: 62 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 528.281/s, 528.281/s/gpu LR: 3.208198e-09 Logit Scale: 100.000 Contrastive_loss: 0.21209 (0.20194) Loss: 0.21209 (0.20194)
2024-08-30,06:15:53 | INFO | Train Epoch: 62 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 509.318/s, 509.318/s/gpu LR: 3.198425e-09 Logit Scale: 100.000 Contrastive_loss: 0.25053 (0.20361) Loss: 0.25053 (0.20361)
2024-08-30,06:16:12 | INFO | Train Epoch: 62 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.192, 523.617/s, 523.617/s/gpu LR: 3.188660e-09 Logit Scale: 100.000 Contrastive_loss: 0.16503 (0.20232) Loss: 0.16503 (0.20232)
2024-08-30,06:16:32 | INFO | Train Epoch: 62 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 495.058/s, 495.058/s/gpu LR: 3.178904e-09 Logit Scale: 100.000 Contrastive_loss: 0.14237 (0.20039) Loss: 0.14237 (0.20039)
2024-08-30,06:16:51 | INFO | Train Epoch: 62 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 532.635/s, 532.635/s/gpu LR: 3.169155e-09 Logit Scale: 100.000 Contrastive_loss: 0.25549 (0.20211) Loss: 0.25549 (0.20211)
2024-08-30,06:17:10 | INFO | Train Epoch: 62 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 498.530/s, 498.530/s/gpu LR: 3.159414e-09 Logit Scale: 100.000 Contrastive_loss: 0.24020 (0.20327) Loss: 0.24020 (0.20327)
2024-08-30,06:17:29 | INFO | Train Epoch: 62 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 496.849/s, 496.849/s/gpu LR: 3.149682e-09 Logit Scale: 100.000 Contrastive_loss: 0.27340 (0.20533) Loss: 0.27340 (0.20533)
2024-08-30,06:17:49 | INFO | Train Epoch: 62 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.193, 528.499/s, 528.499/s/gpu LR: 3.139957e-09 Logit Scale: 100.000 Contrastive_loss: 0.18181 (0.20466) Loss: 0.18181 (0.20466)
2024-08-30,06:18:08 | INFO | Train Epoch: 62 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 492.775/s, 492.775/s/gpu LR: 3.130241e-09 Logit Scale: 100.000 Contrastive_loss: 0.23881 (0.20561) Loss: 0.23881 (0.20561)
2024-08-30,06:18:27 | INFO | Train Epoch: 62 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 505.826/s, 505.826/s/gpu LR: 3.120533e-09 Logit Scale: 100.000 Contrastive_loss: 0.29242 (0.20795) Loss: 0.29242 (0.20795)
2024-08-30,06:18:47 | INFO | Train Epoch: 62 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.194, 534.196/s, 534.196/s/gpu LR: 3.110833e-09 Logit Scale: 100.000 Contrastive_loss: 0.14971 (0.20642) Loss: 0.14971 (0.20642)
2024-08-30,06:19:06 | INFO | Train Epoch: 62 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 488.897/s, 488.897/s/gpu LR: 3.101141e-09 Logit Scale: 100.000 Contrastive_loss: 0.25196 (0.20759) Loss: 0.25196 (0.20759)
2024-08-30,06:19:25 | INFO | Train Epoch: 62 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 530.130/s, 530.130/s/gpu LR: 3.091458e-09 Logit Scale: 100.000 Contrastive_loss: 0.22919 (0.20813) Loss: 0.22919 (0.20813)
2024-08-30,06:19:45 | INFO | Train Epoch: 62 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.193, 506.052/s, 506.052/s/gpu LR: 3.081783e-09 Logit Scale: 100.000 Contrastive_loss: 0.12630 (0.20613) Loss: 0.12630 (0.20613)
2024-08-30,06:20:04 | INFO | Train Epoch: 62 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 533.762/s, 533.762/s/gpu LR: 3.072116e-09 Logit Scale: 100.000 Contrastive_loss: 0.21721 (0.20640) Loss: 0.21721 (0.20640)
2024-08-30,06:20:23 | INFO | Train Epoch: 62 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 529.520/s, 529.520/s/gpu LR: 3.062458e-09 Logit Scale: 100.000 Contrastive_loss: 0.085171 (0.20358) Loss: 0.085171 (0.20358)
2024-08-30,06:20:31 | INFO | Eval Epoch: 63 [200 / 1000]	Clip Loss: 0.482953	
2024-08-30,06:20:31 | INFO | Eval Epoch: 63 image_to_text_mean_rank: 2.4100	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6700	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9080	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6720	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4398	epoch: 63.0000	num_samples: 1000.0000
2024-08-30,06:20:33 | INFO | Start epoch 63
2024-08-30,06:20:33 | INFO | Train Epoch: 63 [   100/145000.0 (0%)] Data (t): 0.023 Batch (t): 0.106, 945.707/s, 945.707/s/gpu LR: 3.321137e-09 Logit Scale: 100.000 Contrastive_loss: 0.16415 (0.16415) Loss: 0.16415 (0.16415)
2024-08-30,06:20:52 | INFO | Train Epoch: 63 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 534.943/s, 534.943/s/gpu LR: 3.311276e-09 Logit Scale: 100.000 Contrastive_loss: 0.33028 (0.24721) Loss: 0.33028 (0.24721)
2024-08-30,06:21:12 | INFO | Train Epoch: 63 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 493.865/s, 493.865/s/gpu LR: 3.301423e-09 Logit Scale: 100.000 Contrastive_loss: 0.33835 (0.27759) Loss: 0.33835 (0.27759)
2024-08-30,06:21:31 | INFO | Train Epoch: 63 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 522.147/s, 522.147/s/gpu LR: 3.291578e-09 Logit Scale: 100.000 Contrastive_loss: 0.17447 (0.25181) Loss: 0.17447 (0.25181)
2024-08-30,06:21:50 | INFO | Train Epoch: 63 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 535.937/s, 535.937/s/gpu LR: 3.281740e-09 Logit Scale: 100.000 Contrastive_loss: 0.13013 (0.22748) Loss: 0.13013 (0.22748)
2024-08-30,06:22:09 | INFO | Train Epoch: 63 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 499.571/s, 499.571/s/gpu LR: 3.271909e-09 Logit Scale: 100.000 Contrastive_loss: 0.15316 (0.21509) Loss: 0.15316 (0.21509)
2024-08-30,06:22:29 | INFO | Train Epoch: 63 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 524.379/s, 524.379/s/gpu LR: 3.262086e-09 Logit Scale: 100.000 Contrastive_loss: 0.11322 (0.20054) Loss: 0.11322 (0.20054)
2024-08-30,06:22:48 | INFO | Train Epoch: 63 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 514.105/s, 514.105/s/gpu LR: 3.252271e-09 Logit Scale: 100.000 Contrastive_loss: 0.20965 (0.20167) Loss: 0.20965 (0.20167)
2024-08-30,06:23:07 | INFO | Train Epoch: 63 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.192, 495.537/s, 495.537/s/gpu LR: 3.242463e-09 Logit Scale: 100.000 Contrastive_loss: 0.12527 (0.19319) Loss: 0.12527 (0.19319)
2024-08-30,06:23:27 | INFO | Train Epoch: 63 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.192, 503.562/s, 503.562/s/gpu LR: 3.232663e-09 Logit Scale: 100.000 Contrastive_loss: 0.20646 (0.19451) Loss: 0.20646 (0.19451)
2024-08-30,06:23:46 | INFO | Train Epoch: 63 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.194, 494.379/s, 494.379/s/gpu LR: 3.222871e-09 Logit Scale: 100.000 Contrastive_loss: 0.14780 (0.19027) Loss: 0.14780 (0.19027)
2024-08-30,06:24:05 | INFO | Train Epoch: 63 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.192, 528.809/s, 528.809/s/gpu LR: 3.213087e-09 Logit Scale: 100.000 Contrastive_loss: 0.16537 (0.18819) Loss: 0.16537 (0.18819)
2024-08-30,06:24:24 | INFO | Train Epoch: 63 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 511.473/s, 511.473/s/gpu LR: 3.203310e-09 Logit Scale: 100.000 Contrastive_loss: 0.26674 (0.19423) Loss: 0.26674 (0.19423)
2024-08-30,06:24:44 | INFO | Train Epoch: 63 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 494.510/s, 494.510/s/gpu LR: 3.193542e-09 Logit Scale: 100.000 Contrastive_loss: 0.14828 (0.19095) Loss: 0.14828 (0.19095)
2024-08-30,06:25:03 | INFO | Train Epoch: 63 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.192, 537.374/s, 537.374/s/gpu LR: 3.183781e-09 Logit Scale: 100.000 Contrastive_loss: 0.17460 (0.18986) Loss: 0.17460 (0.18986)
2024-08-30,06:25:12 | INFO | Train Epoch: 63 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.192, 528.933/s, 528.933/s/gpu LR: 3.179001e-09 Logit Scale: 100.000 Contrastive_loss: 0.21169 (0.19123) Loss: 0.21169 (0.19123)
2024-08-30,06:25:22 | INFO | Train Epoch: 63 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 527.766/s, 527.766/s/gpu LR: 3.174028e-09 Logit Scale: 100.000 Contrastive_loss: 0.19042 (0.19118) Loss: 0.19042 (0.19118)
2024-08-30,06:25:42 | INFO | Train Epoch: 63 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.194, 535.178/s, 535.178/s/gpu LR: 3.164284e-09 Logit Scale: 100.000 Contrastive_loss: 0.21955 (0.19275) Loss: 0.21955 (0.19275)
2024-08-30,06:26:01 | INFO | Train Epoch: 63 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 526.063/s, 526.063/s/gpu LR: 3.154547e-09 Logit Scale: 100.000 Contrastive_loss: 0.16691 (0.19139) Loss: 0.16691 (0.19139)
2024-08-30,06:26:20 | INFO | Train Epoch: 63 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 522.495/s, 522.495/s/gpu LR: 3.144818e-09 Logit Scale: 100.000 Contrastive_loss: 0.19349 (0.19150) Loss: 0.19349 (0.19150)
2024-08-30,06:26:39 | INFO | Train Epoch: 63 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 530.565/s, 530.565/s/gpu LR: 3.135098e-09 Logit Scale: 100.000 Contrastive_loss: 0.23687 (0.19366) Loss: 0.23687 (0.19366)
2024-08-30,06:26:59 | INFO | Train Epoch: 63 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 509.173/s, 509.173/s/gpu LR: 3.125386e-09 Logit Scale: 100.000 Contrastive_loss: 0.16038 (0.19215) Loss: 0.16038 (0.19215)
2024-08-30,06:27:18 | INFO | Train Epoch: 63 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 538.053/s, 538.053/s/gpu LR: 3.115682e-09 Logit Scale: 100.000 Contrastive_loss: 0.24408 (0.19440) Loss: 0.24408 (0.19440)
2024-08-30,06:27:37 | INFO | Train Epoch: 63 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 493.907/s, 493.907/s/gpu LR: 3.105986e-09 Logit Scale: 100.000 Contrastive_loss: 0.29811 (0.19873) Loss: 0.29811 (0.19873)
2024-08-30,06:27:56 | INFO | Train Epoch: 63 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 527.430/s, 527.430/s/gpu LR: 3.096298e-09 Logit Scale: 100.000 Contrastive_loss: 0.29252 (0.20248) Loss: 0.29252 (0.20248)
2024-08-30,06:28:16 | INFO | Train Epoch: 63 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 497.241/s, 497.241/s/gpu LR: 3.086619e-09 Logit Scale: 100.000 Contrastive_loss: 0.15648 (0.20071) Loss: 0.15648 (0.20071)
2024-08-30,06:28:35 | INFO | Train Epoch: 63 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 496.452/s, 496.452/s/gpu LR: 3.076948e-09 Logit Scale: 100.000 Contrastive_loss: 0.21119 (0.20110) Loss: 0.21119 (0.20110)
2024-08-30,06:28:54 | INFO | Train Epoch: 63 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.192, 531.939/s, 531.939/s/gpu LR: 3.067286e-09 Logit Scale: 100.000 Contrastive_loss: 0.21160 (0.20147) Loss: 0.21160 (0.20147)
2024-08-30,06:29:13 | INFO | Train Epoch: 63 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.192, 523.415/s, 523.415/s/gpu LR: 3.057632e-09 Logit Scale: 100.000 Contrastive_loss: 0.25020 (0.20315) Loss: 0.25020 (0.20315)
2024-08-30,06:29:33 | INFO | Train Epoch: 63 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.192, 526.690/s, 526.690/s/gpu LR: 3.047987e-09 Logit Scale: 100.000 Contrastive_loss: 0.16456 (0.20187) Loss: 0.16456 (0.20187)
2024-08-30,06:29:52 | INFO | Train Epoch: 63 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.194, 490.803/s, 490.803/s/gpu LR: 3.038350e-09 Logit Scale: 100.000 Contrastive_loss: 0.14220 (0.19994) Loss: 0.14220 (0.19994)
2024-08-30,06:30:11 | INFO | Train Epoch: 63 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 532.182/s, 532.182/s/gpu LR: 3.028722e-09 Logit Scale: 100.000 Contrastive_loss: 0.25503 (0.20166) Loss: 0.25503 (0.20166)
2024-08-30,06:30:31 | INFO | Train Epoch: 63 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 524.939/s, 524.939/s/gpu LR: 3.019103e-09 Logit Scale: 100.000 Contrastive_loss: 0.23996 (0.20282) Loss: 0.23996 (0.20282)
2024-08-30,06:30:50 | INFO | Train Epoch: 63 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.194, 493.204/s, 493.204/s/gpu LR: 3.009492e-09 Logit Scale: 100.000 Contrastive_loss: 0.27287 (0.20488) Loss: 0.27287 (0.20488)
2024-08-30,06:31:09 | INFO | Train Epoch: 63 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.193, 497.423/s, 497.423/s/gpu LR: 2.999890e-09 Logit Scale: 100.000 Contrastive_loss: 0.18159 (0.20422) Loss: 0.18159 (0.20422)
2024-08-30,06:31:29 | INFO | Train Epoch: 63 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 558.187/s, 558.187/s/gpu LR: 2.990296e-09 Logit Scale: 100.000 Contrastive_loss: 0.23796 (0.20515) Loss: 0.23796 (0.20515)
2024-08-30,06:31:48 | INFO | Train Epoch: 63 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 504.704/s, 504.704/s/gpu LR: 2.980712e-09 Logit Scale: 100.000 Contrastive_loss: 0.29195 (0.20750) Loss: 0.29195 (0.20750)
2024-08-30,06:32:07 | INFO | Train Epoch: 63 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 527.656/s, 527.656/s/gpu LR: 2.971136e-09 Logit Scale: 100.000 Contrastive_loss: 0.14927 (0.20597) Loss: 0.14927 (0.20597)
2024-08-30,06:32:26 | INFO | Train Epoch: 63 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 529.308/s, 529.308/s/gpu LR: 2.961570e-09 Logit Scale: 100.000 Contrastive_loss: 0.25146 (0.20713) Loss: 0.25146 (0.20713)
2024-08-30,06:32:46 | INFO | Train Epoch: 63 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 505.349/s, 505.349/s/gpu LR: 2.952012e-09 Logit Scale: 100.000 Contrastive_loss: 0.22863 (0.20767) Loss: 0.22863 (0.20767)
2024-08-30,06:33:05 | INFO | Train Epoch: 63 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 489.391/s, 489.391/s/gpu LR: 2.942463e-09 Logit Scale: 100.000 Contrastive_loss: 0.12594 (0.20568) Loss: 0.12594 (0.20568)
2024-08-30,06:33:24 | INFO | Train Epoch: 63 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 496.981/s, 496.981/s/gpu LR: 2.932923e-09 Logit Scale: 100.000 Contrastive_loss: 0.21678 (0.20594) Loss: 0.21678 (0.20594)
2024-08-30,06:33:44 | INFO | Train Epoch: 63 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 534.151/s, 534.151/s/gpu LR: 2.923393e-09 Logit Scale: 100.000 Contrastive_loss: 0.084968 (0.20313) Loss: 0.084968 (0.20313)
2024-08-30,06:33:51 | INFO | Eval Epoch: 64 [200 / 1000]	Clip Loss: 0.483031	
2024-08-30,06:33:52 | INFO | Eval Epoch: 64 image_to_text_mean_rank: 2.4080	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6700	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9100	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4399	epoch: 64.0000	num_samples: 1000.0000
2024-08-30,06:33:53 | INFO | Start epoch 64
2024-08-30,06:33:53 | INFO | Train Epoch: 64 [   100/145000.0 (0%)] Data (t): 0.019 Batch (t): 0.109, 916.499/s, 916.499/s/gpu LR: 3.178904e-09 Logit Scale: 100.000 Contrastive_loss: 0.16354 (0.16354) Loss: 0.16354 (0.16354)
2024-08-30,06:34:13 | INFO | Train Epoch: 64 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 532.258/s, 532.258/s/gpu LR: 3.169155e-09 Logit Scale: 100.000 Contrastive_loss: 0.32983 (0.24668) Loss: 0.32983 (0.24668)
2024-08-30,06:34:32 | INFO | Train Epoch: 64 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.192, 523.006/s, 523.006/s/gpu LR: 3.159414e-09 Logit Scale: 100.000 Contrastive_loss: 0.33758 (0.27698) Loss: 0.33758 (0.27698)
2024-08-30,06:34:51 | INFO | Train Epoch: 64 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 528.970/s, 528.970/s/gpu LR: 3.149682e-09 Logit Scale: 100.000 Contrastive_loss: 0.17399 (0.25123) Loss: 0.17399 (0.25123)
2024-08-30,06:35:11 | INFO | Train Epoch: 64 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.194, 491.498/s, 491.498/s/gpu LR: 3.139957e-09 Logit Scale: 100.000 Contrastive_loss: 0.12986 (0.22696) Loss: 0.12986 (0.22696)
2024-08-30,06:35:30 | INFO | Train Epoch: 64 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 505.523/s, 505.523/s/gpu LR: 3.130241e-09 Logit Scale: 100.000 Contrastive_loss: 0.15273 (0.21459) Loss: 0.15273 (0.21459)
2024-08-30,06:35:49 | INFO | Train Epoch: 64 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 502.924/s, 502.924/s/gpu LR: 3.120533e-09 Logit Scale: 100.000 Contrastive_loss: 0.11302 (0.20008) Loss: 0.11302 (0.20008)
2024-08-30,06:36:08 | INFO | Train Epoch: 64 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 532.521/s, 532.521/s/gpu LR: 3.110833e-09 Logit Scale: 100.000 Contrastive_loss: 0.20887 (0.20118) Loss: 0.20887 (0.20118)
2024-08-30,06:36:28 | INFO | Train Epoch: 64 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.192, 500.573/s, 500.573/s/gpu LR: 3.101141e-09 Logit Scale: 100.000 Contrastive_loss: 0.12486 (0.19270) Loss: 0.12486 (0.19270)
2024-08-30,06:36:47 | INFO | Train Epoch: 64 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 506.650/s, 506.650/s/gpu LR: 3.091458e-09 Logit Scale: 100.000 Contrastive_loss: 0.20618 (0.19405) Loss: 0.20618 (0.19405)
2024-08-30,06:37:06 | INFO | Train Epoch: 64 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 519.307/s, 519.307/s/gpu LR: 3.081783e-09 Logit Scale: 100.000 Contrastive_loss: 0.14736 (0.18980) Loss: 0.14736 (0.18980)
2024-08-30,06:37:26 | INFO | Train Epoch: 64 [110100/145000.0 (76%)] Data (t): 0.109 Batch (t): 0.193, 519.570/s, 519.570/s/gpu LR: 3.072116e-09 Logit Scale: 100.000 Contrastive_loss: 0.16499 (0.18773) Loss: 0.16499 (0.18773)
2024-08-30,06:37:45 | INFO | Train Epoch: 64 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 503.078/s, 503.078/s/gpu LR: 3.062458e-09 Logit Scale: 100.000 Contrastive_loss: 0.26620 (0.19377) Loss: 0.26620 (0.19377)
2024-08-30,06:38:04 | INFO | Train Epoch: 64 [130100/145000.0 (90%)] Data (t): 0.107 Batch (t): 0.192, 526.922/s, 526.922/s/gpu LR: 3.052809e-09 Logit Scale: 100.000 Contrastive_loss: 0.14797 (0.19050) Loss: 0.14797 (0.19050)
2024-08-30,06:38:23 | INFO | Train Epoch: 64 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 539.437/s, 539.437/s/gpu LR: 3.043168e-09 Logit Scale: 100.000 Contrastive_loss: 0.17430 (0.18942) Loss: 0.17430 (0.18942)
2024-08-30,06:38:33 | INFO | Train Epoch: 64 [145000/145000.0 (100%)] Data (t): 0.110 Batch (t): 0.194, 528.068/s, 528.068/s/gpu LR: 3.038447e-09 Logit Scale: 100.000 Contrastive_loss: 0.21066 (0.19075) Loss: 0.21066 (0.19075)
2024-08-30,06:38:43 | INFO | Train Epoch: 64 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 530.048/s, 530.048/s/gpu LR: 3.033535e-09 Logit Scale: 100.000 Contrastive_loss: 0.19003 (0.19071) Loss: 0.19003 (0.19071)
2024-08-30,06:39:02 | INFO | Train Epoch: 64 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.192, 535.395/s, 535.395/s/gpu LR: 3.023911e-09 Logit Scale: 100.000 Contrastive_loss: 0.21893 (0.19227) Loss: 0.21893 (0.19227)
2024-08-30,06:39:21 | INFO | Train Epoch: 64 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 519.064/s, 519.064/s/gpu LR: 3.014296e-09 Logit Scale: 100.000 Contrastive_loss: 0.16647 (0.19092) Loss: 0.16647 (0.19092)
2024-08-30,06:39:41 | INFO | Train Epoch: 64 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 537.765/s, 537.765/s/gpu LR: 3.004690e-09 Logit Scale: 100.000 Contrastive_loss: 0.19318 (0.19103) Loss: 0.19318 (0.19103)
2024-08-30,06:40:00 | INFO | Train Epoch: 64 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 526.421/s, 526.421/s/gpu LR: 2.995092e-09 Logit Scale: 100.000 Contrastive_loss: 0.23638 (0.19319) Loss: 0.23638 (0.19319)
2024-08-30,06:40:19 | INFO | Train Epoch: 64 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 508.395/s, 508.395/s/gpu LR: 2.985503e-09 Logit Scale: 100.000 Contrastive_loss: 0.16013 (0.19169) Loss: 0.16013 (0.19169)
2024-08-30,06:40:38 | INFO | Train Epoch: 64 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 503.140/s, 503.140/s/gpu LR: 2.975923e-09 Logit Scale: 100.000 Contrastive_loss: 0.24362 (0.19394) Loss: 0.24362 (0.19394)
2024-08-30,06:40:58 | INFO | Train Epoch: 64 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.194, 537.686/s, 537.686/s/gpu LR: 2.966352e-09 Logit Scale: 100.000 Contrastive_loss: 0.29739 (0.19825) Loss: 0.29739 (0.19825)
2024-08-30,06:41:17 | INFO | Train Epoch: 64 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 533.734/s, 533.734/s/gpu LR: 2.956790e-09 Logit Scale: 100.000 Contrastive_loss: 0.29220 (0.20201) Loss: 0.29220 (0.20201)
2024-08-30,06:41:36 | INFO | Train Epoch: 64 [240100/145000.0 (166%)] Data (t): 0.107 Batch (t): 0.192, 503.140/s, 503.140/s/gpu LR: 2.947236e-09 Logit Scale: 100.000 Contrastive_loss: 0.15604 (0.20024) Loss: 0.15604 (0.20024)
2024-08-30,06:41:56 | INFO | Train Epoch: 64 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 505.391/s, 505.391/s/gpu LR: 2.937692e-09 Logit Scale: 100.000 Contrastive_loss: 0.21062 (0.20063) Loss: 0.21062 (0.20063)
2024-08-30,06:42:15 | INFO | Train Epoch: 64 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 531.794/s, 531.794/s/gpu LR: 2.928157e-09 Logit Scale: 100.000 Contrastive_loss: 0.21119 (0.20101) Loss: 0.21119 (0.20101)
2024-08-30,06:42:34 | INFO | Train Epoch: 64 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 530.350/s, 530.350/s/gpu LR: 2.918631e-09 Logit Scale: 100.000 Contrastive_loss: 0.24980 (0.20269) Loss: 0.24980 (0.20269)
2024-08-30,06:42:53 | INFO | Train Epoch: 64 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.192, 538.886/s, 538.886/s/gpu LR: 2.909114e-09 Logit Scale: 100.000 Contrastive_loss: 0.16439 (0.20141) Loss: 0.16439 (0.20141)
2024-08-30,06:43:13 | INFO | Train Epoch: 64 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.194, 537.663/s, 537.663/s/gpu LR: 2.899606e-09 Logit Scale: 100.000 Contrastive_loss: 0.14205 (0.19950) Loss: 0.14205 (0.19950)
2024-08-30,06:43:32 | INFO | Train Epoch: 64 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 527.727/s, 527.727/s/gpu LR: 2.890107e-09 Logit Scale: 100.000 Contrastive_loss: 0.25451 (0.20122) Loss: 0.25451 (0.20122)
2024-08-30,06:43:51 | INFO | Train Epoch: 64 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 506.739/s, 506.739/s/gpu LR: 2.880618e-09 Logit Scale: 100.000 Contrastive_loss: 0.23970 (0.20238) Loss: 0.23970 (0.20238)
2024-08-30,06:44:11 | INFO | Train Epoch: 64 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 497.021/s, 497.021/s/gpu LR: 2.871138e-09 Logit Scale: 100.000 Contrastive_loss: 0.27222 (0.20444) Loss: 0.27222 (0.20444)
2024-08-30,06:44:30 | INFO | Train Epoch: 64 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 532.508/s, 532.508/s/gpu LR: 2.861667e-09 Logit Scale: 100.000 Contrastive_loss: 0.18137 (0.20378) Loss: 0.18137 (0.20378)
2024-08-30,06:44:49 | INFO | Train Epoch: 64 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 505.759/s, 505.759/s/gpu LR: 2.852206e-09 Logit Scale: 100.000 Contrastive_loss: 0.23723 (0.20471) Loss: 0.23723 (0.20471)
2024-08-30,06:45:08 | INFO | Train Epoch: 64 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.194, 499.572/s, 499.572/s/gpu LR: 2.842754e-09 Logit Scale: 100.000 Contrastive_loss: 0.29146 (0.20705) Loss: 0.29146 (0.20705)
2024-08-30,06:45:28 | INFO | Train Epoch: 64 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 531.190/s, 531.190/s/gpu LR: 2.833311e-09 Logit Scale: 100.000 Contrastive_loss: 0.14893 (0.20552) Loss: 0.14893 (0.20552)
2024-08-30,06:45:47 | INFO | Train Epoch: 64 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 501.470/s, 501.470/s/gpu LR: 2.823878e-09 Logit Scale: 100.000 Contrastive_loss: 0.25097 (0.20669) Loss: 0.25097 (0.20669)
2024-08-30,06:46:06 | INFO | Train Epoch: 64 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 492.436/s, 492.436/s/gpu LR: 2.814455e-09 Logit Scale: 100.000 Contrastive_loss: 0.22824 (0.20723) Loss: 0.22824 (0.20723)
2024-08-30,06:46:25 | INFO | Train Epoch: 64 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 529.639/s, 529.639/s/gpu LR: 2.805041e-09 Logit Scale: 100.000 Contrastive_loss: 0.12558 (0.20523) Loss: 0.12558 (0.20523)
2024-08-30,06:46:45 | INFO | Train Epoch: 64 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.192, 523.586/s, 523.586/s/gpu LR: 2.795637e-09 Logit Scale: 100.000 Contrastive_loss: 0.21617 (0.20549) Loss: 0.21617 (0.20549)
2024-08-30,06:47:04 | INFO | Train Epoch: 64 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 487.717/s, 487.717/s/gpu LR: 2.786242e-09 Logit Scale: 100.000 Contrastive_loss: 0.084700 (0.20269) Loss: 0.084700 (0.20269)
2024-08-30,06:47:11 | INFO | Eval Epoch: 65 [200 / 1000]	Clip Loss: 0.483146	
2024-08-30,06:47:12 | INFO | Eval Epoch: 65 image_to_text_mean_rank: 2.4100	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6700	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9150	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9060	text_to_image_R@10: 0.9660	clip_val_loss: 0.4400	epoch: 65.0000	num_samples: 1000.0000
2024-08-30,06:47:13 | INFO | Start epoch 65
2024-08-30,06:47:14 | INFO | Train Epoch: 65 [   100/145000.0 (0%)] Data (t): 0.023 Batch (t): 0.110, 908.584/s, 908.584/s/gpu LR: 3.038350e-09 Logit Scale: 100.000 Contrastive_loss: 0.16320 (0.16320) Loss: 0.16320 (0.16320)
2024-08-30,06:47:33 | INFO | Train Epoch: 65 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 497.074/s, 497.074/s/gpu LR: 3.028722e-09 Logit Scale: 100.000 Contrastive_loss: 0.32927 (0.24624) Loss: 0.32927 (0.24624)
2024-08-30,06:47:52 | INFO | Train Epoch: 65 [ 20100/145000.0 (14%)] Data (t): 0.110 Batch (t): 0.194, 508.484/s, 508.484/s/gpu LR: 3.019103e-09 Logit Scale: 100.000 Contrastive_loss: 0.33678 (0.27642) Loss: 0.33678 (0.27642)
2024-08-30,06:48:12 | INFO | Train Epoch: 65 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 526.204/s, 526.204/s/gpu LR: 3.009492e-09 Logit Scale: 100.000 Contrastive_loss: 0.17339 (0.25066) Loss: 0.17339 (0.25066)
2024-08-30,06:48:31 | INFO | Train Epoch: 65 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.192, 504.350/s, 504.350/s/gpu LR: 2.999890e-09 Logit Scale: 100.000 Contrastive_loss: 0.12963 (0.22646) Loss: 0.12963 (0.22646)
2024-08-30,06:48:50 | INFO | Train Epoch: 65 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.192, 564.868/s, 564.868/s/gpu LR: 2.990296e-09 Logit Scale: 100.000 Contrastive_loss: 0.15234 (0.21410) Loss: 0.15234 (0.21410)
2024-08-30,06:49:09 | INFO | Train Epoch: 65 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 503.675/s, 503.675/s/gpu LR: 2.980712e-09 Logit Scale: 100.000 Contrastive_loss: 0.11275 (0.19962) Loss: 0.11275 (0.19962)
2024-08-30,06:49:29 | INFO | Train Epoch: 65 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 531.519/s, 531.519/s/gpu LR: 2.971136e-09 Logit Scale: 100.000 Contrastive_loss: 0.20830 (0.20071) Loss: 0.20830 (0.20071)
2024-08-30,06:49:48 | INFO | Train Epoch: 65 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.192, 499.901/s, 499.901/s/gpu LR: 2.961570e-09 Logit Scale: 100.000 Contrastive_loss: 0.12438 (0.19223) Loss: 0.12438 (0.19223)
2024-08-30,06:50:07 | INFO | Train Epoch: 65 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.192, 529.296/s, 529.296/s/gpu LR: 2.952012e-09 Logit Scale: 100.000 Contrastive_loss: 0.20556 (0.19356) Loss: 0.20556 (0.19356)
2024-08-30,06:50:26 | INFO | Train Epoch: 65 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 499.385/s, 499.385/s/gpu LR: 2.942463e-09 Logit Scale: 100.000 Contrastive_loss: 0.14702 (0.18933) Loss: 0.14702 (0.18933)
2024-08-30,06:50:46 | INFO | Train Epoch: 65 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 530.371/s, 530.371/s/gpu LR: 2.932923e-09 Logit Scale: 100.000 Contrastive_loss: 0.16452 (0.18726) Loss: 0.16452 (0.18726)
2024-08-30,06:51:05 | INFO | Train Epoch: 65 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 506.942/s, 506.942/s/gpu LR: 2.923393e-09 Logit Scale: 100.000 Contrastive_loss: 0.26578 (0.19330) Loss: 0.26578 (0.19330)
2024-08-30,06:51:24 | INFO | Train Epoch: 65 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 510.564/s, 510.564/s/gpu LR: 2.913871e-09 Logit Scale: 100.000 Contrastive_loss: 0.14776 (0.19005) Loss: 0.14776 (0.19005)
2024-08-30,06:51:44 | INFO | Train Epoch: 65 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 535.130/s, 535.130/s/gpu LR: 2.904359e-09 Logit Scale: 100.000 Contrastive_loss: 0.17402 (0.18898) Loss: 0.17402 (0.18898)
2024-08-30,06:51:53 | INFO | Train Epoch: 65 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 530.633/s, 530.633/s/gpu LR: 2.899701e-09 Logit Scale: 100.000 Contrastive_loss: 0.20996 (0.19029) Loss: 0.20996 (0.19029)
2024-08-30,06:52:03 | INFO | Train Epoch: 65 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.194, 538.017/s, 538.017/s/gpu LR: 2.894855e-09 Logit Scale: 100.000 Contrastive_loss: 0.18950 (0.19025) Loss: 0.18950 (0.19025)
2024-08-30,06:52:22 | INFO | Train Epoch: 65 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 526.481/s, 526.481/s/gpu LR: 2.885361e-09 Logit Scale: 100.000 Contrastive_loss: 0.21823 (0.19180) Loss: 0.21823 (0.19180)
2024-08-30,06:52:41 | INFO | Train Epoch: 65 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 511.516/s, 511.516/s/gpu LR: 2.875877e-09 Logit Scale: 100.000 Contrastive_loss: 0.16607 (0.19045) Loss: 0.16607 (0.19045)
2024-08-30,06:53:01 | INFO | Train Epoch: 65 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 521.097/s, 521.097/s/gpu LR: 2.866401e-09 Logit Scale: 100.000 Contrastive_loss: 0.19286 (0.19057) Loss: 0.19286 (0.19057)
2024-08-30,06:53:20 | INFO | Train Epoch: 65 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.192, 532.213/s, 532.213/s/gpu LR: 2.856935e-09 Logit Scale: 100.000 Contrastive_loss: 0.23594 (0.19273) Loss: 0.23594 (0.19273)
2024-08-30,06:53:39 | INFO | Train Epoch: 65 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 504.240/s, 504.240/s/gpu LR: 2.847478e-09 Logit Scale: 100.000 Contrastive_loss: 0.15979 (0.19123) Loss: 0.15979 (0.19123)
2024-08-30,06:53:59 | INFO | Train Epoch: 65 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 523.390/s, 523.390/s/gpu LR: 2.838031e-09 Logit Scale: 100.000 Contrastive_loss: 0.24313 (0.19349) Loss: 0.24313 (0.19349)
2024-08-30,06:54:18 | INFO | Train Epoch: 65 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 541.009/s, 541.009/s/gpu LR: 2.828593e-09 Logit Scale: 100.000 Contrastive_loss: 0.29680 (0.19779) Loss: 0.29680 (0.19779)
2024-08-30,06:54:37 | INFO | Train Epoch: 65 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 520.886/s, 520.886/s/gpu LR: 2.819165e-09 Logit Scale: 100.000 Contrastive_loss: 0.29170 (0.20155) Loss: 0.29170 (0.20155)
2024-08-30,06:54:56 | INFO | Train Epoch: 65 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 514.490/s, 514.490/s/gpu LR: 2.809747e-09 Logit Scale: 100.000 Contrastive_loss: 0.15553 (0.19978) Loss: 0.15553 (0.19978)
2024-08-30,06:55:16 | INFO | Train Epoch: 65 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 491.151/s, 491.151/s/gpu LR: 2.800338e-09 Logit Scale: 100.000 Contrastive_loss: 0.21042 (0.20017) Loss: 0.21042 (0.20017)
2024-08-30,06:55:35 | INFO | Train Epoch: 65 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.192, 495.888/s, 495.888/s/gpu LR: 2.790938e-09 Logit Scale: 100.000 Contrastive_loss: 0.21067 (0.20055) Loss: 0.21067 (0.20055)
2024-08-30,06:55:54 | INFO | Train Epoch: 65 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 496.709/s, 496.709/s/gpu LR: 2.781548e-09 Logit Scale: 100.000 Contrastive_loss: 0.24940 (0.20223) Loss: 0.24940 (0.20223)
2024-08-30,06:56:13 | INFO | Train Epoch: 65 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.192, 527.125/s, 527.125/s/gpu LR: 2.772168e-09 Logit Scale: 100.000 Contrastive_loss: 0.16412 (0.20096) Loss: 0.16412 (0.20096)
2024-08-30,06:56:33 | INFO | Train Epoch: 65 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.192, 509.153/s, 509.153/s/gpu LR: 2.762798e-09 Logit Scale: 100.000 Contrastive_loss: 0.14180 (0.19905) Loss: 0.14180 (0.19905)
2024-08-30,06:56:52 | INFO | Train Epoch: 65 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 509.548/s, 509.548/s/gpu LR: 2.753438e-09 Logit Scale: 100.000 Contrastive_loss: 0.25397 (0.20077) Loss: 0.25397 (0.20077)
2024-08-30,06:57:11 | INFO | Train Epoch: 65 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 528.356/s, 528.356/s/gpu LR: 2.744087e-09 Logit Scale: 100.000 Contrastive_loss: 0.23927 (0.20194) Loss: 0.23927 (0.20194)
2024-08-30,06:57:30 | INFO | Train Epoch: 65 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 528.905/s, 528.905/s/gpu LR: 2.734747e-09 Logit Scale: 100.000 Contrastive_loss: 0.27167 (0.20399) Loss: 0.27167 (0.20399)
2024-08-30,06:57:50 | INFO | Train Epoch: 65 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.192, 532.163/s, 532.163/s/gpu LR: 2.725416e-09 Logit Scale: 100.000 Contrastive_loss: 0.18117 (0.20333) Loss: 0.18117 (0.20333)
2024-08-30,06:58:09 | INFO | Train Epoch: 65 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.193, 522.794/s, 522.794/s/gpu LR: 2.716095e-09 Logit Scale: 100.000 Contrastive_loss: 0.23659 (0.20426) Loss: 0.23659 (0.20426)
2024-08-30,06:58:28 | INFO | Train Epoch: 65 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 521.406/s, 521.406/s/gpu LR: 2.706785e-09 Logit Scale: 100.000 Contrastive_loss: 0.29090 (0.20660) Loss: 0.29090 (0.20660)
2024-08-30,06:58:48 | INFO | Train Epoch: 65 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 525.980/s, 525.980/s/gpu LR: 2.697484e-09 Logit Scale: 100.000 Contrastive_loss: 0.14853 (0.20507) Loss: 0.14853 (0.20507)
2024-08-30,06:59:07 | INFO | Train Epoch: 65 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.192, 493.630/s, 493.630/s/gpu LR: 2.688193e-09 Logit Scale: 100.000 Contrastive_loss: 0.25063 (0.20624) Loss: 0.25063 (0.20624)
2024-08-30,06:59:26 | INFO | Train Epoch: 65 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 516.822/s, 516.822/s/gpu LR: 2.678913e-09 Logit Scale: 100.000 Contrastive_loss: 0.22786 (0.20678) Loss: 0.22786 (0.20678)
2024-08-30,06:59:45 | INFO | Train Epoch: 65 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 496.709/s, 496.709/s/gpu LR: 2.669643e-09 Logit Scale: 100.000 Contrastive_loss: 0.12527 (0.20479) Loss: 0.12527 (0.20479)
2024-08-30,07:00:05 | INFO | Train Epoch: 65 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.193, 502.839/s, 502.839/s/gpu LR: 2.660383e-09 Logit Scale: 100.000 Contrastive_loss: 0.21583 (0.20506) Loss: 0.21583 (0.20506)
2024-08-30,07:00:24 | INFO | Train Epoch: 65 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 498.996/s, 498.996/s/gpu LR: 2.651133e-09 Logit Scale: 100.000 Contrastive_loss: 0.084513 (0.20225) Loss: 0.084513 (0.20225)
2024-08-30,07:00:31 | INFO | Eval Epoch: 66 [200 / 1000]	Clip Loss: 0.483176	
2024-08-30,07:00:32 | INFO | Eval Epoch: 66 image_to_text_mean_rank: 2.4100	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6710	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9220	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9060	text_to_image_R@10: 0.9660	clip_val_loss: 0.4400	epoch: 66.0000	num_samples: 1000.0000
2024-08-30,07:00:33 | INFO | Start epoch 66
2024-08-30,07:00:34 | INFO | Train Epoch: 66 [   100/145000.0 (0%)] Data (t): 0.024 Batch (t): 0.107, 932.538/s, 932.538/s/gpu LR: 2.899606e-09 Logit Scale: 100.000 Contrastive_loss: 0.16301 (0.16301) Loss: 0.16301 (0.16301)
2024-08-30,07:00:53 | INFO | Train Epoch: 66 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 496.601/s, 496.601/s/gpu LR: 2.890107e-09 Logit Scale: 100.000 Contrastive_loss: 0.32873 (0.24587) Loss: 0.32873 (0.24587)
2024-08-30,07:01:12 | INFO | Train Epoch: 66 [ 20100/145000.0 (14%)] Data (t): 0.110 Batch (t): 0.194, 491.775/s, 491.775/s/gpu LR: 2.880618e-09 Logit Scale: 100.000 Contrastive_loss: 0.33604 (0.27593) Loss: 0.33604 (0.27593)
2024-08-30,07:01:32 | INFO | Train Epoch: 66 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 523.932/s, 523.932/s/gpu LR: 2.871138e-09 Logit Scale: 100.000 Contrastive_loss: 0.17289 (0.25017) Loss: 0.17289 (0.25017)
2024-08-30,07:01:51 | INFO | Train Epoch: 66 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 533.047/s, 533.047/s/gpu LR: 2.861667e-09 Logit Scale: 100.000 Contrastive_loss: 0.12942 (0.22602) Loss: 0.12942 (0.22602)
2024-08-30,07:02:10 | INFO | Train Epoch: 66 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.192, 531.046/s, 531.046/s/gpu LR: 2.852206e-09 Logit Scale: 100.000 Contrastive_loss: 0.15192 (0.21367) Loss: 0.15192 (0.21367)
2024-08-30,07:02:29 | INFO | Train Epoch: 66 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 496.691/s, 496.691/s/gpu LR: 2.842754e-09 Logit Scale: 100.000 Contrastive_loss: 0.11245 (0.19921) Loss: 0.11245 (0.19921)
2024-08-30,07:02:49 | INFO | Train Epoch: 66 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 522.756/s, 522.756/s/gpu LR: 2.833311e-09 Logit Scale: 100.000 Contrastive_loss: 0.20758 (0.20026) Loss: 0.20758 (0.20026)
2024-08-30,07:03:08 | INFO | Train Epoch: 66 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 539.833/s, 539.833/s/gpu LR: 2.823878e-09 Logit Scale: 100.000 Contrastive_loss: 0.12402 (0.19179) Loss: 0.12402 (0.19179)
2024-08-30,07:03:27 | INFO | Train Epoch: 66 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 492.387/s, 492.387/s/gpu LR: 2.814455e-09 Logit Scale: 100.000 Contrastive_loss: 0.20511 (0.19312) Loss: 0.20511 (0.19312)
2024-08-30,07:03:47 | INFO | Train Epoch: 66 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 495.949/s, 495.949/s/gpu LR: 2.805041e-09 Logit Scale: 100.000 Contrastive_loss: 0.14679 (0.18891) Loss: 0.14679 (0.18891)
2024-08-30,07:04:06 | INFO | Train Epoch: 66 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 497.363/s, 497.363/s/gpu LR: 2.795637e-09 Logit Scale: 100.000 Contrastive_loss: 0.16425 (0.18685) Loss: 0.16425 (0.18685)
2024-08-30,07:04:25 | INFO | Train Epoch: 66 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 539.810/s, 539.810/s/gpu LR: 2.786242e-09 Logit Scale: 100.000 Contrastive_loss: 0.26541 (0.19289) Loss: 0.26541 (0.19289)
2024-08-30,07:04:45 | INFO | Train Epoch: 66 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 501.999/s, 501.999/s/gpu LR: 2.776857e-09 Logit Scale: 100.000 Contrastive_loss: 0.14746 (0.18965) Loss: 0.14746 (0.18965)
2024-08-30,07:05:04 | INFO | Train Epoch: 66 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.192, 495.705/s, 495.705/s/gpu LR: 2.767482e-09 Logit Scale: 100.000 Contrastive_loss: 0.17378 (0.18859) Loss: 0.17378 (0.18859)
2024-08-30,07:05:13 | INFO | Train Epoch: 66 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 518.461/s, 518.461/s/gpu LR: 2.762892e-09 Logit Scale: 100.000 Contrastive_loss: 0.20921 (0.18988) Loss: 0.20921 (0.18988)
2024-08-30,07:05:23 | INFO | Train Epoch: 66 [150100/145000.0 (104%)] Data (t): 0.107 Batch (t): 0.191, 524.382/s, 524.382/s/gpu LR: 2.758117e-09 Logit Scale: 100.000 Contrastive_loss: 0.18909 (0.18983) Loss: 0.18909 (0.18983)
2024-08-30,07:05:42 | INFO | Train Epoch: 66 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 532.103/s, 532.103/s/gpu LR: 2.748761e-09 Logit Scale: 100.000 Contrastive_loss: 0.21776 (0.19138) Loss: 0.21776 (0.19138)
2024-08-30,07:06:02 | INFO | Train Epoch: 66 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 531.860/s, 531.860/s/gpu LR: 2.739416e-09 Logit Scale: 100.000 Contrastive_loss: 0.16575 (0.19004) Loss: 0.16575 (0.19004)
2024-08-30,07:06:21 | INFO | Train Epoch: 66 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 506.854/s, 506.854/s/gpu LR: 2.730080e-09 Logit Scale: 100.000 Contrastive_loss: 0.19259 (0.19016) Loss: 0.19259 (0.19016)
2024-08-30,07:06:40 | INFO | Train Epoch: 66 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 525.593/s, 525.593/s/gpu LR: 2.720754e-09 Logit Scale: 100.000 Contrastive_loss: 0.23555 (0.19232) Loss: 0.23555 (0.19232)
2024-08-30,07:06:59 | INFO | Train Epoch: 66 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 499.904/s, 499.904/s/gpu LR: 2.711439e-09 Logit Scale: 100.000 Contrastive_loss: 0.15952 (0.19083) Loss: 0.15952 (0.19083)
2024-08-30,07:07:19 | INFO | Train Epoch: 66 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 497.295/s, 497.295/s/gpu LR: 2.702133e-09 Logit Scale: 100.000 Contrastive_loss: 0.24278 (0.19309) Loss: 0.24278 (0.19309)
2024-08-30,07:07:38 | INFO | Train Epoch: 66 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 526.670/s, 526.670/s/gpu LR: 2.692837e-09 Logit Scale: 100.000 Contrastive_loss: 0.29631 (0.19739) Loss: 0.29631 (0.19739)
2024-08-30,07:07:57 | INFO | Train Epoch: 66 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 494.483/s, 494.483/s/gpu LR: 2.683552e-09 Logit Scale: 100.000 Contrastive_loss: 0.29119 (0.20114) Loss: 0.29119 (0.20114)
2024-08-30,07:08:16 | INFO | Train Epoch: 66 [240100/145000.0 (166%)] Data (t): 0.107 Batch (t): 0.192, 522.353/s, 522.353/s/gpu LR: 2.674277e-09 Logit Scale: 100.000 Contrastive_loss: 0.15503 (0.19937) Loss: 0.15503 (0.19937)
2024-08-30,07:08:36 | INFO | Train Epoch: 66 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 500.237/s, 500.237/s/gpu LR: 2.665012e-09 Logit Scale: 100.000 Contrastive_loss: 0.21011 (0.19977) Loss: 0.21011 (0.19977)
2024-08-30,07:08:55 | INFO | Train Epoch: 66 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 537.162/s, 537.162/s/gpu LR: 2.655757e-09 Logit Scale: 100.000 Contrastive_loss: 0.21037 (0.20015) Loss: 0.21037 (0.20015)
2024-08-30,07:09:14 | INFO | Train Epoch: 66 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 521.068/s, 521.068/s/gpu LR: 2.646512e-09 Logit Scale: 100.000 Contrastive_loss: 0.24899 (0.20183) Loss: 0.24899 (0.20183)
2024-08-30,07:09:33 | INFO | Train Epoch: 66 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 531.381/s, 531.381/s/gpu LR: 2.637278e-09 Logit Scale: 100.000 Contrastive_loss: 0.16371 (0.20056) Loss: 0.16371 (0.20056)
2024-08-30,07:09:53 | INFO | Train Epoch: 66 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 521.622/s, 521.622/s/gpu LR: 2.628054e-09 Logit Scale: 100.000 Contrastive_loss: 0.14165 (0.19866) Loss: 0.14165 (0.19866)
2024-08-30,07:10:12 | INFO | Train Epoch: 66 [300100/145000.0 (207%)] Data (t): 0.107 Batch (t): 0.193, 496.478/s, 496.478/s/gpu LR: 2.618840e-09 Logit Scale: 100.000 Contrastive_loss: 0.25358 (0.20038) Loss: 0.25358 (0.20038)
2024-08-30,07:10:31 | INFO | Train Epoch: 66 [310100/145000.0 (214%)] Data (t): 0.110 Batch (t): 0.194, 535.901/s, 535.901/s/gpu LR: 2.609637e-09 Logit Scale: 100.000 Contrastive_loss: 0.23915 (0.20155) Loss: 0.23915 (0.20155)
2024-08-30,07:10:51 | INFO | Train Epoch: 66 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 536.616/s, 536.616/s/gpu LR: 2.600445e-09 Logit Scale: 100.000 Contrastive_loss: 0.27114 (0.20360) Loss: 0.27114 (0.20360)
2024-08-30,07:11:10 | INFO | Train Epoch: 66 [330100/145000.0 (228%)] Data (t): 0.107 Batch (t): 0.192, 500.515/s, 500.515/s/gpu LR: 2.591263e-09 Logit Scale: 100.000 Contrastive_loss: 0.18081 (0.20295) Loss: 0.18081 (0.20295)
2024-08-30,07:11:29 | INFO | Train Epoch: 66 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 510.899/s, 510.899/s/gpu LR: 2.582091e-09 Logit Scale: 100.000 Contrastive_loss: 0.23601 (0.20387) Loss: 0.23601 (0.20387)
2024-08-30,07:11:49 | INFO | Train Epoch: 66 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 495.089/s, 495.089/s/gpu LR: 2.572930e-09 Logit Scale: 100.000 Contrastive_loss: 0.29054 (0.20621) Loss: 0.29054 (0.20621)
2024-08-30,07:12:08 | INFO | Train Epoch: 66 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 533.170/s, 533.170/s/gpu LR: 2.563780e-09 Logit Scale: 100.000 Contrastive_loss: 0.14817 (0.20468) Loss: 0.14817 (0.20468)
2024-08-30,07:12:27 | INFO | Train Epoch: 66 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 530.253/s, 530.253/s/gpu LR: 2.554641e-09 Logit Scale: 100.000 Contrastive_loss: 0.25005 (0.20584) Loss: 0.25005 (0.20584)
2024-08-30,07:12:46 | INFO | Train Epoch: 66 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 493.033/s, 493.033/s/gpu LR: 2.545512e-09 Logit Scale: 100.000 Contrastive_loss: 0.22748 (0.20638) Loss: 0.22748 (0.20638)
2024-08-30,07:13:06 | INFO | Train Epoch: 66 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.193, 525.537/s, 525.537/s/gpu LR: 2.536394e-09 Logit Scale: 100.000 Contrastive_loss: 0.12489 (0.20440) Loss: 0.12489 (0.20440)
2024-08-30,07:13:25 | INFO | Train Epoch: 66 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 497.071/s, 497.071/s/gpu LR: 2.527287e-09 Logit Scale: 100.000 Contrastive_loss: 0.21539 (0.20466) Loss: 0.21539 (0.20466)
2024-08-30,07:13:44 | INFO | Train Epoch: 66 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 523.788/s, 523.788/s/gpu LR: 2.518190e-09 Logit Scale: 100.000 Contrastive_loss: 0.084250 (0.20186) Loss: 0.084250 (0.20186)
2024-08-30,07:13:52 | INFO | Eval Epoch: 67 [200 / 1000]	Clip Loss: 0.483240	
2024-08-30,07:13:53 | INFO | Eval Epoch: 67 image_to_text_mean_rank: 2.4090	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6710	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9210	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9060	text_to_image_R@10: 0.9660	clip_val_loss: 0.4401	epoch: 67.0000	num_samples: 1000.0000
2024-08-30,07:13:54 | INFO | Start epoch 67
2024-08-30,07:13:54 | INFO | Train Epoch: 67 [   100/145000.0 (0%)] Data (t): 0.034 Batch (t): 0.120, 832.375/s, 832.375/s/gpu LR: 2.762798e-09 Logit Scale: 100.000 Contrastive_loss: 0.16256 (0.16256) Loss: 0.16256 (0.16256)
2024-08-30,07:14:13 | INFO | Train Epoch: 67 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 522.089/s, 522.089/s/gpu LR: 2.753438e-09 Logit Scale: 100.000 Contrastive_loss: 0.32815 (0.24536) Loss: 0.32815 (0.24536)
2024-08-30,07:14:33 | INFO | Train Epoch: 67 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 499.791/s, 499.791/s/gpu LR: 2.744087e-09 Logit Scale: 100.000 Contrastive_loss: 0.33536 (0.27536) Loss: 0.33536 (0.27536)
2024-08-30,07:14:52 | INFO | Train Epoch: 67 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 533.835/s, 533.835/s/gpu LR: 2.734747e-09 Logit Scale: 100.000 Contrastive_loss: 0.17244 (0.24963) Loss: 0.17244 (0.24963)
2024-08-30,07:15:11 | INFO | Train Epoch: 67 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 532.900/s, 532.900/s/gpu LR: 2.725416e-09 Logit Scale: 100.000 Contrastive_loss: 0.12918 (0.22554) Loss: 0.12918 (0.22554)
2024-08-30,07:15:31 | INFO | Train Epoch: 67 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 530.781/s, 530.781/s/gpu LR: 2.716095e-09 Logit Scale: 100.000 Contrastive_loss: 0.15142 (0.21318) Loss: 0.15142 (0.21318)
2024-08-30,07:15:50 | INFO | Train Epoch: 67 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 506.549/s, 506.549/s/gpu LR: 2.706785e-09 Logit Scale: 100.000 Contrastive_loss: 0.11218 (0.19876) Loss: 0.11218 (0.19876)
2024-08-30,07:16:09 | INFO | Train Epoch: 67 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 496.938/s, 496.938/s/gpu LR: 2.697484e-09 Logit Scale: 100.000 Contrastive_loss: 0.20696 (0.19978) Loss: 0.20696 (0.19978)
2024-08-30,07:16:28 | INFO | Train Epoch: 67 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 492.826/s, 492.826/s/gpu LR: 2.688193e-09 Logit Scale: 100.000 Contrastive_loss: 0.12363 (0.19132) Loss: 0.12363 (0.19132)
2024-08-30,07:16:48 | INFO | Train Epoch: 67 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 500.983/s, 500.983/s/gpu LR: 2.678913e-09 Logit Scale: 100.000 Contrastive_loss: 0.20461 (0.19265) Loss: 0.20461 (0.19265)
2024-08-30,07:17:07 | INFO | Train Epoch: 67 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 496.611/s, 496.611/s/gpu LR: 2.669643e-09 Logit Scale: 100.000 Contrastive_loss: 0.14646 (0.18845) Loss: 0.14646 (0.18845)
2024-08-30,07:17:26 | INFO | Train Epoch: 67 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.192, 542.130/s, 542.130/s/gpu LR: 2.660383e-09 Logit Scale: 100.000 Contrastive_loss: 0.16382 (0.18640) Loss: 0.16382 (0.18640)
2024-08-30,07:17:46 | INFO | Train Epoch: 67 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 499.258/s, 499.258/s/gpu LR: 2.651133e-09 Logit Scale: 100.000 Contrastive_loss: 0.26497 (0.19244) Loss: 0.26497 (0.19244)
2024-08-30,07:18:05 | INFO | Train Epoch: 67 [130100/145000.0 (90%)] Data (t): 0.107 Batch (t): 0.193, 527.064/s, 527.064/s/gpu LR: 2.641894e-09 Logit Scale: 100.000 Contrastive_loss: 0.14733 (0.18922) Loss: 0.14733 (0.18922)
2024-08-30,07:18:24 | INFO | Train Epoch: 67 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 527.538/s, 527.538/s/gpu LR: 2.632665e-09 Logit Scale: 100.000 Contrastive_loss: 0.17362 (0.18818) Loss: 0.17362 (0.18818)
2024-08-30,07:18:34 | INFO | Train Epoch: 67 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 531.527/s, 531.527/s/gpu LR: 2.628146e-09 Logit Scale: 100.000 Contrastive_loss: 0.20863 (0.18946) Loss: 0.20863 (0.18946)
2024-08-30,07:18:43 | INFO | Train Epoch: 67 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.194, 506.014/s, 506.014/s/gpu LR: 2.623446e-09 Logit Scale: 100.000 Contrastive_loss: 0.18874 (0.18942) Loss: 0.18874 (0.18942)
2024-08-30,07:19:03 | INFO | Train Epoch: 67 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 506.748/s, 506.748/s/gpu LR: 2.614238e-09 Logit Scale: 100.000 Contrastive_loss: 0.21720 (0.19096) Loss: 0.21720 (0.19096)
2024-08-30,07:19:22 | INFO | Train Epoch: 67 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 502.561/s, 502.561/s/gpu LR: 2.605040e-09 Logit Scale: 100.000 Contrastive_loss: 0.16532 (0.18961) Loss: 0.16532 (0.18961)
2024-08-30,07:19:41 | INFO | Train Epoch: 67 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 525.746/s, 525.746/s/gpu LR: 2.595853e-09 Logit Scale: 100.000 Contrastive_loss: 0.19235 (0.18975) Loss: 0.19235 (0.18975)
2024-08-30,07:20:01 | INFO | Train Epoch: 67 [190100/145000.0 (131%)] Data (t): 0.107 Batch (t): 0.192, 498.450/s, 498.450/s/gpu LR: 2.586676e-09 Logit Scale: 100.000 Contrastive_loss: 0.23509 (0.19191) Loss: 0.23509 (0.19191)
2024-08-30,07:20:20 | INFO | Train Epoch: 67 [200100/145000.0 (138%)] Data (t): 0.107 Batch (t): 0.192, 502.012/s, 502.012/s/gpu LR: 2.577510e-09 Logit Scale: 100.000 Contrastive_loss: 0.15927 (0.19042) Loss: 0.15927 (0.19042)
2024-08-30,07:20:39 | INFO | Train Epoch: 67 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 498.292/s, 498.292/s/gpu LR: 2.568354e-09 Logit Scale: 100.000 Contrastive_loss: 0.24247 (0.19268) Loss: 0.24247 (0.19268)
2024-08-30,07:20:58 | INFO | Train Epoch: 67 [220100/145000.0 (152%)] Data (t): 0.107 Batch (t): 0.192, 509.510/s, 509.510/s/gpu LR: 2.559209e-09 Logit Scale: 100.000 Contrastive_loss: 0.29560 (0.19697) Loss: 0.29560 (0.19697)
2024-08-30,07:21:18 | INFO | Train Epoch: 67 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 525.854/s, 525.854/s/gpu LR: 2.550075e-09 Logit Scale: 100.000 Contrastive_loss: 0.29070 (0.20072) Loss: 0.29070 (0.20072)
2024-08-30,07:21:37 | INFO | Train Epoch: 67 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 490.151/s, 490.151/s/gpu LR: 2.540952e-09 Logit Scale: 100.000 Contrastive_loss: 0.15466 (0.19895) Loss: 0.15466 (0.19895)
2024-08-30,07:21:56 | INFO | Train Epoch: 67 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 527.536/s, 527.536/s/gpu LR: 2.531839e-09 Logit Scale: 100.000 Contrastive_loss: 0.20985 (0.19935) Loss: 0.20985 (0.19935)
2024-08-30,07:22:15 | INFO | Train Epoch: 67 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 529.059/s, 529.059/s/gpu LR: 2.522737e-09 Logit Scale: 100.000 Contrastive_loss: 0.21001 (0.19973) Loss: 0.21001 (0.19973)
2024-08-30,07:22:35 | INFO | Train Epoch: 67 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 498.911/s, 498.911/s/gpu LR: 2.513646e-09 Logit Scale: 100.000 Contrastive_loss: 0.24880 (0.20143) Loss: 0.24880 (0.20143)
2024-08-30,07:22:54 | INFO | Train Epoch: 67 [280100/145000.0 (193%)] Data (t): 0.107 Batch (t): 0.192, 496.715/s, 496.715/s/gpu LR: 2.504566e-09 Logit Scale: 100.000 Contrastive_loss: 0.16354 (0.20016) Loss: 0.16354 (0.20016)
2024-08-30,07:23:13 | INFO | Train Epoch: 67 [290100/145000.0 (200%)] Data (t): 0.107 Batch (t): 0.193, 538.012/s, 538.012/s/gpu LR: 2.495497e-09 Logit Scale: 100.000 Contrastive_loss: 0.14147 (0.19827) Loss: 0.14147 (0.19827)
2024-08-30,07:23:32 | INFO | Train Epoch: 67 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 533.110/s, 533.110/s/gpu LR: 2.486439e-09 Logit Scale: 100.000 Contrastive_loss: 0.25313 (0.19998) Loss: 0.25313 (0.19998)
2024-08-30,07:23:52 | INFO | Train Epoch: 67 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 520.048/s, 520.048/s/gpu LR: 2.477392e-09 Logit Scale: 100.000 Contrastive_loss: 0.23890 (0.20116) Loss: 0.23890 (0.20116)
2024-08-30,07:24:11 | INFO | Train Epoch: 67 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.192, 536.251/s, 536.251/s/gpu LR: 2.468356e-09 Logit Scale: 100.000 Contrastive_loss: 0.27076 (0.20321) Loss: 0.27076 (0.20321)
2024-08-30,07:24:30 | INFO | Train Epoch: 67 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.193, 523.350/s, 523.350/s/gpu LR: 2.459331e-09 Logit Scale: 100.000 Contrastive_loss: 0.18068 (0.20257) Loss: 0.18068 (0.20257)
2024-08-30,07:24:49 | INFO | Train Epoch: 67 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 523.845/s, 523.845/s/gpu LR: 2.450317e-09 Logit Scale: 100.000 Contrastive_loss: 0.23540 (0.20348) Loss: 0.23540 (0.20348)
2024-08-30,07:25:09 | INFO | Train Epoch: 67 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.193, 531.722/s, 531.722/s/gpu LR: 2.441315e-09 Logit Scale: 100.000 Contrastive_loss: 0.29007 (0.20582) Loss: 0.29007 (0.20582)
2024-08-30,07:25:28 | INFO | Train Epoch: 67 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.194, 493.028/s, 493.028/s/gpu LR: 2.432323e-09 Logit Scale: 100.000 Contrastive_loss: 0.14783 (0.20429) Loss: 0.14783 (0.20429)
2024-08-30,07:25:47 | INFO | Train Epoch: 67 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 503.645/s, 503.645/s/gpu LR: 2.423343e-09 Logit Scale: 100.000 Contrastive_loss: 0.24959 (0.20546) Loss: 0.24959 (0.20546)
2024-08-30,07:26:07 | INFO | Train Epoch: 67 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 536.542/s, 536.542/s/gpu LR: 2.414374e-09 Logit Scale: 100.000 Contrastive_loss: 0.22711 (0.20600) Loss: 0.22711 (0.20600)
2024-08-30,07:26:26 | INFO | Train Epoch: 67 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 527.977/s, 527.977/s/gpu LR: 2.405417e-09 Logit Scale: 100.000 Contrastive_loss: 0.12462 (0.20401) Loss: 0.12462 (0.20401)
2024-08-30,07:26:45 | INFO | Train Epoch: 67 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 491.079/s, 491.079/s/gpu LR: 2.396471e-09 Logit Scale: 100.000 Contrastive_loss: 0.21511 (0.20428) Loss: 0.21511 (0.20428)
2024-08-30,07:27:05 | INFO | Train Epoch: 67 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 502.214/s, 502.214/s/gpu LR: 2.387536e-09 Logit Scale: 100.000 Contrastive_loss: 0.084050 (0.20148) Loss: 0.084050 (0.20148)
2024-08-30,07:27:12 | INFO | Eval Epoch: 68 [200 / 1000]	Clip Loss: 0.483401	
2024-08-30,07:27:13 | INFO | Eval Epoch: 68 image_to_text_mean_rank: 2.4100	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6710	image_to_text_R@5: 0.9070	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9250	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9060	text_to_image_R@10: 0.9660	clip_val_loss: 0.4402	epoch: 68.0000	num_samples: 1000.0000
2024-08-30,07:27:14 | INFO | Start epoch 68
2024-08-30,07:27:14 | INFO | Train Epoch: 68 [   100/145000.0 (0%)] Data (t): 0.020 Batch (t): 0.098, 1021.61/s, 1021.61/s/gpu LR: 2.628054e-09 Logit Scale: 100.000 Contrastive_loss: 0.16223 (0.16223) Loss: 0.16223 (0.16223)
2024-08-30,07:27:34 | INFO | Train Epoch: 68 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 493.184/s, 493.184/s/gpu LR: 2.618840e-09 Logit Scale: 100.000 Contrastive_loss: 0.32786 (0.24505) Loss: 0.32786 (0.24505)
2024-08-30,07:27:53 | INFO | Train Epoch: 68 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 533.013/s, 533.013/s/gpu LR: 2.609637e-09 Logit Scale: 100.000 Contrastive_loss: 0.33463 (0.27491) Loss: 0.33463 (0.27491)
2024-08-30,07:28:12 | INFO | Train Epoch: 68 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 534.384/s, 534.384/s/gpu LR: 2.600445e-09 Logit Scale: 100.000 Contrastive_loss: 0.17197 (0.24917) Loss: 0.17197 (0.24917)
2024-08-30,07:28:32 | INFO | Train Epoch: 68 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 497.666/s, 497.666/s/gpu LR: 2.591263e-09 Logit Scale: 100.000 Contrastive_loss: 0.12899 (0.22514) Loss: 0.12899 (0.22514)
2024-08-30,07:28:51 | INFO | Train Epoch: 68 [ 50100/145000.0 (35%)] Data (t): 0.107 Batch (t): 0.192, 517.856/s, 517.856/s/gpu LR: 2.582091e-09 Logit Scale: 100.000 Contrastive_loss: 0.15102 (0.21278) Loss: 0.15102 (0.21278)
2024-08-30,07:29:10 | INFO | Train Epoch: 68 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 496.568/s, 496.568/s/gpu LR: 2.572930e-09 Logit Scale: 100.000 Contrastive_loss: 0.11203 (0.19839) Loss: 0.11203 (0.19839)
2024-08-30,07:29:29 | INFO | Train Epoch: 68 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 496.757/s, 496.757/s/gpu LR: 2.563780e-09 Logit Scale: 100.000 Contrastive_loss: 0.20635 (0.19939) Loss: 0.20635 (0.19939)
2024-08-30,07:29:49 | INFO | Train Epoch: 68 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 505.738/s, 505.738/s/gpu LR: 2.554641e-09 Logit Scale: 100.000 Contrastive_loss: 0.12337 (0.19094) Loss: 0.12337 (0.19094)
2024-08-30,07:30:08 | INFO | Train Epoch: 68 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 494.514/s, 494.514/s/gpu LR: 2.545512e-09 Logit Scale: 100.000 Contrastive_loss: 0.20418 (0.19226) Loss: 0.20418 (0.19226)
2024-08-30,07:30:27 | INFO | Train Epoch: 68 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 494.012/s, 494.012/s/gpu LR: 2.536394e-09 Logit Scale: 100.000 Contrastive_loss: 0.14622 (0.18808) Loss: 0.14622 (0.18808)
2024-08-30,07:30:47 | INFO | Train Epoch: 68 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 502.771/s, 502.771/s/gpu LR: 2.527287e-09 Logit Scale: 100.000 Contrastive_loss: 0.16352 (0.18603) Loss: 0.16352 (0.18603)
2024-08-30,07:31:06 | INFO | Train Epoch: 68 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 508.721/s, 508.721/s/gpu LR: 2.518190e-09 Logit Scale: 100.000 Contrastive_loss: 0.26456 (0.19207) Loss: 0.26456 (0.19207)
2024-08-30,07:31:25 | INFO | Train Epoch: 68 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 525.038/s, 525.038/s/gpu LR: 2.509105e-09 Logit Scale: 100.000 Contrastive_loss: 0.14706 (0.18886) Loss: 0.14706 (0.18886)
2024-08-30,07:31:44 | INFO | Train Epoch: 68 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 493.514/s, 493.514/s/gpu LR: 2.500030e-09 Logit Scale: 100.000 Contrastive_loss: 0.17331 (0.18782) Loss: 0.17331 (0.18782)
2024-08-30,07:31:54 | INFO | Train Epoch: 68 [145000/145000.0 (100%)] Data (t): 0.107 Batch (t): 0.192, 500.469/s, 500.469/s/gpu LR: 2.495588e-09 Logit Scale: 100.000 Contrastive_loss: 0.20786 (0.18907) Loss: 0.20786 (0.18907)
2024-08-30,07:32:04 | INFO | Train Epoch: 68 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 533.936/s, 533.936/s/gpu LR: 2.490967e-09 Logit Scale: 100.000 Contrastive_loss: 0.18824 (0.18902) Loss: 0.18824 (0.18902)
2024-08-30,07:32:23 | INFO | Train Epoch: 68 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.192, 491.840/s, 491.840/s/gpu LR: 2.481914e-09 Logit Scale: 100.000 Contrastive_loss: 0.21674 (0.19056) Loss: 0.21674 (0.19056)
2024-08-30,07:32:42 | INFO | Train Epoch: 68 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 532.314/s, 532.314/s/gpu LR: 2.472872e-09 Logit Scale: 100.000 Contrastive_loss: 0.16508 (0.18922) Loss: 0.16508 (0.18922)
2024-08-30,07:33:01 | INFO | Train Epoch: 68 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 513.239/s, 513.239/s/gpu LR: 2.463842e-09 Logit Scale: 100.000 Contrastive_loss: 0.19215 (0.18937) Loss: 0.19215 (0.18937)
2024-08-30,07:33:21 | INFO | Train Epoch: 68 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 496.219/s, 496.219/s/gpu LR: 2.454823e-09 Logit Scale: 100.000 Contrastive_loss: 0.23474 (0.19153) Loss: 0.23474 (0.19153)
2024-08-30,07:33:40 | INFO | Train Epoch: 68 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 524.796/s, 524.796/s/gpu LR: 2.445814e-09 Logit Scale: 100.000 Contrastive_loss: 0.15902 (0.19005) Loss: 0.15902 (0.19005)
2024-08-30,07:33:59 | INFO | Train Epoch: 68 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 505.054/s, 505.054/s/gpu LR: 2.436818e-09 Logit Scale: 100.000 Contrastive_loss: 0.24214 (0.19232) Loss: 0.24214 (0.19232)
2024-08-30,07:34:19 | INFO | Train Epoch: 68 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.192, 502.905/s, 502.905/s/gpu LR: 2.427832e-09 Logit Scale: 100.000 Contrastive_loss: 0.29510 (0.19660) Loss: 0.29510 (0.19660)
2024-08-30,07:34:38 | INFO | Train Epoch: 68 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 527.452/s, 527.452/s/gpu LR: 2.418857e-09 Logit Scale: 100.000 Contrastive_loss: 0.29028 (0.20035) Loss: 0.29028 (0.20035)
2024-08-30,07:34:57 | INFO | Train Epoch: 68 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 509.329/s, 509.329/s/gpu LR: 2.409894e-09 Logit Scale: 100.000 Contrastive_loss: 0.15418 (0.19857) Loss: 0.15418 (0.19857)
2024-08-30,07:35:16 | INFO | Train Epoch: 68 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 532.416/s, 532.416/s/gpu LR: 2.400942e-09 Logit Scale: 100.000 Contrastive_loss: 0.20956 (0.19898) Loss: 0.20956 (0.19898)
2024-08-30,07:35:36 | INFO | Train Epoch: 68 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.192, 522.251/s, 522.251/s/gpu LR: 2.392002e-09 Logit Scale: 100.000 Contrastive_loss: 0.20960 (0.19936) Loss: 0.20960 (0.19936)
2024-08-30,07:35:55 | INFO | Train Epoch: 68 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 530.520/s, 530.520/s/gpu LR: 2.383073e-09 Logit Scale: 100.000 Contrastive_loss: 0.24844 (0.20105) Loss: 0.24844 (0.20105)
2024-08-30,07:36:14 | INFO | Train Epoch: 68 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 487.064/s, 487.064/s/gpu LR: 2.374156e-09 Logit Scale: 100.000 Contrastive_loss: 0.16324 (0.19979) Loss: 0.16324 (0.19979)
2024-08-30,07:36:33 | INFO | Train Epoch: 68 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 505.574/s, 505.574/s/gpu LR: 2.365250e-09 Logit Scale: 100.000 Contrastive_loss: 0.14128 (0.19790) Loss: 0.14128 (0.19790)
2024-08-30,07:36:53 | INFO | Train Epoch: 68 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 492.791/s, 492.791/s/gpu LR: 2.356355e-09 Logit Scale: 100.000 Contrastive_loss: 0.25265 (0.19961) Loss: 0.25265 (0.19961)
2024-08-30,07:37:12 | INFO | Train Epoch: 68 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 525.466/s, 525.466/s/gpu LR: 2.347473e-09 Logit Scale: 100.000 Contrastive_loss: 0.23873 (0.20080) Loss: 0.23873 (0.20080)
2024-08-30,07:37:31 | INFO | Train Epoch: 68 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 500.546/s, 500.546/s/gpu LR: 2.338602e-09 Logit Scale: 100.000 Contrastive_loss: 0.27027 (0.20284) Loss: 0.27027 (0.20284)
2024-08-30,07:37:50 | INFO | Train Epoch: 68 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 526.421/s, 526.421/s/gpu LR: 2.329742e-09 Logit Scale: 100.000 Contrastive_loss: 0.18041 (0.20220) Loss: 0.18041 (0.20220)
2024-08-30,07:38:10 | INFO | Train Epoch: 68 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 534.591/s, 534.591/s/gpu LR: 2.320894e-09 Logit Scale: 100.000 Contrastive_loss: 0.23483 (0.20311) Loss: 0.23483 (0.20311)
2024-08-30,07:38:29 | INFO | Train Epoch: 68 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 520.968/s, 520.968/s/gpu LR: 2.312058e-09 Logit Scale: 100.000 Contrastive_loss: 0.28958 (0.20544) Loss: 0.28958 (0.20544)
2024-08-30,07:38:48 | INFO | Train Epoch: 68 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 503.064/s, 503.064/s/gpu LR: 2.303234e-09 Logit Scale: 100.000 Contrastive_loss: 0.14763 (0.20392) Loss: 0.14763 (0.20392)
2024-08-30,07:39:08 | INFO | Train Epoch: 68 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.193, 522.040/s, 522.040/s/gpu LR: 2.294422e-09 Logit Scale: 100.000 Contrastive_loss: 0.24930 (0.20509) Loss: 0.24930 (0.20509)
2024-08-30,07:39:27 | INFO | Train Epoch: 68 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 496.433/s, 496.433/s/gpu LR: 2.285621e-09 Logit Scale: 100.000 Contrastive_loss: 0.22680 (0.20563) Loss: 0.22680 (0.20563)
2024-08-30,07:39:46 | INFO | Train Epoch: 68 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 528.806/s, 528.806/s/gpu LR: 2.276833e-09 Logit Scale: 100.000 Contrastive_loss: 0.12431 (0.20365) Loss: 0.12431 (0.20365)
2024-08-30,07:40:05 | INFO | Train Epoch: 68 [400100/145000.0 (276%)] Data (t): 0.107 Batch (t): 0.192, 539.423/s, 539.423/s/gpu LR: 2.268056e-09 Logit Scale: 100.000 Contrastive_loss: 0.21473 (0.20391) Loss: 0.21473 (0.20391)
2024-08-30,07:40:25 | INFO | Train Epoch: 68 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.192, 531.069/s, 531.069/s/gpu LR: 2.259291e-09 Logit Scale: 100.000 Contrastive_loss: 0.083874 (0.20112) Loss: 0.083874 (0.20112)
2024-08-30,07:40:32 | INFO | Eval Epoch: 69 [200 / 1000]	Clip Loss: 0.483602	
2024-08-30,07:40:33 | INFO | Eval Epoch: 69 image_to_text_mean_rank: 2.4080	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6720	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9260	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9060	text_to_image_R@10: 0.9660	clip_val_loss: 0.4403	epoch: 69.0000	num_samples: 1000.0000
2024-08-30,07:40:34 | INFO | Start epoch 69
2024-08-30,07:40:34 | INFO | Train Epoch: 69 [   100/145000.0 (0%)] Data (t): 0.025 Batch (t): 0.107, 930.477/s, 930.477/s/gpu LR: 2.495497e-09 Logit Scale: 100.000 Contrastive_loss: 0.16195 (0.16195) Loss: 0.16195 (0.16195)
2024-08-30,07:40:54 | INFO | Train Epoch: 69 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 533.578/s, 533.578/s/gpu LR: 2.486439e-09 Logit Scale: 100.000 Contrastive_loss: 0.32749 (0.24472) Loss: 0.32749 (0.24472)
2024-08-30,07:41:13 | INFO | Train Epoch: 69 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 498.900/s, 498.900/s/gpu LR: 2.477392e-09 Logit Scale: 100.000 Contrastive_loss: 0.33395 (0.27447) Loss: 0.33395 (0.27447)
2024-08-30,07:41:32 | INFO | Train Epoch: 69 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 534.159/s, 534.159/s/gpu LR: 2.468356e-09 Logit Scale: 100.000 Contrastive_loss: 0.17159 (0.24875) Loss: 0.17159 (0.24875)
2024-08-30,07:41:52 | INFO | Train Epoch: 69 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 516.644/s, 516.644/s/gpu LR: 2.459331e-09 Logit Scale: 100.000 Contrastive_loss: 0.12881 (0.22476) Loss: 0.12881 (0.22476)
2024-08-30,07:42:11 | INFO | Train Epoch: 69 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.192, 523.147/s, 523.147/s/gpu LR: 2.450317e-09 Logit Scale: 100.000 Contrastive_loss: 0.15073 (0.21242) Loss: 0.15073 (0.21242)
2024-08-30,07:42:30 | INFO | Train Epoch: 69 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 524.234/s, 524.234/s/gpu LR: 2.441315e-09 Logit Scale: 100.000 Contrastive_loss: 0.11174 (0.19804) Loss: 0.11174 (0.19804)
2024-08-30,07:42:49 | INFO | Train Epoch: 69 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 496.179/s, 496.179/s/gpu LR: 2.432323e-09 Logit Scale: 100.000 Contrastive_loss: 0.20575 (0.19900) Loss: 0.20575 (0.19900)
2024-08-30,07:43:09 | INFO | Train Epoch: 69 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 497.508/s, 497.508/s/gpu LR: 2.423343e-09 Logit Scale: 100.000 Contrastive_loss: 0.12307 (0.19057) Loss: 0.12307 (0.19057)
2024-08-30,07:43:28 | INFO | Train Epoch: 69 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 506.003/s, 506.003/s/gpu LR: 2.414374e-09 Logit Scale: 100.000 Contrastive_loss: 0.20392 (0.19190) Loss: 0.20392 (0.19190)
2024-08-30,07:43:47 | INFO | Train Epoch: 69 [100100/145000.0 (69%)] Data (t): 0.107 Batch (t): 0.192, 527.352/s, 527.352/s/gpu LR: 2.405417e-09 Logit Scale: 100.000 Contrastive_loss: 0.14599 (0.18773) Loss: 0.14599 (0.18773)
2024-08-30,07:44:06 | INFO | Train Epoch: 69 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.192, 540.454/s, 540.454/s/gpu LR: 2.396471e-09 Logit Scale: 100.000 Contrastive_loss: 0.16314 (0.18568) Loss: 0.16314 (0.18568)
2024-08-30,07:44:26 | INFO | Train Epoch: 69 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.192, 489.635/s, 489.635/s/gpu LR: 2.387536e-09 Logit Scale: 100.000 Contrastive_loss: 0.26422 (0.19172) Loss: 0.26422 (0.19172)
2024-08-30,07:44:45 | INFO | Train Epoch: 69 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 519.281/s, 519.281/s/gpu LR: 2.378613e-09 Logit Scale: 100.000 Contrastive_loss: 0.14674 (0.18851) Loss: 0.14674 (0.18851)
2024-08-30,07:45:04 | INFO | Train Epoch: 69 [140100/145000.0 (97%)] Data (t): 0.107 Batch (t): 0.192, 563.685/s, 563.685/s/gpu LR: 2.369701e-09 Logit Scale: 100.000 Contrastive_loss: 0.17308 (0.18748) Loss: 0.17308 (0.18748)
2024-08-30,07:45:14 | INFO | Train Epoch: 69 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 500.346/s, 500.346/s/gpu LR: 2.365339e-09 Logit Scale: 100.000 Contrastive_loss: 0.20747 (0.18873) Loss: 0.20747 (0.18873)
2024-08-30,07:45:23 | INFO | Train Epoch: 69 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 499.106/s, 499.106/s/gpu LR: 2.360801e-09 Logit Scale: 100.000 Contrastive_loss: 0.18791 (0.18868) Loss: 0.18791 (0.18868)
2024-08-30,07:45:43 | INFO | Train Epoch: 69 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 524.000/s, 524.000/s/gpu LR: 2.351913e-09 Logit Scale: 100.000 Contrastive_loss: 0.21620 (0.19021) Loss: 0.21620 (0.19021)
2024-08-30,07:46:02 | INFO | Train Epoch: 69 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 506.346/s, 506.346/s/gpu LR: 2.343036e-09 Logit Scale: 100.000 Contrastive_loss: 0.16476 (0.18887) Loss: 0.16476 (0.18887)
2024-08-30,07:46:21 | INFO | Train Epoch: 69 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.192, 527.303/s, 527.303/s/gpu LR: 2.334170e-09 Logit Scale: 100.000 Contrastive_loss: 0.19195 (0.18902) Loss: 0.19195 (0.18902)
2024-08-30,07:46:41 | INFO | Train Epoch: 69 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 534.077/s, 534.077/s/gpu LR: 2.325317e-09 Logit Scale: 100.000 Contrastive_loss: 0.23438 (0.19118) Loss: 0.23438 (0.19118)
2024-08-30,07:47:00 | INFO | Train Epoch: 69 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.192, 495.966/s, 495.966/s/gpu LR: 2.316475e-09 Logit Scale: 100.000 Contrastive_loss: 0.15875 (0.18971) Loss: 0.15875 (0.18971)
2024-08-30,07:47:19 | INFO | Train Epoch: 69 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 487.065/s, 487.065/s/gpu LR: 2.307645e-09 Logit Scale: 100.000 Contrastive_loss: 0.24191 (0.19198) Loss: 0.24191 (0.19198)
2024-08-30,07:47:38 | INFO | Train Epoch: 69 [220100/145000.0 (152%)] Data (t): 0.107 Batch (t): 0.192, 533.304/s, 533.304/s/gpu LR: 2.298826e-09 Logit Scale: 100.000 Contrastive_loss: 0.29459 (0.19625) Loss: 0.29459 (0.19625)
2024-08-30,07:47:57 | INFO | Train Epoch: 69 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 534.165/s, 534.165/s/gpu LR: 2.290020e-09 Logit Scale: 100.000 Contrastive_loss: 0.28999 (0.20000) Loss: 0.28999 (0.20000)
2024-08-30,07:48:17 | INFO | Train Epoch: 69 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 502.148/s, 502.148/s/gpu LR: 2.281225e-09 Logit Scale: 100.000 Contrastive_loss: 0.15375 (0.19822) Loss: 0.15375 (0.19822)
2024-08-30,07:48:36 | INFO | Train Epoch: 69 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 532.997/s, 532.997/s/gpu LR: 2.272443e-09 Logit Scale: 100.000 Contrastive_loss: 0.20932 (0.19864) Loss: 0.20932 (0.19864)
2024-08-30,07:48:55 | INFO | Train Epoch: 69 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 498.879/s, 498.879/s/gpu LR: 2.263672e-09 Logit Scale: 100.000 Contrastive_loss: 0.20938 (0.19902) Loss: 0.20938 (0.19902)
2024-08-30,07:49:15 | INFO | Train Epoch: 69 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 502.823/s, 502.823/s/gpu LR: 2.254913e-09 Logit Scale: 100.000 Contrastive_loss: 0.24824 (0.20072) Loss: 0.24824 (0.20072)
2024-08-30,07:49:34 | INFO | Train Epoch: 69 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.192, 529.229/s, 529.229/s/gpu LR: 2.246167e-09 Logit Scale: 100.000 Contrastive_loss: 0.16300 (0.19946) Loss: 0.16300 (0.19946)
2024-08-30,07:49:53 | INFO | Train Epoch: 69 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 492.495/s, 492.495/s/gpu LR: 2.237432e-09 Logit Scale: 100.000 Contrastive_loss: 0.14116 (0.19758) Loss: 0.14116 (0.19758)
2024-08-30,07:50:12 | INFO | Train Epoch: 69 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 535.436/s, 535.436/s/gpu LR: 2.228710e-09 Logit Scale: 100.000 Contrastive_loss: 0.25219 (0.19929) Loss: 0.25219 (0.19929)
2024-08-30,07:50:32 | INFO | Train Epoch: 69 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 498.013/s, 498.013/s/gpu LR: 2.220000e-09 Logit Scale: 100.000 Contrastive_loss: 0.23852 (0.20047) Loss: 0.23852 (0.20047)
2024-08-30,07:50:51 | INFO | Train Epoch: 69 [320100/145000.0 (221%)] Data (t): 0.107 Batch (t): 0.193, 494.793/s, 494.793/s/gpu LR: 2.211302e-09 Logit Scale: 100.000 Contrastive_loss: 0.26983 (0.20251) Loss: 0.26983 (0.20251)
2024-08-30,07:51:10 | INFO | Train Epoch: 69 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 535.686/s, 535.686/s/gpu LR: 2.202616e-09 Logit Scale: 100.000 Contrastive_loss: 0.18026 (0.20188) Loss: 0.18026 (0.20188)
2024-08-30,07:51:30 | INFO | Train Epoch: 69 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 502.985/s, 502.985/s/gpu LR: 2.193942e-09 Logit Scale: 100.000 Contrastive_loss: 0.23434 (0.20278) Loss: 0.23434 (0.20278)
2024-08-30,07:51:49 | INFO | Train Epoch: 69 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.193, 521.757/s, 521.757/s/gpu LR: 2.185281e-09 Logit Scale: 100.000 Contrastive_loss: 0.28940 (0.20512) Loss: 0.28940 (0.20512)
2024-08-30,07:52:08 | INFO | Train Epoch: 69 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 495.049/s, 495.049/s/gpu LR: 2.176632e-09 Logit Scale: 100.000 Contrastive_loss: 0.14726 (0.20360) Loss: 0.14726 (0.20360)
2024-08-30,07:52:27 | INFO | Train Epoch: 69 [370100/145000.0 (255%)] Data (t): 0.107 Batch (t): 0.192, 531.898/s, 531.898/s/gpu LR: 2.167995e-09 Logit Scale: 100.000 Contrastive_loss: 0.24886 (0.20476) Loss: 0.24886 (0.20476)
2024-08-30,07:52:46 | INFO | Train Epoch: 69 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 494.810/s, 494.810/s/gpu LR: 2.159371e-09 Logit Scale: 100.000 Contrastive_loss: 0.22650 (0.20530) Loss: 0.22650 (0.20530)
2024-08-30,07:53:06 | INFO | Train Epoch: 69 [390100/145000.0 (269%)] Data (t): 0.107 Batch (t): 0.192, 498.917/s, 498.917/s/gpu LR: 2.150760e-09 Logit Scale: 100.000 Contrastive_loss: 0.12411 (0.20332) Loss: 0.12411 (0.20332)
2024-08-30,07:53:25 | INFO | Train Epoch: 69 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.192, 532.126/s, 532.126/s/gpu LR: 2.142161e-09 Logit Scale: 100.000 Contrastive_loss: 0.21444 (0.20359) Loss: 0.21444 (0.20359)
2024-08-30,07:53:44 | INFO | Train Epoch: 69 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 528.989/s, 528.989/s/gpu LR: 2.133574e-09 Logit Scale: 100.000 Contrastive_loss: 0.083669 (0.20080) Loss: 0.083669 (0.20080)
2024-08-30,07:53:52 | INFO | Eval Epoch: 70 [200 / 1000]	Clip Loss: 0.483567	
2024-08-30,07:53:52 | INFO | Eval Epoch: 70 image_to_text_mean_rank: 2.4070	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6710	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9290	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9060	text_to_image_R@10: 0.9660	clip_val_loss: 0.4402	epoch: 70.0000	num_samples: 1000.0000
2024-08-30,07:53:54 | INFO | Start epoch 70
2024-08-30,07:53:54 | INFO | Train Epoch: 70 [   100/145000.0 (0%)] Data (t): 0.023 Batch (t): 0.105, 952.835/s, 952.835/s/gpu LR: 2.365250e-09 Logit Scale: 100.000 Contrastive_loss: 0.16183 (0.16183) Loss: 0.16183 (0.16183)
2024-08-30,07:54:13 | INFO | Train Epoch: 70 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 495.073/s, 495.073/s/gpu LR: 2.356355e-09 Logit Scale: 100.000 Contrastive_loss: 0.32718 (0.24450) Loss: 0.32718 (0.24450)
2024-08-30,07:54:33 | INFO | Train Epoch: 70 [ 20100/145000.0 (14%)] Data (t): 0.110 Batch (t): 0.194, 502.532/s, 502.532/s/gpu LR: 2.347473e-09 Logit Scale: 100.000 Contrastive_loss: 0.33354 (0.27418) Loss: 0.33354 (0.27418)
2024-08-30,07:54:52 | INFO | Train Epoch: 70 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.194, 523.086/s, 523.086/s/gpu LR: 2.338602e-09 Logit Scale: 100.000 Contrastive_loss: 0.17119 (0.24844) Loss: 0.17119 (0.24844)
2024-08-30,07:55:11 | INFO | Train Epoch: 70 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 506.904/s, 506.904/s/gpu LR: 2.329742e-09 Logit Scale: 100.000 Contrastive_loss: 0.12866 (0.22448) Loss: 0.12866 (0.22448)
2024-08-30,07:55:31 | INFO | Train Epoch: 70 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 531.746/s, 531.746/s/gpu LR: 2.320894e-09 Logit Scale: 100.000 Contrastive_loss: 0.15028 (0.21211) Loss: 0.15028 (0.21211)
2024-08-30,07:55:50 | INFO | Train Epoch: 70 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 504.335/s, 504.335/s/gpu LR: 2.312058e-09 Logit Scale: 100.000 Contrastive_loss: 0.11168 (0.19777) Loss: 0.11168 (0.19777)
2024-08-30,07:56:09 | INFO | Train Epoch: 70 [ 70100/145000.0 (48%)] Data (t): 0.107 Batch (t): 0.192, 518.444/s, 518.444/s/gpu LR: 2.303234e-09 Logit Scale: 100.000 Contrastive_loss: 0.20526 (0.19870) Loss: 0.20526 (0.19870)
2024-08-30,07:56:28 | INFO | Train Epoch: 70 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 499.711/s, 499.711/s/gpu LR: 2.294422e-09 Logit Scale: 100.000 Contrastive_loss: 0.12282 (0.19027) Loss: 0.12282 (0.19027)
2024-08-30,07:56:48 | INFO | Train Epoch: 70 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 523.959/s, 523.959/s/gpu LR: 2.285621e-09 Logit Scale: 100.000 Contrastive_loss: 0.20361 (0.19161) Loss: 0.20361 (0.19161)
2024-08-30,07:57:07 | INFO | Train Epoch: 70 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 526.030/s, 526.030/s/gpu LR: 2.276833e-09 Logit Scale: 100.000 Contrastive_loss: 0.14569 (0.18743) Loss: 0.14569 (0.18743)
2024-08-30,07:57:26 | INFO | Train Epoch: 70 [110100/145000.0 (76%)] Data (t): 0.107 Batch (t): 0.192, 523.257/s, 523.257/s/gpu LR: 2.268056e-09 Logit Scale: 100.000 Contrastive_loss: 0.16290 (0.18539) Loss: 0.16290 (0.18539)
2024-08-30,07:57:46 | INFO | Train Epoch: 70 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.192, 532.348/s, 532.348/s/gpu LR: 2.259291e-09 Logit Scale: 100.000 Contrastive_loss: 0.26380 (0.19142) Loss: 0.26380 (0.19142)
2024-08-30,07:58:05 | INFO | Train Epoch: 70 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 534.014/s, 534.014/s/gpu LR: 2.250539e-09 Logit Scale: 100.000 Contrastive_loss: 0.14674 (0.18823) Loss: 0.14674 (0.18823)
2024-08-30,07:58:24 | INFO | Train Epoch: 70 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 495.669/s, 495.669/s/gpu LR: 2.241798e-09 Logit Scale: 100.000 Contrastive_loss: 0.17297 (0.18721) Loss: 0.17297 (0.18721)
2024-08-30,07:58:34 | INFO | Train Epoch: 70 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 504.080/s, 504.080/s/gpu LR: 2.237520e-09 Logit Scale: 100.000 Contrastive_loss: 0.20685 (0.18844) Loss: 0.20685 (0.18844)
2024-08-30,07:58:43 | INFO | Train Epoch: 70 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 501.454/s, 501.454/s/gpu LR: 2.233070e-09 Logit Scale: 100.000 Contrastive_loss: 0.18765 (0.18839) Loss: 0.18765 (0.18839)
2024-08-30,07:59:03 | INFO | Train Epoch: 70 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 528.129/s, 528.129/s/gpu LR: 2.224353e-09 Logit Scale: 100.000 Contrastive_loss: 0.21574 (0.18991) Loss: 0.21574 (0.18991)
2024-08-30,07:59:22 | INFO | Train Epoch: 70 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 506.874/s, 506.874/s/gpu LR: 2.215649e-09 Logit Scale: 100.000 Contrastive_loss: 0.16437 (0.18857) Loss: 0.16437 (0.18857)
2024-08-30,07:59:41 | INFO | Train Epoch: 70 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 501.210/s, 501.210/s/gpu LR: 2.206957e-09 Logit Scale: 100.000 Contrastive_loss: 0.19171 (0.18872) Loss: 0.19171 (0.18872)
2024-08-30,08:00:01 | INFO | Train Epoch: 70 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 495.090/s, 495.090/s/gpu LR: 2.198277e-09 Logit Scale: 100.000 Contrastive_loss: 0.23409 (0.19088) Loss: 0.23409 (0.19088)
2024-08-30,08:00:20 | INFO | Train Epoch: 70 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 539.245/s, 539.245/s/gpu LR: 2.189610e-09 Logit Scale: 100.000 Contrastive_loss: 0.15861 (0.18942) Loss: 0.15861 (0.18942)
2024-08-30,08:00:39 | INFO | Train Epoch: 70 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 499.738/s, 499.738/s/gpu LR: 2.180955e-09 Logit Scale: 100.000 Contrastive_loss: 0.24162 (0.19169) Loss: 0.24162 (0.19169)
2024-08-30,08:00:58 | INFO | Train Epoch: 70 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 533.958/s, 533.958/s/gpu LR: 2.172312e-09 Logit Scale: 100.000 Contrastive_loss: 0.29425 (0.19596) Loss: 0.29425 (0.19596)
2024-08-30,08:01:18 | INFO | Train Epoch: 70 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 532.521/s, 532.521/s/gpu LR: 2.163682e-09 Logit Scale: 100.000 Contrastive_loss: 0.28963 (0.19971) Loss: 0.28963 (0.19971)
2024-08-30,08:01:37 | INFO | Train Epoch: 70 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 529.445/s, 529.445/s/gpu LR: 2.155064e-09 Logit Scale: 100.000 Contrastive_loss: 0.15339 (0.19793) Loss: 0.15339 (0.19793)
2024-08-30,08:01:56 | INFO | Train Epoch: 70 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 529.427/s, 529.427/s/gpu LR: 2.146459e-09 Logit Scale: 100.000 Contrastive_loss: 0.20895 (0.19833) Loss: 0.20895 (0.19833)
2024-08-30,08:02:15 | INFO | Train Epoch: 70 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 505.474/s, 505.474/s/gpu LR: 2.137866e-09 Logit Scale: 100.000 Contrastive_loss: 0.20904 (0.19872) Loss: 0.20904 (0.19872)
2024-08-30,08:02:35 | INFO | Train Epoch: 70 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 531.783/s, 531.783/s/gpu LR: 2.129285e-09 Logit Scale: 100.000 Contrastive_loss: 0.24785 (0.20041) Loss: 0.24785 (0.20041)
2024-08-30,08:02:54 | INFO | Train Epoch: 70 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.194, 505.592/s, 505.592/s/gpu LR: 2.120718e-09 Logit Scale: 100.000 Contrastive_loss: 0.16282 (0.19916) Loss: 0.16282 (0.19916)
2024-08-30,08:03:13 | INFO | Train Epoch: 70 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 501.301/s, 501.301/s/gpu LR: 2.112162e-09 Logit Scale: 100.000 Contrastive_loss: 0.14105 (0.19728) Loss: 0.14105 (0.19728)
2024-08-30,08:03:33 | INFO | Train Epoch: 70 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 491.332/s, 491.332/s/gpu LR: 2.103620e-09 Logit Scale: 100.000 Contrastive_loss: 0.25179 (0.19899) Loss: 0.25179 (0.19899)
2024-08-30,08:03:52 | INFO | Train Epoch: 70 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 538.205/s, 538.205/s/gpu LR: 2.095090e-09 Logit Scale: 100.000 Contrastive_loss: 0.23841 (0.20018) Loss: 0.23841 (0.20018)
2024-08-30,08:04:11 | INFO | Train Epoch: 70 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 502.234/s, 502.234/s/gpu LR: 2.086573e-09 Logit Scale: 100.000 Contrastive_loss: 0.26949 (0.20222) Loss: 0.26949 (0.20222)
2024-08-30,08:04:30 | INFO | Train Epoch: 70 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.192, 532.664/s, 532.664/s/gpu LR: 2.078069e-09 Logit Scale: 100.000 Contrastive_loss: 0.18005 (0.20159) Loss: 0.18005 (0.20159)
2024-08-30,08:04:50 | INFO | Train Epoch: 70 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 535.245/s, 535.245/s/gpu LR: 2.069578e-09 Logit Scale: 100.000 Contrastive_loss: 0.23387 (0.20248) Loss: 0.23387 (0.20248)
2024-08-30,08:05:09 | INFO | Train Epoch: 70 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 496.960/s, 496.960/s/gpu LR: 2.061099e-09 Logit Scale: 100.000 Contrastive_loss: 0.28899 (0.20482) Loss: 0.28899 (0.20482)
2024-08-30,08:05:28 | INFO | Train Epoch: 70 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 530.336/s, 530.336/s/gpu LR: 2.052634e-09 Logit Scale: 100.000 Contrastive_loss: 0.14698 (0.20330) Loss: 0.14698 (0.20330)
2024-08-30,08:05:47 | INFO | Train Epoch: 70 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.192, 536.006/s, 536.006/s/gpu LR: 2.044181e-09 Logit Scale: 100.000 Contrastive_loss: 0.24861 (0.20446) Loss: 0.24861 (0.20446)
2024-08-30,08:06:07 | INFO | Train Epoch: 70 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 508.478/s, 508.478/s/gpu LR: 2.035741e-09 Logit Scale: 100.000 Contrastive_loss: 0.22623 (0.20500) Loss: 0.22623 (0.20500)
2024-08-30,08:06:26 | INFO | Train Epoch: 70 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 501.243/s, 501.243/s/gpu LR: 2.027314e-09 Logit Scale: 100.000 Contrastive_loss: 0.12380 (0.20302) Loss: 0.12380 (0.20302)
2024-08-30,08:06:45 | INFO | Train Epoch: 70 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 496.557/s, 496.557/s/gpu LR: 2.018901e-09 Logit Scale: 100.000 Contrastive_loss: 0.21415 (0.20329) Loss: 0.21415 (0.20329)
2024-08-30,08:07:04 | INFO | Train Epoch: 70 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 508.761/s, 508.761/s/gpu LR: 2.010500e-09 Logit Scale: 100.000 Contrastive_loss: 0.083525 (0.20050) Loss: 0.083525 (0.20050)
2024-08-30,08:07:12 | INFO | Eval Epoch: 71 [200 / 1000]	Clip Loss: 0.483713	
2024-08-30,08:07:12 | INFO | Eval Epoch: 71 image_to_text_mean_rank: 2.4050	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6740	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9290	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4403	epoch: 71.0000	num_samples: 1000.0000
2024-08-30,08:07:14 | INFO | Start epoch 71
2024-08-30,08:07:14 | INFO | Train Epoch: 71 [   100/145000.0 (0%)] Data (t): 0.025 Batch (t): 0.112, 889.951/s, 889.951/s/gpu LR: 2.237432e-09 Logit Scale: 100.000 Contrastive_loss: 0.16152 (0.16152) Loss: 0.16152 (0.16152)
2024-08-30,08:07:33 | INFO | Train Epoch: 71 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 513.239/s, 513.239/s/gpu LR: 2.228710e-09 Logit Scale: 100.000 Contrastive_loss: 0.32669 (0.24411) Loss: 0.32669 (0.24411)
2024-08-30,08:07:53 | INFO | Train Epoch: 71 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 497.537/s, 497.537/s/gpu LR: 2.220000e-09 Logit Scale: 100.000 Contrastive_loss: 0.33281 (0.27368) Loss: 0.33281 (0.27368)
2024-08-30,08:08:12 | INFO | Train Epoch: 71 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 498.140/s, 498.140/s/gpu LR: 2.211302e-09 Logit Scale: 100.000 Contrastive_loss: 0.17086 (0.24797) Loss: 0.17086 (0.24797)
2024-08-30,08:08:31 | INFO | Train Epoch: 71 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 527.922/s, 527.922/s/gpu LR: 2.202616e-09 Logit Scale: 100.000 Contrastive_loss: 0.12849 (0.22407) Loss: 0.12849 (0.22407)
2024-08-30,08:08:50 | INFO | Train Epoch: 71 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 531.624/s, 531.624/s/gpu LR: 2.193942e-09 Logit Scale: 100.000 Contrastive_loss: 0.14999 (0.21173) Loss: 0.14999 (0.21173)
2024-08-30,08:09:10 | INFO | Train Epoch: 71 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 526.754/s, 526.754/s/gpu LR: 2.185281e-09 Logit Scale: 100.000 Contrastive_loss: 0.11144 (0.19740) Loss: 0.11144 (0.19740)
2024-08-30,08:09:29 | INFO | Train Epoch: 71 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 506.062/s, 506.062/s/gpu LR: 2.176632e-09 Logit Scale: 100.000 Contrastive_loss: 0.20484 (0.19833) Loss: 0.20484 (0.19833)
2024-08-30,08:09:48 | INFO | Train Epoch: 71 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 504.038/s, 504.038/s/gpu LR: 2.167995e-09 Logit Scale: 100.000 Contrastive_loss: 0.12257 (0.18991) Loss: 0.12257 (0.18991)
2024-08-30,08:10:08 | INFO | Train Epoch: 71 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 499.177/s, 499.177/s/gpu LR: 2.159371e-09 Logit Scale: 100.000 Contrastive_loss: 0.20327 (0.19125) Loss: 0.20327 (0.19125)
2024-08-30,08:10:27 | INFO | Train Epoch: 71 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 529.550/s, 529.550/s/gpu LR: 2.150760e-09 Logit Scale: 100.000 Contrastive_loss: 0.14550 (0.18709) Loss: 0.14550 (0.18709)
2024-08-30,08:10:46 | INFO | Train Epoch: 71 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 537.045/s, 537.045/s/gpu LR: 2.142161e-09 Logit Scale: 100.000 Contrastive_loss: 0.16269 (0.18506) Loss: 0.16269 (0.18506)
2024-08-30,08:11:06 | INFO | Train Epoch: 71 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 541.143/s, 541.143/s/gpu LR: 2.133574e-09 Logit Scale: 100.000 Contrastive_loss: 0.26354 (0.19109) Loss: 0.26354 (0.19109)
2024-08-30,08:11:25 | INFO | Train Epoch: 71 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 497.667/s, 497.667/s/gpu LR: 2.125000e-09 Logit Scale: 100.000 Contrastive_loss: 0.14643 (0.18790) Loss: 0.14643 (0.18790)
2024-08-30,08:11:44 | INFO | Train Epoch: 71 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.192, 488.020/s, 488.020/s/gpu LR: 2.116438e-09 Logit Scale: 100.000 Contrastive_loss: 0.17271 (0.18689) Loss: 0.17271 (0.18689)
2024-08-30,08:11:53 | INFO | Train Epoch: 71 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 508.829/s, 508.829/s/gpu LR: 2.112248e-09 Logit Scale: 100.000 Contrastive_loss: 0.20638 (0.18811) Loss: 0.20638 (0.18811)
2024-08-30,08:12:03 | INFO | Train Epoch: 71 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 502.181/s, 502.181/s/gpu LR: 2.107890e-09 Logit Scale: 100.000 Contrastive_loss: 0.18744 (0.18807) Loss: 0.18744 (0.18807)
2024-08-30,08:12:23 | INFO | Train Epoch: 71 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 496.794/s, 496.794/s/gpu LR: 2.099354e-09 Logit Scale: 100.000 Contrastive_loss: 0.21540 (0.18959) Loss: 0.21540 (0.18959)
2024-08-30,08:12:42 | INFO | Train Epoch: 71 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 526.259/s, 526.259/s/gpu LR: 2.090830e-09 Logit Scale: 100.000 Contrastive_loss: 0.16411 (0.18825) Loss: 0.16411 (0.18825)
2024-08-30,08:13:01 | INFO | Train Epoch: 71 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 538.426/s, 538.426/s/gpu LR: 2.082320e-09 Logit Scale: 100.000 Contrastive_loss: 0.19172 (0.18842) Loss: 0.19172 (0.18842)
2024-08-30,08:13:20 | INFO | Train Epoch: 71 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 534.483/s, 534.483/s/gpu LR: 2.073822e-09 Logit Scale: 100.000 Contrastive_loss: 0.23377 (0.19058) Loss: 0.23377 (0.19058)
2024-08-30,08:13:40 | INFO | Train Epoch: 71 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 490.897/s, 490.897/s/gpu LR: 2.065337e-09 Logit Scale: 100.000 Contrastive_loss: 0.15834 (0.18911) Loss: 0.15834 (0.18911)
2024-08-30,08:13:59 | INFO | Train Epoch: 71 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 502.279/s, 502.279/s/gpu LR: 2.056865e-09 Logit Scale: 100.000 Contrastive_loss: 0.24125 (0.19138) Loss: 0.24125 (0.19138)
2024-08-30,08:14:18 | INFO | Train Epoch: 71 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 500.612/s, 500.612/s/gpu LR: 2.048406e-09 Logit Scale: 100.000 Contrastive_loss: 0.29365 (0.19564) Loss: 0.29365 (0.19564)
2024-08-30,08:14:37 | INFO | Train Epoch: 71 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 541.908/s, 541.908/s/gpu LR: 2.039959e-09 Logit Scale: 100.000 Contrastive_loss: 0.28931 (0.19939) Loss: 0.28931 (0.19939)
2024-08-30,08:14:57 | INFO | Train Epoch: 71 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 496.883/s, 496.883/s/gpu LR: 2.031526e-09 Logit Scale: 100.000 Contrastive_loss: 0.15321 (0.19761) Loss: 0.15321 (0.19761)
2024-08-30,08:15:16 | INFO | Train Epoch: 71 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 520.221/s, 520.221/s/gpu LR: 2.023106e-09 Logit Scale: 100.000 Contrastive_loss: 0.20880 (0.19803) Loss: 0.20880 (0.19803)
2024-08-30,08:15:35 | INFO | Train Epoch: 71 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 528.736/s, 528.736/s/gpu LR: 2.014699e-09 Logit Scale: 100.000 Contrastive_loss: 0.20882 (0.19841) Loss: 0.20882 (0.19841)
2024-08-30,08:15:55 | INFO | Train Epoch: 71 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 496.571/s, 496.571/s/gpu LR: 2.006305e-09 Logit Scale: 100.000 Contrastive_loss: 0.24768 (0.20011) Loss: 0.24768 (0.20011)
2024-08-30,08:16:14 | INFO | Train Epoch: 71 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 527.273/s, 527.273/s/gpu LR: 1.997924e-09 Logit Scale: 100.000 Contrastive_loss: 0.16265 (0.19886) Loss: 0.16265 (0.19886)
2024-08-30,08:16:33 | INFO | Train Epoch: 71 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.192, 523.944/s, 523.944/s/gpu LR: 1.989556e-09 Logit Scale: 100.000 Contrastive_loss: 0.14097 (0.19700) Loss: 0.14097 (0.19700)
2024-08-30,08:16:52 | INFO | Train Epoch: 71 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 489.085/s, 489.085/s/gpu LR: 1.981201e-09 Logit Scale: 100.000 Contrastive_loss: 0.25146 (0.19870) Loss: 0.25146 (0.19870)
2024-08-30,08:17:12 | INFO | Train Epoch: 71 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 522.385/s, 522.385/s/gpu LR: 1.972860e-09 Logit Scale: 100.000 Contrastive_loss: 0.23832 (0.19990) Loss: 0.23832 (0.19990)
2024-08-30,08:17:31 | INFO | Train Epoch: 71 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 499.365/s, 499.365/s/gpu LR: 1.964532e-09 Logit Scale: 100.000 Contrastive_loss: 0.26925 (0.20194) Loss: 0.26925 (0.20194)
2024-08-30,08:17:50 | INFO | Train Epoch: 71 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 525.063/s, 525.063/s/gpu LR: 1.956217e-09 Logit Scale: 100.000 Contrastive_loss: 0.17996 (0.20131) Loss: 0.17996 (0.20131)
2024-08-30,08:18:10 | INFO | Train Epoch: 71 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 523.379/s, 523.379/s/gpu LR: 1.947916e-09 Logit Scale: 100.000 Contrastive_loss: 0.23351 (0.20220) Loss: 0.23351 (0.20220)
2024-08-30,08:18:29 | INFO | Train Epoch: 71 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.192, 502.033/s, 502.033/s/gpu LR: 1.939628e-09 Logit Scale: 100.000 Contrastive_loss: 0.28867 (0.20454) Loss: 0.28867 (0.20454)
2024-08-30,08:18:48 | INFO | Train Epoch: 71 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 526.114/s, 526.114/s/gpu LR: 1.931353e-09 Logit Scale: 100.000 Contrastive_loss: 0.14675 (0.20302) Loss: 0.14675 (0.20302)
2024-08-30,08:19:08 | INFO | Train Epoch: 71 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 523.799/s, 523.799/s/gpu LR: 1.923092e-09 Logit Scale: 100.000 Contrastive_loss: 0.24828 (0.20418) Loss: 0.24828 (0.20418)
2024-08-30,08:19:27 | INFO | Train Epoch: 71 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 526.208/s, 526.208/s/gpu LR: 1.914844e-09 Logit Scale: 100.000 Contrastive_loss: 0.22594 (0.20473) Loss: 0.22594 (0.20473)
2024-08-30,08:19:46 | INFO | Train Epoch: 71 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 529.893/s, 529.893/s/gpu LR: 1.906610e-09 Logit Scale: 100.000 Contrastive_loss: 0.12367 (0.20275) Loss: 0.12367 (0.20275)
2024-08-30,08:20:05 | INFO | Train Epoch: 71 [400100/145000.0 (276%)] Data (t): 0.107 Batch (t): 0.192, 540.342/s, 540.342/s/gpu LR: 1.898390e-09 Logit Scale: 100.000 Contrastive_loss: 0.21389 (0.20301) Loss: 0.21389 (0.20301)
2024-08-30,08:20:24 | INFO | Train Epoch: 71 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.192, 526.672/s, 526.672/s/gpu LR: 1.890183e-09 Logit Scale: 100.000 Contrastive_loss: 0.083434 (0.20023) Loss: 0.083434 (0.20023)
2024-08-30,08:20:32 | INFO | Eval Epoch: 72 [200 / 1000]	Clip Loss: 0.483620	
2024-08-30,08:20:33 | INFO | Eval Epoch: 72 image_to_text_mean_rank: 2.4050	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6730	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9280	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4403	epoch: 72.0000	num_samples: 1000.0000
2024-08-30,08:20:34 | INFO | Start epoch 72
2024-08-30,08:20:34 | INFO | Train Epoch: 72 [   100/145000.0 (0%)] Data (t): 0.035 Batch (t): 0.127, 790.450/s, 790.450/s/gpu LR: 2.112162e-09 Logit Scale: 100.000 Contrastive_loss: 0.16114 (0.16114) Loss: 0.16114 (0.16114)
2024-08-30,08:20:54 | INFO | Train Epoch: 72 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.195, 499.496/s, 499.496/s/gpu LR: 2.103620e-09 Logit Scale: 100.000 Contrastive_loss: 0.32641 (0.24377) Loss: 0.32641 (0.24377)
2024-08-30,08:21:13 | INFO | Train Epoch: 72 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 505.928/s, 505.928/s/gpu LR: 2.095090e-09 Logit Scale: 100.000 Contrastive_loss: 0.33234 (0.27329) Loss: 0.33234 (0.27329)
2024-08-30,08:21:32 | INFO | Train Epoch: 72 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.192, 531.633/s, 531.633/s/gpu LR: 2.086573e-09 Logit Scale: 100.000 Contrastive_loss: 0.17047 (0.24759) Loss: 0.17047 (0.24759)
2024-08-30,08:21:51 | INFO | Train Epoch: 72 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 534.836/s, 534.836/s/gpu LR: 2.078069e-09 Logit Scale: 100.000 Contrastive_loss: 0.12835 (0.22374) Loss: 0.12835 (0.22374)
2024-08-30,08:22:11 | INFO | Train Epoch: 72 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.192, 540.229/s, 540.229/s/gpu LR: 2.069578e-09 Logit Scale: 100.000 Contrastive_loss: 0.14976 (0.21141) Loss: 0.14976 (0.21141)
2024-08-30,08:22:30 | INFO | Train Epoch: 72 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 499.906/s, 499.906/s/gpu LR: 2.061099e-09 Logit Scale: 100.000 Contrastive_loss: 0.11124 (0.19710) Loss: 0.11124 (0.19710)
2024-08-30,08:22:49 | INFO | Train Epoch: 72 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 536.532/s, 536.532/s/gpu LR: 2.052634e-09 Logit Scale: 100.000 Contrastive_loss: 0.20445 (0.19802) Loss: 0.20445 (0.19802)
2024-08-30,08:23:08 | INFO | Train Epoch: 72 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.192, 531.923/s, 531.923/s/gpu LR: 2.044181e-09 Logit Scale: 100.000 Contrastive_loss: 0.12236 (0.18961) Loss: 0.12236 (0.18961)
2024-08-30,08:23:28 | INFO | Train Epoch: 72 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 491.463/s, 491.463/s/gpu LR: 2.035741e-09 Logit Scale: 100.000 Contrastive_loss: 0.20296 (0.19095) Loss: 0.20296 (0.19095)
2024-08-30,08:23:47 | INFO | Train Epoch: 72 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 495.209/s, 495.209/s/gpu LR: 2.027314e-09 Logit Scale: 100.000 Contrastive_loss: 0.14531 (0.18680) Loss: 0.14531 (0.18680)
2024-08-30,08:24:06 | INFO | Train Epoch: 72 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 498.830/s, 498.830/s/gpu LR: 2.018901e-09 Logit Scale: 100.000 Contrastive_loss: 0.16250 (0.18477) Loss: 0.16250 (0.18477)
2024-08-30,08:24:26 | INFO | Train Epoch: 72 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 532.894/s, 532.894/s/gpu LR: 2.010500e-09 Logit Scale: 100.000 Contrastive_loss: 0.26328 (0.19081) Loss: 0.26328 (0.19081)
2024-08-30,08:24:45 | INFO | Train Epoch: 72 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 521.655/s, 521.655/s/gpu LR: 2.002112e-09 Logit Scale: 100.000 Contrastive_loss: 0.14626 (0.18763) Loss: 0.14626 (0.18763)
2024-08-30,08:25:04 | INFO | Train Epoch: 72 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 528.633/s, 528.633/s/gpu LR: 1.993738e-09 Logit Scale: 100.000 Contrastive_loss: 0.17247 (0.18662) Loss: 0.17247 (0.18662)
2024-08-30,08:25:14 | INFO | Train Epoch: 72 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.193, 497.027/s, 497.027/s/gpu LR: 1.989639e-09 Logit Scale: 100.000 Contrastive_loss: 0.20588 (0.18782) Loss: 0.20588 (0.18782)
2024-08-30,08:25:24 | INFO | Train Epoch: 72 [150100/145000.0 (104%)] Data (t): 0.110 Batch (t): 0.194, 493.825/s, 493.825/s/gpu LR: 1.985377e-09 Logit Scale: 100.000 Contrastive_loss: 0.18697 (0.18777) Loss: 0.18697 (0.18777)
2024-08-30,08:25:43 | INFO | Train Epoch: 72 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 524.737/s, 524.737/s/gpu LR: 1.977029e-09 Logit Scale: 100.000 Contrastive_loss: 0.21495 (0.18928) Loss: 0.21495 (0.18928)
2024-08-30,08:26:02 | INFO | Train Epoch: 72 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 496.855/s, 496.855/s/gpu LR: 1.968694e-09 Logit Scale: 100.000 Contrastive_loss: 0.16398 (0.18795) Loss: 0.16398 (0.18795)
2024-08-30,08:26:21 | INFO | Train Epoch: 72 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 537.737/s, 537.737/s/gpu LR: 1.960373e-09 Logit Scale: 100.000 Contrastive_loss: 0.19152 (0.18813) Loss: 0.19152 (0.18813)
2024-08-30,08:26:41 | INFO | Train Epoch: 72 [190100/145000.0 (131%)] Data (t): 0.109 Batch (t): 0.193, 492.224/s, 492.224/s/gpu LR: 1.952065e-09 Logit Scale: 100.000 Contrastive_loss: 0.23340 (0.19029) Loss: 0.23340 (0.19029)
2024-08-30,08:27:00 | INFO | Train Epoch: 72 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 531.476/s, 531.476/s/gpu LR: 1.943770e-09 Logit Scale: 100.000 Contrastive_loss: 0.15821 (0.18883) Loss: 0.15821 (0.18883)
2024-08-30,08:27:19 | INFO | Train Epoch: 72 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 523.026/s, 523.026/s/gpu LR: 1.935489e-09 Logit Scale: 100.000 Contrastive_loss: 0.24104 (0.19110) Loss: 0.24104 (0.19110)
2024-08-30,08:27:39 | INFO | Train Epoch: 72 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 494.549/s, 494.549/s/gpu LR: 1.927221e-09 Logit Scale: 100.000 Contrastive_loss: 0.29331 (0.19536) Loss: 0.29331 (0.19536)
2024-08-30,08:27:58 | INFO | Train Epoch: 72 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 499.438/s, 499.438/s/gpu LR: 1.918967e-09 Logit Scale: 100.000 Contrastive_loss: 0.28898 (0.19910) Loss: 0.28898 (0.19910)
2024-08-30,08:28:17 | INFO | Train Epoch: 72 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 494.076/s, 494.076/s/gpu LR: 1.910726e-09 Logit Scale: 100.000 Contrastive_loss: 0.15293 (0.19733) Loss: 0.15293 (0.19733)
2024-08-30,08:28:36 | INFO | Train Epoch: 72 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 532.862/s, 532.862/s/gpu LR: 1.902498e-09 Logit Scale: 100.000 Contrastive_loss: 0.20855 (0.19774) Loss: 0.20855 (0.19774)
2024-08-30,08:28:56 | INFO | Train Epoch: 72 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 538.250/s, 538.250/s/gpu LR: 1.894285e-09 Logit Scale: 100.000 Contrastive_loss: 0.20856 (0.19813) Loss: 0.20856 (0.19813)
2024-08-30,08:29:15 | INFO | Train Epoch: 72 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 491.805/s, 491.805/s/gpu LR: 1.886085e-09 Logit Scale: 100.000 Contrastive_loss: 0.24744 (0.19983) Loss: 0.24744 (0.19983)
2024-08-30,08:29:34 | INFO | Train Epoch: 72 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.192, 492.547/s, 492.547/s/gpu LR: 1.877898e-09 Logit Scale: 100.000 Contrastive_loss: 0.16236 (0.19858) Loss: 0.16236 (0.19858)
2024-08-30,08:29:53 | INFO | Train Epoch: 72 [290100/145000.0 (200%)] Data (t): 0.107 Batch (t): 0.192, 528.566/s, 528.566/s/gpu LR: 1.869725e-09 Logit Scale: 100.000 Contrastive_loss: 0.14087 (0.19672) Loss: 0.14087 (0.19672)
2024-08-30,08:30:13 | INFO | Train Epoch: 72 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 536.749/s, 536.749/s/gpu LR: 1.861566e-09 Logit Scale: 100.000 Contrastive_loss: 0.25123 (0.19842) Loss: 0.25123 (0.19842)
2024-08-30,08:30:32 | INFO | Train Epoch: 72 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 534.885/s, 534.885/s/gpu LR: 1.853421e-09 Logit Scale: 100.000 Contrastive_loss: 0.23808 (0.19962) Loss: 0.23808 (0.19962)
2024-08-30,08:30:51 | INFO | Train Epoch: 72 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 498.736/s, 498.736/s/gpu LR: 1.845290e-09 Logit Scale: 100.000 Contrastive_loss: 0.26889 (0.20166) Loss: 0.26889 (0.20166)
2024-08-30,08:31:10 | INFO | Train Epoch: 72 [330100/145000.0 (228%)] Data (t): 0.107 Batch (t): 0.193, 535.181/s, 535.181/s/gpu LR: 1.837172e-09 Logit Scale: 100.000 Contrastive_loss: 0.17979 (0.20104) Loss: 0.17979 (0.20104)
2024-08-30,08:31:30 | INFO | Train Epoch: 72 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 502.323/s, 502.323/s/gpu LR: 1.829068e-09 Logit Scale: 100.000 Contrastive_loss: 0.23306 (0.20192) Loss: 0.23306 (0.20192)
2024-08-30,08:31:49 | INFO | Train Epoch: 72 [350100/145000.0 (241%)] Data (t): 0.107 Batch (t): 0.192, 529.095/s, 529.095/s/gpu LR: 1.820979e-09 Logit Scale: 100.000 Contrastive_loss: 0.28859 (0.20427) Loss: 0.28859 (0.20427)
2024-08-30,08:32:08 | INFO | Train Epoch: 72 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.192, 501.424/s, 501.424/s/gpu LR: 1.812903e-09 Logit Scale: 100.000 Contrastive_loss: 0.14653 (0.20275) Loss: 0.14653 (0.20275)
2024-08-30,08:32:27 | INFO | Train Epoch: 72 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 496.841/s, 496.841/s/gpu LR: 1.804841e-09 Logit Scale: 100.000 Contrastive_loss: 0.24792 (0.20391) Loss: 0.24792 (0.20391)
2024-08-30,08:32:47 | INFO | Train Epoch: 72 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 507.237/s, 507.237/s/gpu LR: 1.796793e-09 Logit Scale: 100.000 Contrastive_loss: 0.22578 (0.20445) Loss: 0.22578 (0.20445)
2024-08-30,08:33:06 | INFO | Train Epoch: 72 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 509.828/s, 509.828/s/gpu LR: 1.788759e-09 Logit Scale: 100.000 Contrastive_loss: 0.12340 (0.20248) Loss: 0.12340 (0.20248)
2024-08-30,08:33:25 | INFO | Train Epoch: 72 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.192, 501.865/s, 501.865/s/gpu LR: 1.780739e-09 Logit Scale: 100.000 Contrastive_loss: 0.21372 (0.20274) Loss: 0.21372 (0.20274)
2024-08-30,08:33:44 | INFO | Train Epoch: 72 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 523.209/s, 523.209/s/gpu LR: 1.772734e-09 Logit Scale: 100.000 Contrastive_loss: 0.083181 (0.19996) Loss: 0.083181 (0.19996)
2024-08-30,08:33:52 | INFO | Eval Epoch: 73 [200 / 1000]	Clip Loss: 0.483730	
2024-08-30,08:33:53 | INFO | Eval Epoch: 73 image_to_text_mean_rank: 2.4030	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6730	image_to_text_R@5: 0.9090	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9360	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4404	epoch: 73.0000	num_samples: 1000.0000
2024-08-30,08:33:54 | INFO | Start epoch 73
2024-08-30,08:33:54 | INFO | Train Epoch: 73 [   100/145000.0 (0%)] Data (t): 0.022 Batch (t): 0.110, 905.010/s, 905.010/s/gpu LR: 1.989556e-09 Logit Scale: 100.000 Contrastive_loss: 0.16104 (0.16104) Loss: 0.16104 (0.16104)
2024-08-30,08:34:14 | INFO | Train Epoch: 73 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 529.916/s, 529.916/s/gpu LR: 1.981201e-09 Logit Scale: 100.000 Contrastive_loss: 0.32606 (0.24355) Loss: 0.32606 (0.24355)
2024-08-30,08:34:33 | INFO | Train Epoch: 73 [ 20100/145000.0 (14%)] Data (t): 0.110 Batch (t): 0.194, 497.055/s, 497.055/s/gpu LR: 1.972860e-09 Logit Scale: 100.000 Contrastive_loss: 0.33189 (0.27300) Loss: 0.33189 (0.27300)
2024-08-30,08:34:52 | INFO | Train Epoch: 73 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 532.001/s, 532.001/s/gpu LR: 1.964532e-09 Logit Scale: 100.000 Contrastive_loss: 0.17022 (0.24731) Loss: 0.17022 (0.24731)
2024-08-30,08:35:12 | INFO | Train Epoch: 73 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.194, 527.247/s, 527.247/s/gpu LR: 1.956217e-09 Logit Scale: 100.000 Contrastive_loss: 0.12817 (0.22348) Loss: 0.12817 (0.22348)
2024-08-30,08:35:31 | INFO | Train Epoch: 73 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 537.080/s, 537.080/s/gpu LR: 1.947916e-09 Logit Scale: 100.000 Contrastive_loss: 0.14942 (0.21113) Loss: 0.14942 (0.21113)
2024-08-30,08:35:50 | INFO | Train Epoch: 73 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 531.216/s, 531.216/s/gpu LR: 1.939628e-09 Logit Scale: 100.000 Contrastive_loss: 0.11107 (0.19684) Loss: 0.11107 (0.19684)
2024-08-30,08:36:10 | INFO | Train Epoch: 73 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 526.319/s, 526.319/s/gpu LR: 1.931353e-09 Logit Scale: 100.000 Contrastive_loss: 0.20400 (0.19774) Loss: 0.20400 (0.19774)
2024-08-30,08:36:29 | INFO | Train Epoch: 73 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 518.245/s, 518.245/s/gpu LR: 1.923092e-09 Logit Scale: 100.000 Contrastive_loss: 0.12211 (0.18933) Loss: 0.12211 (0.18933)
2024-08-30,08:36:48 | INFO | Train Epoch: 73 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.194, 493.064/s, 493.064/s/gpu LR: 1.914844e-09 Logit Scale: 100.000 Contrastive_loss: 0.20268 (0.19067) Loss: 0.20268 (0.19067)
2024-08-30,08:37:08 | INFO | Train Epoch: 73 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 509.513/s, 509.513/s/gpu LR: 1.906610e-09 Logit Scale: 100.000 Contrastive_loss: 0.14516 (0.18653) Loss: 0.14516 (0.18653)
2024-08-30,08:37:27 | INFO | Train Epoch: 73 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 533.215/s, 533.215/s/gpu LR: 1.898390e-09 Logit Scale: 100.000 Contrastive_loss: 0.16229 (0.18451) Loss: 0.16229 (0.18451)
2024-08-30,08:37:46 | INFO | Train Epoch: 73 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 501.752/s, 501.752/s/gpu LR: 1.890183e-09 Logit Scale: 100.000 Contrastive_loss: 0.26309 (0.19056) Loss: 0.26309 (0.19056)
2024-08-30,08:38:05 | INFO | Train Epoch: 73 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 531.510/s, 531.510/s/gpu LR: 1.881990e-09 Logit Scale: 100.000 Contrastive_loss: 0.14620 (0.18739) Loss: 0.14620 (0.18739)
2024-08-30,08:38:25 | INFO | Train Epoch: 73 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 553.973/s, 553.973/s/gpu LR: 1.873810e-09 Logit Scale: 100.000 Contrastive_loss: 0.17247 (0.18639) Loss: 0.17247 (0.18639)
2024-08-30,08:38:34 | INFO | Train Epoch: 73 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 541.812/s, 541.812/s/gpu LR: 1.869807e-09 Logit Scale: 100.000 Contrastive_loss: 0.20548 (0.18759) Loss: 0.20548 (0.18759)
2024-08-30,08:38:44 | INFO | Train Epoch: 73 [150100/145000.0 (104%)] Data (t): 0.110 Batch (t): 0.193, 526.966/s, 526.966/s/gpu LR: 1.865644e-09 Logit Scale: 100.000 Contrastive_loss: 0.18675 (0.18754) Loss: 0.18675 (0.18754)
2024-08-30,08:39:03 | INFO | Train Epoch: 73 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 494.789/s, 494.789/s/gpu LR: 1.857492e-09 Logit Scale: 100.000 Contrastive_loss: 0.21473 (0.18905) Loss: 0.21473 (0.18905)
2024-08-30,08:39:22 | INFO | Train Epoch: 73 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 536.341/s, 536.341/s/gpu LR: 1.849354e-09 Logit Scale: 100.000 Contrastive_loss: 0.16373 (0.18771) Loss: 0.16373 (0.18771)
2024-08-30,08:39:42 | INFO | Train Epoch: 73 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 531.451/s, 531.451/s/gpu LR: 1.841229e-09 Logit Scale: 100.000 Contrastive_loss: 0.19129 (0.18789) Loss: 0.19129 (0.18789)
2024-08-30,08:40:01 | INFO | Train Epoch: 73 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 530.087/s, 530.087/s/gpu LR: 1.833118e-09 Logit Scale: 100.000 Contrastive_loss: 0.23318 (0.19005) Loss: 0.23318 (0.19005)
2024-08-30,08:40:20 | INFO | Train Epoch: 73 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 496.425/s, 496.425/s/gpu LR: 1.825022e-09 Logit Scale: 100.000 Contrastive_loss: 0.15800 (0.18859) Loss: 0.15800 (0.18859)
2024-08-30,08:40:39 | INFO | Train Epoch: 73 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 513.420/s, 513.420/s/gpu LR: 1.816939e-09 Logit Scale: 100.000 Contrastive_loss: 0.24077 (0.19086) Loss: 0.24077 (0.19086)
2024-08-30,08:40:59 | INFO | Train Epoch: 73 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 527.292/s, 527.292/s/gpu LR: 1.808870e-09 Logit Scale: 100.000 Contrastive_loss: 0.29302 (0.19512) Loss: 0.29302 (0.19512)
2024-08-30,08:41:18 | INFO | Train Epoch: 73 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 524.662/s, 524.662/s/gpu LR: 1.800815e-09 Logit Scale: 100.000 Contrastive_loss: 0.28880 (0.19887) Loss: 0.28880 (0.19887)
2024-08-30,08:41:37 | INFO | Train Epoch: 73 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 492.155/s, 492.155/s/gpu LR: 1.792774e-09 Logit Scale: 100.000 Contrastive_loss: 0.15257 (0.19708) Loss: 0.15257 (0.19708)
2024-08-30,08:41:57 | INFO | Train Epoch: 73 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 502.536/s, 502.536/s/gpu LR: 1.784747e-09 Logit Scale: 100.000 Contrastive_loss: 0.20838 (0.19750) Loss: 0.20838 (0.19750)
2024-08-30,08:42:16 | INFO | Train Epoch: 73 [260100/145000.0 (179%)] Data (t): 0.107 Batch (t): 0.192, 536.934/s, 536.934/s/gpu LR: 1.776735e-09 Logit Scale: 100.000 Contrastive_loss: 0.20828 (0.19789) Loss: 0.20828 (0.19789)
2024-08-30,08:42:35 | INFO | Train Epoch: 73 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 531.475/s, 531.475/s/gpu LR: 1.768736e-09 Logit Scale: 100.000 Contrastive_loss: 0.24729 (0.19959) Loss: 0.24729 (0.19959)
2024-08-30,08:42:54 | INFO | Train Epoch: 73 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.194, 493.764/s, 493.764/s/gpu LR: 1.760752e-09 Logit Scale: 100.000 Contrastive_loss: 0.16224 (0.19835) Loss: 0.16224 (0.19835)
2024-08-30,08:43:14 | INFO | Train Epoch: 73 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 527.536/s, 527.536/s/gpu LR: 1.752782e-09 Logit Scale: 100.000 Contrastive_loss: 0.14069 (0.19649) Loss: 0.14069 (0.19649)
2024-08-30,08:43:33 | INFO | Train Epoch: 73 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 492.728/s, 492.728/s/gpu LR: 1.744826e-09 Logit Scale: 100.000 Contrastive_loss: 0.25098 (0.19819) Loss: 0.25098 (0.19819)
2024-08-30,08:43:52 | INFO | Train Epoch: 73 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 506.513/s, 506.513/s/gpu LR: 1.736884e-09 Logit Scale: 100.000 Contrastive_loss: 0.23795 (0.19939) Loss: 0.23795 (0.19939)
2024-08-30,08:44:11 | INFO | Train Epoch: 73 [320100/145000.0 (221%)] Data (t): 0.107 Batch (t): 0.191, 528.155/s, 528.155/s/gpu LR: 1.728957e-09 Logit Scale: 100.000 Contrastive_loss: 0.26866 (0.20143) Loss: 0.26866 (0.20143)
2024-08-30,08:44:31 | INFO | Train Epoch: 73 [330100/145000.0 (228%)] Data (t): 0.110 Batch (t): 0.193, 527.746/s, 527.746/s/gpu LR: 1.721044e-09 Logit Scale: 100.000 Contrastive_loss: 0.17964 (0.20081) Loss: 0.17964 (0.20081)
2024-08-30,08:44:50 | INFO | Train Epoch: 73 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.193, 530.304/s, 530.304/s/gpu LR: 1.713145e-09 Logit Scale: 100.000 Contrastive_loss: 0.23255 (0.20169) Loss: 0.23255 (0.20169)
2024-08-30,08:45:09 | INFO | Train Epoch: 73 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.193, 504.062/s, 504.062/s/gpu LR: 1.705261e-09 Logit Scale: 100.000 Contrastive_loss: 0.28813 (0.20403) Loss: 0.28813 (0.20403)
2024-08-30,08:45:29 | INFO | Train Epoch: 73 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 524.524/s, 524.524/s/gpu LR: 1.697391e-09 Logit Scale: 100.000 Contrastive_loss: 0.14629 (0.20251) Loss: 0.14629 (0.20251)
2024-08-30,08:45:48 | INFO | Train Epoch: 73 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.192, 501.249/s, 501.249/s/gpu LR: 1.689536e-09 Logit Scale: 100.000 Contrastive_loss: 0.24771 (0.20367) Loss: 0.24771 (0.20367)
2024-08-30,08:46:07 | INFO | Train Epoch: 73 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 509.667/s, 509.667/s/gpu LR: 1.681695e-09 Logit Scale: 100.000 Contrastive_loss: 0.22563 (0.20422) Loss: 0.22563 (0.20422)
2024-08-30,08:46:26 | INFO | Train Epoch: 73 [390100/145000.0 (269%)] Data (t): 0.109 Batch (t): 0.193, 539.903/s, 539.903/s/gpu LR: 1.673869e-09 Logit Scale: 100.000 Contrastive_loss: 0.12317 (0.20224) Loss: 0.12317 (0.20224)
2024-08-30,08:46:46 | INFO | Train Epoch: 73 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 543.385/s, 543.385/s/gpu LR: 1.666058e-09 Logit Scale: 100.000 Contrastive_loss: 0.21343 (0.20251) Loss: 0.21343 (0.20251)
2024-08-30,08:47:05 | INFO | Train Epoch: 73 [410100/145000.0 (283%)] Data (t): 0.110 Batch (t): 0.194, 494.326/s, 494.326/s/gpu LR: 1.658261e-09 Logit Scale: 100.000 Contrastive_loss: 0.083134 (0.19973) Loss: 0.083134 (0.19973)
2024-08-30,08:47:13 | INFO | Eval Epoch: 74 [200 / 1000]	Clip Loss: 0.483793	
2024-08-30,08:47:13 | INFO | Eval Epoch: 74 image_to_text_mean_rank: 2.4020	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6740	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9350	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4404	epoch: 74.0000	num_samples: 1000.0000
2024-08-30,08:47:15 | INFO | Start epoch 74
2024-08-30,08:47:15 | INFO | Train Epoch: 74 [   100/145000.0 (0%)] Data (t): 0.025 Batch (t): 0.114, 876.391/s, 876.391/s/gpu LR: 1.869725e-09 Logit Scale: 100.000 Contrastive_loss: 0.16077 (0.16077) Loss: 0.16077 (0.16077)
2024-08-30,08:47:34 | INFO | Train Epoch: 74 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 507.253/s, 507.253/s/gpu LR: 1.861566e-09 Logit Scale: 100.000 Contrastive_loss: 0.32593 (0.24335) Loss: 0.32593 (0.24335)
2024-08-30,08:47:54 | INFO | Train Epoch: 74 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 536.700/s, 536.700/s/gpu LR: 1.853421e-09 Logit Scale: 100.000 Contrastive_loss: 0.33139 (0.27270) Loss: 0.33139 (0.27270)
2024-08-30,08:48:13 | INFO | Train Epoch: 74 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 509.184/s, 509.184/s/gpu LR: 1.845290e-09 Logit Scale: 100.000 Contrastive_loss: 0.16992 (0.24700) Loss: 0.16992 (0.24700)
2024-08-30,08:48:32 | INFO | Train Epoch: 74 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 534.752/s, 534.752/s/gpu LR: 1.837172e-09 Logit Scale: 100.000 Contrastive_loss: 0.12807 (0.22322) Loss: 0.12807 (0.22322)
2024-08-30,08:48:51 | INFO | Train Epoch: 74 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 492.459/s, 492.459/s/gpu LR: 1.829068e-09 Logit Scale: 100.000 Contrastive_loss: 0.14927 (0.21089) Loss: 0.14927 (0.21089)
2024-08-30,08:49:11 | INFO | Train Epoch: 74 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 497.081/s, 497.081/s/gpu LR: 1.820979e-09 Logit Scale: 100.000 Contrastive_loss: 0.11091 (0.19661) Loss: 0.11091 (0.19661)
2024-08-30,08:49:30 | INFO | Train Epoch: 74 [ 70100/145000.0 (48%)] Data (t): 0.110 Batch (t): 0.193, 494.497/s, 494.497/s/gpu LR: 1.812903e-09 Logit Scale: 100.000 Contrastive_loss: 0.20353 (0.19747) Loss: 0.20353 (0.19747)
2024-08-30,08:49:49 | INFO | Train Epoch: 74 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 537.949/s, 537.949/s/gpu LR: 1.804841e-09 Logit Scale: 100.000 Contrastive_loss: 0.12194 (0.18908) Loss: 0.12194 (0.18908)
2024-08-30,08:50:09 | INFO | Train Epoch: 74 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.192, 502.565/s, 502.565/s/gpu LR: 1.796793e-09 Logit Scale: 100.000 Contrastive_loss: 0.20246 (0.19042) Loss: 0.20246 (0.19042)
2024-08-30,08:50:28 | INFO | Train Epoch: 74 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 531.728/s, 531.728/s/gpu LR: 1.788759e-09 Logit Scale: 100.000 Contrastive_loss: 0.14505 (0.18629) Loss: 0.14505 (0.18629)
2024-08-30,08:50:47 | INFO | Train Epoch: 74 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.192, 498.983/s, 498.983/s/gpu LR: 1.780739e-09 Logit Scale: 100.000 Contrastive_loss: 0.16203 (0.18427) Loss: 0.16203 (0.18427)
2024-08-30,08:51:06 | INFO | Train Epoch: 74 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 499.015/s, 499.015/s/gpu LR: 1.772734e-09 Logit Scale: 100.000 Contrastive_loss: 0.26279 (0.19031) Loss: 0.26279 (0.19031)
2024-08-30,08:51:26 | INFO | Train Epoch: 74 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 496.552/s, 496.552/s/gpu LR: 1.764742e-09 Logit Scale: 100.000 Contrastive_loss: 0.14600 (0.18715) Loss: 0.14600 (0.18715)
2024-08-30,08:51:45 | INFO | Train Epoch: 74 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 506.722/s, 506.722/s/gpu LR: 1.756765e-09 Logit Scale: 100.000 Contrastive_loss: 0.17233 (0.18616) Loss: 0.17233 (0.18616)
2024-08-30,08:51:55 | INFO | Train Epoch: 74 [145000/145000.0 (100%)] Data (t): 0.107 Batch (t): 0.193, 526.963/s, 526.963/s/gpu LR: 1.752861e-09 Logit Scale: 100.000 Contrastive_loss: 0.20507 (0.18734) Loss: 0.20507 (0.18734)
2024-08-30,08:52:04 | INFO | Train Epoch: 74 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.192, 498.460/s, 498.460/s/gpu LR: 1.748802e-09 Logit Scale: 100.000 Contrastive_loss: 0.18655 (0.18729) Loss: 0.18655 (0.18729)
2024-08-30,08:52:24 | INFO | Train Epoch: 74 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 495.288/s, 495.288/s/gpu LR: 1.740853e-09 Logit Scale: 100.000 Contrastive_loss: 0.21429 (0.18879) Loss: 0.21429 (0.18879)
2024-08-30,08:52:43 | INFO | Train Epoch: 74 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 504.989/s, 504.989/s/gpu LR: 1.732919e-09 Logit Scale: 100.000 Contrastive_loss: 0.16346 (0.18746) Loss: 0.16346 (0.18746)
2024-08-30,08:53:02 | INFO | Train Epoch: 74 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 532.043/s, 532.043/s/gpu LR: 1.724998e-09 Logit Scale: 100.000 Contrastive_loss: 0.19110 (0.18764) Loss: 0.19110 (0.18764)
2024-08-30,08:53:21 | INFO | Train Epoch: 74 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 498.763/s, 498.763/s/gpu LR: 1.717093e-09 Logit Scale: 100.000 Contrastive_loss: 0.23292 (0.18980) Loss: 0.23292 (0.18980)
2024-08-30,08:53:41 | INFO | Train Epoch: 74 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.192, 494.362/s, 494.362/s/gpu LR: 1.709201e-09 Logit Scale: 100.000 Contrastive_loss: 0.15790 (0.18835) Loss: 0.15790 (0.18835)
2024-08-30,08:54:00 | INFO | Train Epoch: 74 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.194, 501.443/s, 501.443/s/gpu LR: 1.701324e-09 Logit Scale: 100.000 Contrastive_loss: 0.24053 (0.19062) Loss: 0.24053 (0.19062)
2024-08-30,08:54:19 | INFO | Train Epoch: 74 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 530.255/s, 530.255/s/gpu LR: 1.693462e-09 Logit Scale: 100.000 Contrastive_loss: 0.29277 (0.19487) Loss: 0.29277 (0.19487)
2024-08-30,08:54:39 | INFO | Train Epoch: 74 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 496.448/s, 496.448/s/gpu LR: 1.685614e-09 Logit Scale: 100.000 Contrastive_loss: 0.28845 (0.19862) Loss: 0.28845 (0.19862)
2024-08-30,08:54:58 | INFO | Train Epoch: 74 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 525.887/s, 525.887/s/gpu LR: 1.677780e-09 Logit Scale: 100.000 Contrastive_loss: 0.15241 (0.19684) Loss: 0.15241 (0.19684)
2024-08-30,08:55:17 | INFO | Train Epoch: 74 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 521.652/s, 521.652/s/gpu LR: 1.669962e-09 Logit Scale: 100.000 Contrastive_loss: 0.20818 (0.19726) Loss: 0.20818 (0.19726)
2024-08-30,08:55:37 | INFO | Train Epoch: 74 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 529.237/s, 529.237/s/gpu LR: 1.662157e-09 Logit Scale: 100.000 Contrastive_loss: 0.20815 (0.19765) Loss: 0.20815 (0.19765)
2024-08-30,08:55:56 | INFO | Train Epoch: 74 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 524.805/s, 524.805/s/gpu LR: 1.654368e-09 Logit Scale: 100.000 Contrastive_loss: 0.24699 (0.19935) Loss: 0.24699 (0.19935)
2024-08-30,08:56:15 | INFO | Train Epoch: 74 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 493.017/s, 493.017/s/gpu LR: 1.646593e-09 Logit Scale: 100.000 Contrastive_loss: 0.16213 (0.19811) Loss: 0.16213 (0.19811)
2024-08-30,08:56:34 | INFO | Train Epoch: 74 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.193, 534.829/s, 534.829/s/gpu LR: 1.638833e-09 Logit Scale: 100.000 Contrastive_loss: 0.14059 (0.19625) Loss: 0.14059 (0.19625)
2024-08-30,08:56:54 | INFO | Train Epoch: 74 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 536.061/s, 536.061/s/gpu LR: 1.631087e-09 Logit Scale: 100.000 Contrastive_loss: 0.25079 (0.19796) Loss: 0.25079 (0.19796)
2024-08-30,08:57:13 | INFO | Train Epoch: 74 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.194, 497.740/s, 497.740/s/gpu LR: 1.623356e-09 Logit Scale: 100.000 Contrastive_loss: 0.23772 (0.19916) Loss: 0.23772 (0.19916)
2024-08-30,08:57:32 | INFO | Train Epoch: 74 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.192, 497.708/s, 497.708/s/gpu LR: 1.615640e-09 Logit Scale: 100.000 Contrastive_loss: 0.26833 (0.20120) Loss: 0.26833 (0.20120)
2024-08-30,08:57:52 | INFO | Train Epoch: 74 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.193, 519.213/s, 519.213/s/gpu LR: 1.607939e-09 Logit Scale: 100.000 Contrastive_loss: 0.17945 (0.20058) Loss: 0.17945 (0.20058)
2024-08-30,08:58:11 | INFO | Train Epoch: 74 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 494.576/s, 494.576/s/gpu LR: 1.600253e-09 Logit Scale: 100.000 Contrastive_loss: 0.23223 (0.20146) Loss: 0.23223 (0.20146)
2024-08-30,08:58:30 | INFO | Train Epoch: 74 [350100/145000.0 (241%)] Data (t): 0.107 Batch (t): 0.192, 500.088/s, 500.088/s/gpu LR: 1.592582e-09 Logit Scale: 100.000 Contrastive_loss: 0.28800 (0.20379) Loss: 0.28800 (0.20379)
2024-08-30,08:58:49 | INFO | Train Epoch: 74 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 521.254/s, 521.254/s/gpu LR: 1.584926e-09 Logit Scale: 100.000 Contrastive_loss: 0.14625 (0.20228) Loss: 0.14625 (0.20228)
2024-08-30,08:59:09 | INFO | Train Epoch: 74 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.192, 529.982/s, 529.982/s/gpu LR: 1.577284e-09 Logit Scale: 100.000 Contrastive_loss: 0.24749 (0.20344) Loss: 0.24749 (0.20344)
2024-08-30,08:59:28 | INFO | Train Epoch: 74 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 502.415/s, 502.415/s/gpu LR: 1.569658e-09 Logit Scale: 100.000 Contrastive_loss: 0.22536 (0.20399) Loss: 0.22536 (0.20399)
2024-08-30,08:59:47 | INFO | Train Epoch: 74 [390100/145000.0 (269%)] Data (t): 0.107 Batch (t): 0.192, 529.097/s, 529.097/s/gpu LR: 1.562047e-09 Logit Scale: 100.000 Contrastive_loss: 0.12302 (0.20201) Loss: 0.12302 (0.20201)
2024-08-30,09:00:06 | INFO | Train Epoch: 74 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.193, 496.844/s, 496.844/s/gpu LR: 1.554451e-09 Logit Scale: 100.000 Contrastive_loss: 0.21305 (0.20228) Loss: 0.21305 (0.20228)
2024-08-30,09:00:26 | INFO | Train Epoch: 74 [410100/145000.0 (283%)] Data (t): 0.107 Batch (t): 0.192, 528.036/s, 528.036/s/gpu LR: 1.546869e-09 Logit Scale: 100.000 Contrastive_loss: 0.082948 (0.19950) Loss: 0.082948 (0.19950)
2024-08-30,09:00:33 | INFO | Eval Epoch: 75 [200 / 1000]	Clip Loss: 0.483837	
2024-08-30,09:00:34 | INFO | Eval Epoch: 75 image_to_text_mean_rank: 2.4030	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6740	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9360	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6720	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4405	epoch: 75.0000	num_samples: 1000.0000
2024-08-30,09:00:35 | INFO | Start epoch 75
2024-08-30,09:00:35 | INFO | Train Epoch: 75 [   100/145000.0 (0%)] Data (t): 0.025 Batch (t): 0.113, 881.896/s, 881.896/s/gpu LR: 1.752782e-09 Logit Scale: 100.000 Contrastive_loss: 0.16053 (0.16053) Loss: 0.16053 (0.16053)
2024-08-30,09:00:55 | INFO | Train Epoch: 75 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 494.856/s, 494.856/s/gpu LR: 1.744826e-09 Logit Scale: 100.000 Contrastive_loss: 0.32553 (0.24303) Loss: 0.32553 (0.24303)
2024-08-30,09:01:14 | INFO | Train Epoch: 75 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 524.108/s, 524.108/s/gpu LR: 1.736884e-09 Logit Scale: 100.000 Contrastive_loss: 0.33121 (0.27242) Loss: 0.33121 (0.27242)
2024-08-30,09:01:33 | INFO | Train Epoch: 75 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 500.425/s, 500.425/s/gpu LR: 1.728957e-09 Logit Scale: 100.000 Contrastive_loss: 0.16963 (0.24673) Loss: 0.16963 (0.24673)
2024-08-30,09:01:53 | INFO | Train Epoch: 75 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 536.933/s, 536.933/s/gpu LR: 1.721044e-09 Logit Scale: 100.000 Contrastive_loss: 0.12792 (0.22296) Loss: 0.12792 (0.22296)
2024-08-30,09:02:12 | INFO | Train Epoch: 75 [ 50100/145000.0 (35%)] Data (t): 0.108 Batch (t): 0.193, 491.162/s, 491.162/s/gpu LR: 1.713145e-09 Logit Scale: 100.000 Contrastive_loss: 0.14896 (0.21063) Loss: 0.14896 (0.21063)
2024-08-30,09:02:31 | INFO | Train Epoch: 75 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 506.425/s, 506.425/s/gpu LR: 1.705261e-09 Logit Scale: 100.000 Contrastive_loss: 0.11081 (0.19637) Loss: 0.11081 (0.19637)
2024-08-30,09:02:50 | INFO | Train Epoch: 75 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.193, 502.621/s, 502.621/s/gpu LR: 1.697391e-09 Logit Scale: 100.000 Contrastive_loss: 0.20323 (0.19723) Loss: 0.20323 (0.19723)
2024-08-30,09:03:10 | INFO | Train Epoch: 75 [ 80100/145000.0 (55%)] Data (t): 0.107 Batch (t): 0.192, 496.379/s, 496.379/s/gpu LR: 1.689536e-09 Logit Scale: 100.000 Contrastive_loss: 0.12176 (0.18884) Loss: 0.12176 (0.18884)
2024-08-30,09:03:29 | INFO | Train Epoch: 75 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 493.145/s, 493.145/s/gpu LR: 1.681695e-09 Logit Scale: 100.000 Contrastive_loss: 0.20217 (0.19018) Loss: 0.20217 (0.19018)
2024-08-30,09:03:48 | INFO | Train Epoch: 75 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 501.374/s, 501.374/s/gpu LR: 1.673869e-09 Logit Scale: 100.000 Contrastive_loss: 0.14495 (0.18606) Loss: 0.14495 (0.18606)
2024-08-30,09:04:08 | INFO | Train Epoch: 75 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.192, 525.177/s, 525.177/s/gpu LR: 1.666058e-09 Logit Scale: 100.000 Contrastive_loss: 0.16195 (0.18405) Loss: 0.16195 (0.18405)
2024-08-30,09:04:27 | INFO | Train Epoch: 75 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 500.665/s, 500.665/s/gpu LR: 1.658261e-09 Logit Scale: 100.000 Contrastive_loss: 0.26255 (0.19009) Loss: 0.26255 (0.19009)
2024-08-30,09:04:46 | INFO | Train Epoch: 75 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 528.703/s, 528.703/s/gpu LR: 1.650478e-09 Logit Scale: 100.000 Contrastive_loss: 0.14587 (0.18693) Loss: 0.14587 (0.18693)
2024-08-30,09:05:05 | INFO | Train Epoch: 75 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.192, 530.081/s, 530.081/s/gpu LR: 1.642711e-09 Logit Scale: 100.000 Contrastive_loss: 0.17219 (0.18595) Loss: 0.17219 (0.18595)
2024-08-30,09:05:15 | INFO | Train Epoch: 75 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 525.889/s, 525.889/s/gpu LR: 1.638910e-09 Logit Scale: 100.000 Contrastive_loss: 0.20484 (0.18713) Loss: 0.20484 (0.18713)
2024-08-30,09:05:25 | INFO | Train Epoch: 75 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.192, 507.741/s, 507.741/s/gpu LR: 1.634958e-09 Logit Scale: 100.000 Contrastive_loss: 0.18633 (0.18708) Loss: 0.18633 (0.18708)
2024-08-30,09:05:44 | INFO | Train Epoch: 75 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.192, 525.545/s, 525.545/s/gpu LR: 1.627220e-09 Logit Scale: 100.000 Contrastive_loss: 0.21401 (0.18858) Loss: 0.21401 (0.18858)
2024-08-30,09:06:03 | INFO | Train Epoch: 75 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 528.665/s, 528.665/s/gpu LR: 1.619497e-09 Logit Scale: 100.000 Contrastive_loss: 0.16325 (0.18725) Loss: 0.16325 (0.18725)
2024-08-30,09:06:22 | INFO | Train Epoch: 75 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 562.796/s, 562.796/s/gpu LR: 1.611788e-09 Logit Scale: 100.000 Contrastive_loss: 0.19098 (0.18743) Loss: 0.19098 (0.18743)
2024-08-30,09:06:42 | INFO | Train Epoch: 75 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 525.864/s, 525.864/s/gpu LR: 1.604094e-09 Logit Scale: 100.000 Contrastive_loss: 0.23274 (0.18959) Loss: 0.23274 (0.18959)
2024-08-30,09:07:01 | INFO | Train Epoch: 75 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 495.004/s, 495.004/s/gpu LR: 1.596416e-09 Logit Scale: 100.000 Contrastive_loss: 0.15765 (0.18814) Loss: 0.15765 (0.18814)
2024-08-30,09:07:20 | INFO | Train Epoch: 75 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 536.954/s, 536.954/s/gpu LR: 1.588752e-09 Logit Scale: 100.000 Contrastive_loss: 0.24042 (0.19041) Loss: 0.24042 (0.19041)
2024-08-30,09:07:39 | INFO | Train Epoch: 75 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.192, 523.720/s, 523.720/s/gpu LR: 1.581103e-09 Logit Scale: 100.000 Contrastive_loss: 0.29239 (0.19466) Loss: 0.29239 (0.19466)
2024-08-30,09:07:59 | INFO | Train Epoch: 75 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 527.382/s, 527.382/s/gpu LR: 1.573469e-09 Logit Scale: 100.000 Contrastive_loss: 0.28842 (0.19841) Loss: 0.28842 (0.19841)
2024-08-30,09:08:18 | INFO | Train Epoch: 75 [240100/145000.0 (166%)] Data (t): 0.109 Batch (t): 0.193, 531.742/s, 531.742/s/gpu LR: 1.565850e-09 Logit Scale: 100.000 Contrastive_loss: 0.15212 (0.19663) Loss: 0.15212 (0.19663)
2024-08-30,09:08:37 | INFO | Train Epoch: 75 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 498.691/s, 498.691/s/gpu LR: 1.558247e-09 Logit Scale: 100.000 Contrastive_loss: 0.20813 (0.19706) Loss: 0.20813 (0.19706)
2024-08-30,09:08:56 | INFO | Train Epoch: 75 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.193, 501.131/s, 501.131/s/gpu LR: 1.550658e-09 Logit Scale: 100.000 Contrastive_loss: 0.20791 (0.19744) Loss: 0.20791 (0.19744)
2024-08-30,09:09:16 | INFO | Train Epoch: 75 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 536.957/s, 536.957/s/gpu LR: 1.543085e-09 Logit Scale: 100.000 Contrastive_loss: 0.24678 (0.19915) Loss: 0.24678 (0.19915)
2024-08-30,09:09:35 | INFO | Train Epoch: 75 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 525.002/s, 525.002/s/gpu LR: 1.535526e-09 Logit Scale: 100.000 Contrastive_loss: 0.16197 (0.19791) Loss: 0.16197 (0.19791)
2024-08-30,09:09:54 | INFO | Train Epoch: 75 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 523.592/s, 523.592/s/gpu LR: 1.527983e-09 Logit Scale: 100.000 Contrastive_loss: 0.14052 (0.19606) Loss: 0.14052 (0.19606)
2024-08-30,09:10:14 | INFO | Train Epoch: 75 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 500.783/s, 500.783/s/gpu LR: 1.520455e-09 Logit Scale: 100.000 Contrastive_loss: 0.25051 (0.19776) Loss: 0.25051 (0.19776)
2024-08-30,09:10:33 | INFO | Train Epoch: 75 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 525.963/s, 525.963/s/gpu LR: 1.512943e-09 Logit Scale: 100.000 Contrastive_loss: 0.23765 (0.19897) Loss: 0.23765 (0.19897)
2024-08-30,09:10:52 | INFO | Train Epoch: 75 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 531.378/s, 531.378/s/gpu LR: 1.505445e-09 Logit Scale: 100.000 Contrastive_loss: 0.26813 (0.20100) Loss: 0.26813 (0.20100)
2024-08-30,09:11:12 | INFO | Train Epoch: 75 [330100/145000.0 (228%)] Data (t): 0.109 Batch (t): 0.193, 502.199/s, 502.199/s/gpu LR: 1.497963e-09 Logit Scale: 100.000 Contrastive_loss: 0.17936 (0.20038) Loss: 0.17936 (0.20038)
2024-08-30,09:11:31 | INFO | Train Epoch: 75 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.192, 489.847/s, 489.847/s/gpu LR: 1.490496e-09 Logit Scale: 100.000 Contrastive_loss: 0.23196 (0.20126) Loss: 0.23196 (0.20126)
2024-08-30,09:11:50 | INFO | Train Epoch: 75 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 527.661/s, 527.661/s/gpu LR: 1.483045e-09 Logit Scale: 100.000 Contrastive_loss: 0.28762 (0.20359) Loss: 0.28762 (0.20359)
2024-08-30,09:12:09 | INFO | Train Epoch: 75 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 535.182/s, 535.182/s/gpu LR: 1.475609e-09 Logit Scale: 100.000 Contrastive_loss: 0.14599 (0.20208) Loss: 0.14599 (0.20208)
2024-08-30,09:12:29 | INFO | Train Epoch: 75 [370100/145000.0 (255%)] Data (t): 0.109 Batch (t): 0.194, 489.824/s, 489.824/s/gpu LR: 1.468189e-09 Logit Scale: 100.000 Contrastive_loss: 0.24729 (0.20324) Loss: 0.24729 (0.20324)
2024-08-30,09:12:48 | INFO | Train Epoch: 75 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 523.987/s, 523.987/s/gpu LR: 1.460784e-09 Logit Scale: 100.000 Contrastive_loss: 0.22517 (0.20378) Loss: 0.22517 (0.20378)
2024-08-30,09:13:07 | INFO | Train Epoch: 75 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 526.034/s, 526.034/s/gpu LR: 1.453395e-09 Logit Scale: 100.000 Contrastive_loss: 0.12282 (0.20181) Loss: 0.12282 (0.20181)
2024-08-30,09:13:27 | INFO | Train Epoch: 75 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.193, 525.592/s, 525.592/s/gpu LR: 1.446021e-09 Logit Scale: 100.000 Contrastive_loss: 0.21299 (0.20208) Loss: 0.21299 (0.20208)
2024-08-30,09:13:46 | INFO | Train Epoch: 75 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 518.632/s, 518.632/s/gpu LR: 1.438663e-09 Logit Scale: 100.000 Contrastive_loss: 0.082851 (0.19930) Loss: 0.082851 (0.19930)
2024-08-30,09:13:53 | INFO | Eval Epoch: 76 [200 / 1000]	Clip Loss: 0.483880	
2024-08-30,09:13:54 | INFO | Eval Epoch: 76 image_to_text_mean_rank: 2.4030	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6740	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9630	text_to_image_mean_rank: 3.9390	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4406	epoch: 76.0000	num_samples: 1000.0000
2024-08-30,09:13:55 | INFO | Start epoch 76
2024-08-30,09:13:56 | INFO | Train Epoch: 76 [   100/145000.0 (0%)] Data (t): 0.020 Batch (t): 0.110, 909.092/s, 909.092/s/gpu LR: 1.638833e-09 Logit Scale: 100.000 Contrastive_loss: 0.16044 (0.16044) Loss: 0.16044 (0.16044)
2024-08-30,09:14:15 | INFO | Train Epoch: 76 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 530.233/s, 530.233/s/gpu LR: 1.631087e-09 Logit Scale: 100.000 Contrastive_loss: 0.32539 (0.24292) Loss: 0.32539 (0.24292)
2024-08-30,09:14:34 | INFO | Train Epoch: 76 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 534.216/s, 534.216/s/gpu LR: 1.623356e-09 Logit Scale: 100.000 Contrastive_loss: 0.33069 (0.27217) Loss: 0.33069 (0.27217)
2024-08-30,09:14:54 | INFO | Train Epoch: 76 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 533.770/s, 533.770/s/gpu LR: 1.615640e-09 Logit Scale: 100.000 Contrastive_loss: 0.16932 (0.24646) Loss: 0.16932 (0.24646)
2024-08-30,09:15:13 | INFO | Train Epoch: 76 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 496.032/s, 496.032/s/gpu LR: 1.607939e-09 Logit Scale: 100.000 Contrastive_loss: 0.12780 (0.22273) Loss: 0.12780 (0.22273)
2024-08-30,09:15:32 | INFO | Train Epoch: 76 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 523.000/s, 523.000/s/gpu LR: 1.600253e-09 Logit Scale: 100.000 Contrastive_loss: 0.14875 (0.21040) Loss: 0.14875 (0.21040)
2024-08-30,09:15:51 | INFO | Train Epoch: 76 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.193, 497.928/s, 497.928/s/gpu LR: 1.592582e-09 Logit Scale: 100.000 Contrastive_loss: 0.11069 (0.19615) Loss: 0.11069 (0.19615)
2024-08-30,09:16:11 | INFO | Train Epoch: 76 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 494.501/s, 494.501/s/gpu LR: 1.584926e-09 Logit Scale: 100.000 Contrastive_loss: 0.20296 (0.19700) Loss: 0.20296 (0.19700)
2024-08-30,09:16:30 | INFO | Train Epoch: 76 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.192, 491.508/s, 491.508/s/gpu LR: 1.577284e-09 Logit Scale: 100.000 Contrastive_loss: 0.12156 (0.18862) Loss: 0.12156 (0.18862)
2024-08-30,09:16:49 | INFO | Train Epoch: 76 [ 90100/145000.0 (62%)] Data (t): 0.109 Batch (t): 0.193, 529.708/s, 529.708/s/gpu LR: 1.569658e-09 Logit Scale: 100.000 Contrastive_loss: 0.20208 (0.18997) Loss: 0.20208 (0.18997)
2024-08-30,09:17:08 | INFO | Train Epoch: 76 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 493.131/s, 493.131/s/gpu LR: 1.562047e-09 Logit Scale: 100.000 Contrastive_loss: 0.14477 (0.18586) Loss: 0.14477 (0.18586)
2024-08-30,09:17:28 | INFO | Train Epoch: 76 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 494.304/s, 494.304/s/gpu LR: 1.554451e-09 Logit Scale: 100.000 Contrastive_loss: 0.16164 (0.18384) Loss: 0.16164 (0.18384)
2024-08-30,09:17:47 | INFO | Train Epoch: 76 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 495.397/s, 495.397/s/gpu LR: 1.546869e-09 Logit Scale: 100.000 Contrastive_loss: 0.26239 (0.18988) Loss: 0.26239 (0.18988)
2024-08-30,09:18:06 | INFO | Train Epoch: 76 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 529.049/s, 529.049/s/gpu LR: 1.539304e-09 Logit Scale: 100.000 Contrastive_loss: 0.14577 (0.18673) Loss: 0.14577 (0.18673)
2024-08-30,09:18:26 | INFO | Train Epoch: 76 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.192, 540.957/s, 540.957/s/gpu LR: 1.531753e-09 Logit Scale: 100.000 Contrastive_loss: 0.17197 (0.18575) Loss: 0.17197 (0.18575)
2024-08-30,09:18:35 | INFO | Train Epoch: 76 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 534.197/s, 534.197/s/gpu LR: 1.528058e-09 Logit Scale: 100.000 Contrastive_loss: 0.20447 (0.18692) Loss: 0.20447 (0.18692)
2024-08-30,09:18:45 | INFO | Train Epoch: 76 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 501.187/s, 501.187/s/gpu LR: 1.524217e-09 Logit Scale: 100.000 Contrastive_loss: 0.18610 (0.18687) Loss: 0.18610 (0.18687)
2024-08-30,09:19:04 | INFO | Train Epoch: 76 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 523.526/s, 523.526/s/gpu LR: 1.516697e-09 Logit Scale: 100.000 Contrastive_loss: 0.21374 (0.18836) Loss: 0.21374 (0.18836)
2024-08-30,09:19:23 | INFO | Train Epoch: 76 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.192, 560.859/s, 560.859/s/gpu LR: 1.509192e-09 Logit Scale: 100.000 Contrastive_loss: 0.16306 (0.18703) Loss: 0.16306 (0.18703)
2024-08-30,09:19:43 | INFO | Train Epoch: 76 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 493.908/s, 493.908/s/gpu LR: 1.501702e-09 Logit Scale: 100.000 Contrastive_loss: 0.19084 (0.18722) Loss: 0.19084 (0.18722)
2024-08-30,09:20:02 | INFO | Train Epoch: 76 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 535.695/s, 535.695/s/gpu LR: 1.494228e-09 Logit Scale: 100.000 Contrastive_loss: 0.23255 (0.18938) Loss: 0.23255 (0.18938)
2024-08-30,09:20:21 | INFO | Train Epoch: 76 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 496.596/s, 496.596/s/gpu LR: 1.486769e-09 Logit Scale: 100.000 Contrastive_loss: 0.15761 (0.18794) Loss: 0.15761 (0.18794)
2024-08-30,09:20:40 | INFO | Train Epoch: 76 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 524.521/s, 524.521/s/gpu LR: 1.479325e-09 Logit Scale: 100.000 Contrastive_loss: 0.24028 (0.19021) Loss: 0.24028 (0.19021)
2024-08-30,09:21:00 | INFO | Train Epoch: 76 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 504.647/s, 504.647/s/gpu LR: 1.471897e-09 Logit Scale: 100.000 Contrastive_loss: 0.29219 (0.19446) Loss: 0.29219 (0.19446)
2024-08-30,09:21:19 | INFO | Train Epoch: 76 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 531.915/s, 531.915/s/gpu LR: 1.464485e-09 Logit Scale: 100.000 Contrastive_loss: 0.28810 (0.19821) Loss: 0.28810 (0.19821)
2024-08-30,09:21:38 | INFO | Train Epoch: 76 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 508.378/s, 508.378/s/gpu LR: 1.457087e-09 Logit Scale: 100.000 Contrastive_loss: 0.15190 (0.19643) Loss: 0.15190 (0.19643)
2024-08-30,09:21:58 | INFO | Train Epoch: 76 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 528.259/s, 528.259/s/gpu LR: 1.449706e-09 Logit Scale: 100.000 Contrastive_loss: 0.20791 (0.19685) Loss: 0.20791 (0.19685)
2024-08-30,09:22:17 | INFO | Train Epoch: 76 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 527.538/s, 527.538/s/gpu LR: 1.442340e-09 Logit Scale: 100.000 Contrastive_loss: 0.20779 (0.19724) Loss: 0.20779 (0.19724)
2024-08-30,09:22:36 | INFO | Train Epoch: 76 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.193, 527.507/s, 527.507/s/gpu LR: 1.434989e-09 Logit Scale: 100.000 Contrastive_loss: 0.24666 (0.19895) Loss: 0.24666 (0.19895)
2024-08-30,09:22:56 | INFO | Train Epoch: 76 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 496.528/s, 496.528/s/gpu LR: 1.427655e-09 Logit Scale: 100.000 Contrastive_loss: 0.16184 (0.19771) Loss: 0.16184 (0.19771)
2024-08-30,09:23:15 | INFO | Train Epoch: 76 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 531.976/s, 531.976/s/gpu LR: 1.420336e-09 Logit Scale: 100.000 Contrastive_loss: 0.14043 (0.19586) Loss: 0.14043 (0.19586)
2024-08-30,09:23:34 | INFO | Train Epoch: 76 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 496.689/s, 496.689/s/gpu LR: 1.413032e-09 Logit Scale: 100.000 Contrastive_loss: 0.25031 (0.19756) Loss: 0.25031 (0.19756)
2024-08-30,09:23:54 | INFO | Train Epoch: 76 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.194, 529.855/s, 529.855/s/gpu LR: 1.405745e-09 Logit Scale: 100.000 Contrastive_loss: 0.23763 (0.19878) Loss: 0.23763 (0.19878)
2024-08-30,09:24:13 | INFO | Train Epoch: 76 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 538.381/s, 538.381/s/gpu LR: 1.398473e-09 Logit Scale: 100.000 Contrastive_loss: 0.26802 (0.20081) Loss: 0.26802 (0.20081)
2024-08-30,09:24:32 | INFO | Train Epoch: 76 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 531.572/s, 531.572/s/gpu LR: 1.391216e-09 Logit Scale: 100.000 Contrastive_loss: 0.17915 (0.20019) Loss: 0.17915 (0.20019)
2024-08-30,09:24:52 | INFO | Train Epoch: 76 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 504.936/s, 504.936/s/gpu LR: 1.383976e-09 Logit Scale: 100.000 Contrastive_loss: 0.23158 (0.20107) Loss: 0.23158 (0.20107)
2024-08-30,09:25:11 | INFO | Train Epoch: 76 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 535.359/s, 535.359/s/gpu LR: 1.376752e-09 Logit Scale: 100.000 Contrastive_loss: 0.28752 (0.20340) Loss: 0.28752 (0.20340)
2024-08-30,09:25:30 | INFO | Train Epoch: 76 [360100/145000.0 (248%)] Data (t): 0.107 Batch (t): 0.192, 530.343/s, 530.343/s/gpu LR: 1.369543e-09 Logit Scale: 100.000 Contrastive_loss: 0.14589 (0.20189) Loss: 0.14589 (0.20189)
2024-08-30,09:25:49 | INFO | Train Epoch: 76 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 536.372/s, 536.372/s/gpu LR: 1.362351e-09 Logit Scale: 100.000 Contrastive_loss: 0.24707 (0.20305) Loss: 0.24707 (0.20305)
2024-08-30,09:26:09 | INFO | Train Epoch: 76 [380100/145000.0 (262%)] Data (t): 0.109 Batch (t): 0.193, 527.067/s, 527.067/s/gpu LR: 1.355174e-09 Logit Scale: 100.000 Contrastive_loss: 0.22501 (0.20360) Loss: 0.22501 (0.20360)
2024-08-30,09:26:28 | INFO | Train Epoch: 76 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.192, 536.139/s, 536.139/s/gpu LR: 1.348013e-09 Logit Scale: 100.000 Contrastive_loss: 0.12273 (0.20162) Loss: 0.12273 (0.20162)
2024-08-30,09:26:47 | INFO | Train Epoch: 76 [400100/145000.0 (276%)] Data (t): 0.107 Batch (t): 0.192, 526.740/s, 526.740/s/gpu LR: 1.340869e-09 Logit Scale: 100.000 Contrastive_loss: 0.21285 (0.20189) Loss: 0.21285 (0.20189)
2024-08-30,09:27:06 | INFO | Train Epoch: 76 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 501.930/s, 501.930/s/gpu LR: 1.333740e-09 Logit Scale: 100.000 Contrastive_loss: 0.082763 (0.19912) Loss: 0.082763 (0.19912)
2024-08-30,09:27:14 | INFO | Eval Epoch: 77 [200 / 1000]	Clip Loss: 0.483954	
2024-08-30,09:27:15 | INFO | Eval Epoch: 77 image_to_text_mean_rank: 2.4010	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6740	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9640	text_to_image_mean_rank: 3.9380	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6710	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4405	epoch: 77.0000	num_samples: 1000.0000
2024-08-30,09:27:16 | INFO | Start epoch 77
2024-08-30,09:27:16 | INFO | Train Epoch: 77 [   100/145000.0 (0%)] Data (t): 0.034 Batch (t): 0.114, 879.055/s, 879.055/s/gpu LR: 1.527983e-09 Logit Scale: 100.000 Contrastive_loss: 0.16031 (0.16031) Loss: 0.16031 (0.16031)
2024-08-30,09:27:36 | INFO | Train Epoch: 77 [ 10100/145000.0 (7%)] Data (t): 0.111 Batch (t): 0.195, 500.669/s, 500.669/s/gpu LR: 1.520455e-09 Logit Scale: 100.000 Contrastive_loss: 0.32508 (0.24270) Loss: 0.32508 (0.24270)
2024-08-30,09:27:55 | INFO | Train Epoch: 77 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 495.408/s, 495.408/s/gpu LR: 1.512943e-09 Logit Scale: 100.000 Contrastive_loss: 0.33034 (0.27191) Loss: 0.33034 (0.27191)
2024-08-30,09:28:14 | INFO | Train Epoch: 77 [ 30100/145000.0 (21%)] Data (t): 0.108 Batch (t): 0.193, 530.301/s, 530.301/s/gpu LR: 1.505445e-09 Logit Scale: 100.000 Contrastive_loss: 0.16921 (0.24624) Loss: 0.16921 (0.24624)
2024-08-30,09:28:34 | INFO | Train Epoch: 77 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 498.867/s, 498.867/s/gpu LR: 1.497963e-09 Logit Scale: 100.000 Contrastive_loss: 0.12774 (0.22254) Loss: 0.12774 (0.22254)
2024-08-30,09:28:53 | INFO | Train Epoch: 77 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.194, 500.881/s, 500.881/s/gpu LR: 1.490496e-09 Logit Scale: 100.000 Contrastive_loss: 0.14863 (0.21022) Loss: 0.14863 (0.21022)
2024-08-30,09:29:12 | INFO | Train Epoch: 77 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 526.652/s, 526.652/s/gpu LR: 1.483045e-09 Logit Scale: 100.000 Contrastive_loss: 0.11059 (0.19599) Loss: 0.11059 (0.19599)
2024-08-30,09:29:32 | INFO | Train Epoch: 77 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 528.344/s, 528.344/s/gpu LR: 1.475609e-09 Logit Scale: 100.000 Contrastive_loss: 0.20263 (0.19682) Loss: 0.20263 (0.19682)
2024-08-30,09:29:51 | INFO | Train Epoch: 77 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 505.325/s, 505.325/s/gpu LR: 1.468189e-09 Logit Scale: 100.000 Contrastive_loss: 0.12147 (0.18845) Loss: 0.12147 (0.18845)
2024-08-30,09:30:10 | INFO | Train Epoch: 77 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.192, 526.858/s, 526.858/s/gpu LR: 1.460784e-09 Logit Scale: 100.000 Contrastive_loss: 0.20186 (0.18979) Loss: 0.20186 (0.18979)
2024-08-30,09:30:29 | INFO | Train Epoch: 77 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 502.432/s, 502.432/s/gpu LR: 1.453395e-09 Logit Scale: 100.000 Contrastive_loss: 0.14453 (0.18567) Loss: 0.14453 (0.18567)
2024-08-30,09:30:49 | INFO | Train Epoch: 77 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 503.005/s, 503.005/s/gpu LR: 1.446021e-09 Logit Scale: 100.000 Contrastive_loss: 0.16153 (0.18366) Loss: 0.16153 (0.18366)
2024-08-30,09:31:08 | INFO | Train Epoch: 77 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 530.715/s, 530.715/s/gpu LR: 1.438663e-09 Logit Scale: 100.000 Contrastive_loss: 0.26220 (0.18970) Loss: 0.26220 (0.18970)
2024-08-30,09:31:27 | INFO | Train Epoch: 77 [130100/145000.0 (90%)] Data (t): 0.107 Batch (t): 0.193, 524.543/s, 524.543/s/gpu LR: 1.431320e-09 Logit Scale: 100.000 Contrastive_loss: 0.14562 (0.18655) Loss: 0.14562 (0.18655)
2024-08-30,09:31:46 | INFO | Train Epoch: 77 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 500.502/s, 500.502/s/gpu LR: 1.423993e-09 Logit Scale: 100.000 Contrastive_loss: 0.17195 (0.18558) Loss: 0.17195 (0.18558)
2024-08-30,09:31:56 | INFO | Train Epoch: 77 [145000/145000.0 (100%)] Data (t): 0.108 Batch (t): 0.194, 531.482/s, 531.482/s/gpu LR: 1.420409e-09 Logit Scale: 100.000 Contrastive_loss: 0.20417 (0.18674) Loss: 0.20417 (0.18674)
2024-08-30,09:32:06 | INFO | Train Epoch: 77 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 502.270/s, 502.270/s/gpu LR: 1.416682e-09 Logit Scale: 100.000 Contrastive_loss: 0.18578 (0.18669) Loss: 0.18578 (0.18669)
2024-08-30,09:32:25 | INFO | Train Epoch: 77 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 531.442/s, 531.442/s/gpu LR: 1.409386e-09 Logit Scale: 100.000 Contrastive_loss: 0.21350 (0.18818) Loss: 0.21350 (0.18818)
2024-08-30,09:32:44 | INFO | Train Epoch: 77 [170100/145000.0 (117%)] Data (t): 0.107 Batch (t): 0.192, 532.173/s, 532.173/s/gpu LR: 1.402107e-09 Logit Scale: 100.000 Contrastive_loss: 0.16286 (0.18684) Loss: 0.16286 (0.18684)
2024-08-30,09:33:04 | INFO | Train Epoch: 77 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 492.511/s, 492.511/s/gpu LR: 1.394843e-09 Logit Scale: 100.000 Contrastive_loss: 0.19075 (0.18704) Loss: 0.19075 (0.18704)
2024-08-30,09:33:23 | INFO | Train Epoch: 77 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 498.429/s, 498.429/s/gpu LR: 1.387594e-09 Logit Scale: 100.000 Contrastive_loss: 0.23230 (0.18919) Loss: 0.23230 (0.18919)
2024-08-30,09:33:42 | INFO | Train Epoch: 77 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 506.519/s, 506.519/s/gpu LR: 1.380362e-09 Logit Scale: 100.000 Contrastive_loss: 0.15750 (0.18775) Loss: 0.15750 (0.18775)
2024-08-30,09:34:01 | INFO | Train Epoch: 77 [210100/145000.0 (145%)] Data (t): 0.107 Batch (t): 0.192, 527.414/s, 527.414/s/gpu LR: 1.373146e-09 Logit Scale: 100.000 Contrastive_loss: 0.23997 (0.19002) Loss: 0.23997 (0.19002)
2024-08-30,09:34:21 | INFO | Train Epoch: 77 [220100/145000.0 (152%)] Data (t): 0.109 Batch (t): 0.193, 498.451/s, 498.451/s/gpu LR: 1.365945e-09 Logit Scale: 100.000 Contrastive_loss: 0.29174 (0.19426) Loss: 0.29174 (0.19426)
2024-08-30,09:34:40 | INFO | Train Epoch: 77 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 517.845/s, 517.845/s/gpu LR: 1.358760e-09 Logit Scale: 100.000 Contrastive_loss: 0.28804 (0.19801) Loss: 0.28804 (0.19801)
2024-08-30,09:34:59 | INFO | Train Epoch: 77 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 528.574/s, 528.574/s/gpu LR: 1.351592e-09 Logit Scale: 100.000 Contrastive_loss: 0.15170 (0.19623) Loss: 0.15170 (0.19623)
2024-08-30,09:35:19 | INFO | Train Epoch: 77 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 489.557/s, 489.557/s/gpu LR: 1.344439e-09 Logit Scale: 100.000 Contrastive_loss: 0.20781 (0.19666) Loss: 0.20781 (0.19666)
2024-08-30,09:35:38 | INFO | Train Epoch: 77 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.194, 507.804/s, 507.804/s/gpu LR: 1.337302e-09 Logit Scale: 100.000 Contrastive_loss: 0.20758 (0.19705) Loss: 0.20758 (0.19705)
2024-08-30,09:35:57 | INFO | Train Epoch: 77 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 496.838/s, 496.838/s/gpu LR: 1.330182e-09 Logit Scale: 100.000 Contrastive_loss: 0.24650 (0.19876) Loss: 0.24650 (0.19876)
2024-08-30,09:36:17 | INFO | Train Epoch: 77 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 522.566/s, 522.566/s/gpu LR: 1.323078e-09 Logit Scale: 100.000 Contrastive_loss: 0.16171 (0.19752) Loss: 0.16171 (0.19752)
2024-08-30,09:36:36 | INFO | Train Epoch: 77 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.193, 502.674/s, 502.674/s/gpu LR: 1.315989e-09 Logit Scale: 100.000 Contrastive_loss: 0.14039 (0.19568) Loss: 0.14039 (0.19568)
2024-08-30,09:36:55 | INFO | Train Epoch: 77 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 532.288/s, 532.288/s/gpu LR: 1.308917e-09 Logit Scale: 100.000 Contrastive_loss: 0.25020 (0.19738) Loss: 0.25020 (0.19738)
2024-08-30,09:37:14 | INFO | Train Epoch: 77 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 503.614/s, 503.614/s/gpu LR: 1.301861e-09 Logit Scale: 100.000 Contrastive_loss: 0.23744 (0.19860) Loss: 0.23744 (0.19860)
2024-08-30,09:37:34 | INFO | Train Epoch: 77 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 526.468/s, 526.468/s/gpu LR: 1.294821e-09 Logit Scale: 100.000 Contrastive_loss: 0.26766 (0.20063) Loss: 0.26766 (0.20063)
2024-08-30,09:37:53 | INFO | Train Epoch: 77 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 537.316/s, 537.316/s/gpu LR: 1.287798e-09 Logit Scale: 100.000 Contrastive_loss: 0.17908 (0.20001) Loss: 0.17908 (0.20001)
2024-08-30,09:38:12 | INFO | Train Epoch: 77 [340100/145000.0 (235%)] Data (t): 0.107 Batch (t): 0.192, 512.016/s, 512.016/s/gpu LR: 1.280791e-09 Logit Scale: 100.000 Contrastive_loss: 0.23131 (0.20088) Loss: 0.23131 (0.20088)
2024-08-30,09:38:32 | INFO | Train Epoch: 77 [350100/145000.0 (241%)] Data (t): 0.109 Batch (t): 0.193, 525.072/s, 525.072/s/gpu LR: 1.273800e-09 Logit Scale: 100.000 Contrastive_loss: 0.28732 (0.20322) Loss: 0.28732 (0.20322)
2024-08-30,09:38:51 | INFO | Train Epoch: 77 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 502.397/s, 502.397/s/gpu LR: 1.266825e-09 Logit Scale: 100.000 Contrastive_loss: 0.14571 (0.20170) Loss: 0.14571 (0.20170)
2024-08-30,09:39:10 | INFO | Train Epoch: 77 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 503.862/s, 503.862/s/gpu LR: 1.259867e-09 Logit Scale: 100.000 Contrastive_loss: 0.24678 (0.20286) Loss: 0.24678 (0.20286)
2024-08-30,09:39:29 | INFO | Train Epoch: 77 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 529.360/s, 529.360/s/gpu LR: 1.252925e-09 Logit Scale: 100.000 Contrastive_loss: 0.22486 (0.20341) Loss: 0.22486 (0.20341)
2024-08-30,09:39:49 | INFO | Train Epoch: 77 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 493.346/s, 493.346/s/gpu LR: 1.246000e-09 Logit Scale: 100.000 Contrastive_loss: 0.12268 (0.20144) Loss: 0.12268 (0.20144)
2024-08-30,09:40:08 | INFO | Train Epoch: 77 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.192, 524.214/s, 524.214/s/gpu LR: 1.239091e-09 Logit Scale: 100.000 Contrastive_loss: 0.21270 (0.20171) Loss: 0.21270 (0.20171)
2024-08-30,09:40:27 | INFO | Train Epoch: 77 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 497.077/s, 497.077/s/gpu LR: 1.232199e-09 Logit Scale: 100.000 Contrastive_loss: 0.082644 (0.19894) Loss: 0.082644 (0.19894)
2024-08-30,09:40:35 | INFO | Eval Epoch: 78 [200 / 1000]	Clip Loss: 0.484016	
2024-08-30,09:40:35 | INFO | Eval Epoch: 78 image_to_text_mean_rank: 2.4010	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6740	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9640	text_to_image_mean_rank: 3.9400	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6720	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4406	epoch: 78.0000	num_samples: 1000.0000
2024-08-30,09:40:37 | INFO | Start epoch 78
2024-08-30,09:40:37 | INFO | Train Epoch: 78 [   100/145000.0 (0%)] Data (t): 0.028 Batch (t): 0.115, 869.926/s, 869.926/s/gpu LR: 1.420336e-09 Logit Scale: 100.000 Contrastive_loss: 0.16016 (0.16016) Loss: 0.16016 (0.16016)
2024-08-30,09:40:56 | INFO | Train Epoch: 78 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.193, 500.459/s, 500.459/s/gpu LR: 1.413032e-09 Logit Scale: 100.000 Contrastive_loss: 0.32500 (0.24258) Loss: 0.32500 (0.24258)
2024-08-30,09:41:16 | INFO | Train Epoch: 78 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.193, 493.711/s, 493.711/s/gpu LR: 1.405745e-09 Logit Scale: 100.000 Contrastive_loss: 0.33001 (0.27172) Loss: 0.33001 (0.27172)
2024-08-30,09:41:35 | INFO | Train Epoch: 78 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 488.562/s, 488.562/s/gpu LR: 1.398473e-09 Logit Scale: 100.000 Contrastive_loss: 0.16898 (0.24604) Loss: 0.16898 (0.24604)
2024-08-30,09:41:54 | INFO | Train Epoch: 78 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.193, 535.770/s, 535.770/s/gpu LR: 1.391216e-09 Logit Scale: 100.000 Contrastive_loss: 0.12762 (0.22235) Loss: 0.12762 (0.22235)
2024-08-30,09:42:13 | INFO | Train Epoch: 78 [ 50100/145000.0 (35%)] Data (t): 0.107 Batch (t): 0.192, 558.275/s, 558.275/s/gpu LR: 1.383976e-09 Logit Scale: 100.000 Contrastive_loss: 0.14844 (0.21004) Loss: 0.14844 (0.21004)
2024-08-30,09:42:33 | INFO | Train Epoch: 78 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 504.950/s, 504.950/s/gpu LR: 1.376752e-09 Logit Scale: 100.000 Contrastive_loss: 0.11055 (0.19582) Loss: 0.11055 (0.19582)
2024-08-30,09:42:52 | INFO | Train Epoch: 78 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.192, 500.130/s, 500.130/s/gpu LR: 1.369543e-09 Logit Scale: 100.000 Contrastive_loss: 0.20229 (0.19663) Loss: 0.20229 (0.19663)
2024-08-30,09:43:11 | INFO | Train Epoch: 78 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 496.548/s, 496.548/s/gpu LR: 1.362351e-09 Logit Scale: 100.000 Contrastive_loss: 0.12127 (0.18826) Loss: 0.12127 (0.18826)
2024-08-30,09:43:31 | INFO | Train Epoch: 78 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 505.021/s, 505.021/s/gpu LR: 1.355174e-09 Logit Scale: 100.000 Contrastive_loss: 0.20158 (0.18959) Loss: 0.20158 (0.18959)
2024-08-30,09:43:50 | INFO | Train Epoch: 78 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 499.305/s, 499.305/s/gpu LR: 1.348013e-09 Logit Scale: 100.000 Contrastive_loss: 0.14454 (0.18550) Loss: 0.14454 (0.18550)
2024-08-30,09:44:09 | INFO | Train Epoch: 78 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.192, 532.552/s, 532.552/s/gpu LR: 1.340869e-09 Logit Scale: 100.000 Contrastive_loss: 0.16140 (0.18349) Loss: 0.16140 (0.18349)
2024-08-30,09:44:28 | INFO | Train Epoch: 78 [120100/145000.0 (83%)] Data (t): 0.109 Batch (t): 0.193, 536.949/s, 536.949/s/gpu LR: 1.333740e-09 Logit Scale: 100.000 Contrastive_loss: 0.26205 (0.18953) Loss: 0.26205 (0.18953)
2024-08-30,09:44:48 | INFO | Train Epoch: 78 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.193, 503.796/s, 503.796/s/gpu LR: 1.326628e-09 Logit Scale: 100.000 Contrastive_loss: 0.14559 (0.18639) Loss: 0.14559 (0.18639)
2024-08-30,09:45:07 | INFO | Train Epoch: 78 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.193, 532.958/s, 532.958/s/gpu LR: 1.319531e-09 Logit Scale: 100.000 Contrastive_loss: 0.17187 (0.18542) Loss: 0.17187 (0.18542)
2024-08-30,09:45:16 | INFO | Train Epoch: 78 [145000/145000.0 (100%)] Data (t): 0.109 Batch (t): 0.193, 540.078/s, 540.078/s/gpu LR: 1.316060e-09 Logit Scale: 100.000 Contrastive_loss: 0.20398 (0.18658) Loss: 0.20398 (0.18658)
2024-08-30,09:45:26 | INFO | Train Epoch: 78 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 528.065/s, 528.065/s/gpu LR: 1.312451e-09 Logit Scale: 100.000 Contrastive_loss: 0.18586 (0.18654) Loss: 0.18586 (0.18654)
2024-08-30,09:45:46 | INFO | Train Epoch: 78 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.194, 506.404/s, 506.404/s/gpu LR: 1.305387e-09 Logit Scale: 100.000 Contrastive_loss: 0.21339 (0.18803) Loss: 0.21339 (0.18803)
2024-08-30,09:46:05 | INFO | Train Epoch: 78 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 529.734/s, 529.734/s/gpu LR: 1.298339e-09 Logit Scale: 100.000 Contrastive_loss: 0.16274 (0.18670) Loss: 0.16274 (0.18670)
2024-08-30,09:46:24 | INFO | Train Epoch: 78 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.192, 535.794/s, 535.794/s/gpu LR: 1.291308e-09 Logit Scale: 100.000 Contrastive_loss: 0.19056 (0.18689) Loss: 0.19056 (0.18689)
2024-08-30,09:46:43 | INFO | Train Epoch: 78 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 532.092/s, 532.092/s/gpu LR: 1.284292e-09 Logit Scale: 100.000 Contrastive_loss: 0.23210 (0.18905) Loss: 0.23210 (0.18905)
2024-08-30,09:47:03 | INFO | Train Epoch: 78 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 506.255/s, 506.255/s/gpu LR: 1.277293e-09 Logit Scale: 100.000 Contrastive_loss: 0.15730 (0.18760) Loss: 0.15730 (0.18760)
2024-08-30,09:47:22 | INFO | Train Epoch: 78 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.193, 531.559/s, 531.559/s/gpu LR: 1.270310e-09 Logit Scale: 100.000 Contrastive_loss: 0.23990 (0.18988) Loss: 0.23990 (0.18988)
2024-08-30,09:47:41 | INFO | Train Epoch: 78 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 529.007/s, 529.007/s/gpu LR: 1.263344e-09 Logit Scale: 100.000 Contrastive_loss: 0.29172 (0.19412) Loss: 0.29172 (0.19412)
2024-08-30,09:48:01 | INFO | Train Epoch: 78 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 505.804/s, 505.804/s/gpu LR: 1.256394e-09 Logit Scale: 100.000 Contrastive_loss: 0.28773 (0.19787) Loss: 0.28773 (0.19787)
2024-08-30,09:48:20 | INFO | Train Epoch: 78 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 500.349/s, 500.349/s/gpu LR: 1.249461e-09 Logit Scale: 100.000 Contrastive_loss: 0.15153 (0.19608) Loss: 0.15153 (0.19608)
2024-08-30,09:48:39 | INFO | Train Epoch: 78 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.193, 530.288/s, 530.288/s/gpu LR: 1.242543e-09 Logit Scale: 100.000 Contrastive_loss: 0.20769 (0.19651) Loss: 0.20769 (0.19651)
2024-08-30,09:48:58 | INFO | Train Epoch: 78 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 524.071/s, 524.071/s/gpu LR: 1.235643e-09 Logit Scale: 100.000 Contrastive_loss: 0.20735 (0.19690) Loss: 0.20735 (0.19690)
2024-08-30,09:49:18 | INFO | Train Epoch: 78 [270100/145000.0 (186%)] Data (t): 0.109 Batch (t): 0.194, 493.704/s, 493.704/s/gpu LR: 1.228759e-09 Logit Scale: 100.000 Contrastive_loss: 0.24640 (0.19861) Loss: 0.24640 (0.19861)
2024-08-30,09:49:37 | INFO | Train Epoch: 78 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 532.480/s, 532.480/s/gpu LR: 1.221891e-09 Logit Scale: 100.000 Contrastive_loss: 0.16165 (0.19737) Loss: 0.16165 (0.19737)
2024-08-30,09:49:56 | INFO | Train Epoch: 78 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.192, 500.624/s, 500.624/s/gpu LR: 1.215040e-09 Logit Scale: 100.000 Contrastive_loss: 0.14036 (0.19554) Loss: 0.14036 (0.19554)
2024-08-30,09:50:15 | INFO | Train Epoch: 78 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 526.454/s, 526.454/s/gpu LR: 1.208206e-09 Logit Scale: 100.000 Contrastive_loss: 0.24995 (0.19724) Loss: 0.24995 (0.19724)
2024-08-30,09:50:35 | INFO | Train Epoch: 78 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.193, 533.627/s, 533.627/s/gpu LR: 1.201388e-09 Logit Scale: 100.000 Contrastive_loss: 0.23735 (0.19845) Loss: 0.23735 (0.19845)
2024-08-30,09:50:54 | INFO | Train Epoch: 78 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.193, 493.741/s, 493.741/s/gpu LR: 1.194587e-09 Logit Scale: 100.000 Contrastive_loss: 0.26756 (0.20048) Loss: 0.26756 (0.20048)
2024-08-30,09:51:13 | INFO | Train Epoch: 78 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 533.568/s, 533.568/s/gpu LR: 1.187803e-09 Logit Scale: 100.000 Contrastive_loss: 0.17902 (0.19987) Loss: 0.17902 (0.19987)
2024-08-30,09:51:33 | INFO | Train Epoch: 78 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.192, 526.425/s, 526.425/s/gpu LR: 1.181035e-09 Logit Scale: 100.000 Contrastive_loss: 0.23094 (0.20073) Loss: 0.23094 (0.20073)
2024-08-30,09:51:52 | INFO | Train Epoch: 78 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 533.667/s, 533.667/s/gpu LR: 1.174284e-09 Logit Scale: 100.000 Contrastive_loss: 0.28711 (0.20307) Loss: 0.28711 (0.20307)
2024-08-30,09:52:11 | INFO | Train Epoch: 78 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 504.747/s, 504.747/s/gpu LR: 1.167550e-09 Logit Scale: 100.000 Contrastive_loss: 0.14554 (0.20155) Loss: 0.14554 (0.20155)
2024-08-30,09:52:31 | INFO | Train Epoch: 78 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 502.047/s, 502.047/s/gpu LR: 1.160833e-09 Logit Scale: 100.000 Contrastive_loss: 0.24668 (0.20271) Loss: 0.24668 (0.20271)
2024-08-30,09:52:50 | INFO | Train Epoch: 78 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.193, 532.119/s, 532.119/s/gpu LR: 1.154132e-09 Logit Scale: 100.000 Contrastive_loss: 0.22465 (0.20326) Loss: 0.22465 (0.20326)
2024-08-30,09:53:09 | INFO | Train Epoch: 78 [390100/145000.0 (269%)] Data (t): 0.107 Batch (t): 0.192, 531.163/s, 531.163/s/gpu LR: 1.147448e-09 Logit Scale: 100.000 Contrastive_loss: 0.12244 (0.20129) Loss: 0.12244 (0.20129)
2024-08-30,09:53:28 | INFO | Train Epoch: 78 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 529.863/s, 529.863/s/gpu LR: 1.140782e-09 Logit Scale: 100.000 Contrastive_loss: 0.21249 (0.20156) Loss: 0.21249 (0.20156)
2024-08-30,09:53:48 | INFO | Train Epoch: 78 [410100/145000.0 (283%)] Data (t): 0.108 Batch (t): 0.193, 538.314/s, 538.314/s/gpu LR: 1.134132e-09 Logit Scale: 100.000 Contrastive_loss: 0.082666 (0.19879) Loss: 0.082666 (0.19879)
2024-08-30,09:53:55 | INFO | Eval Epoch: 79 [200 / 1000]	Clip Loss: 0.484067	
2024-08-30,09:53:56 | INFO | Eval Epoch: 79 image_to_text_mean_rank: 2.4030	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6740	image_to_text_R@5: 0.9070	image_to_text_R@10: 0.9640	text_to_image_mean_rank: 3.9420	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6720	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4407	epoch: 79.0000	num_samples: 1000.0000
2024-08-30,09:53:57 | INFO | Start epoch 79
2024-08-30,09:53:57 | INFO | Train Epoch: 79 [   100/145000.0 (0%)] Data (t): 0.034 Batch (t): 0.114, 876.620/s, 876.620/s/gpu LR: 1.315989e-09 Logit Scale: 100.000 Contrastive_loss: 0.15998 (0.15998) Loss: 0.15998 (0.15998)
2024-08-30,09:54:17 | INFO | Train Epoch: 79 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.195, 493.623/s, 493.623/s/gpu LR: 1.308917e-09 Logit Scale: 100.000 Contrastive_loss: 0.32471 (0.24235) Loss: 0.32471 (0.24235)
2024-08-30,09:54:36 | INFO | Train Epoch: 79 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 531.025/s, 531.025/s/gpu LR: 1.301861e-09 Logit Scale: 100.000 Contrastive_loss: 0.32979 (0.27149) Loss: 0.32979 (0.27149)
2024-08-30,09:54:55 | INFO | Train Epoch: 79 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 524.572/s, 524.572/s/gpu LR: 1.294821e-09 Logit Scale: 100.000 Contrastive_loss: 0.16887 (0.24584) Loss: 0.16887 (0.24584)
2024-08-30,09:55:15 | INFO | Train Epoch: 79 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.194, 493.916/s, 493.916/s/gpu LR: 1.287798e-09 Logit Scale: 100.000 Contrastive_loss: 0.12762 (0.22219) Loss: 0.12762 (0.22219)
2024-08-30,09:55:34 | INFO | Train Epoch: 79 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 520.812/s, 520.812/s/gpu LR: 1.280791e-09 Logit Scale: 100.000 Contrastive_loss: 0.14828 (0.20987) Loss: 0.14828 (0.20987)
2024-08-30,09:55:53 | INFO | Train Epoch: 79 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 523.270/s, 523.270/s/gpu LR: 1.273800e-09 Logit Scale: 100.000 Contrastive_loss: 0.11045 (0.19567) Loss: 0.11045 (0.19567)
2024-08-30,09:56:13 | INFO | Train Epoch: 79 [ 70100/145000.0 (48%)] Data (t): 0.108 Batch (t): 0.192, 535.007/s, 535.007/s/gpu LR: 1.266825e-09 Logit Scale: 100.000 Contrastive_loss: 0.20210 (0.19648) Loss: 0.20210 (0.19648)
2024-08-30,09:56:32 | INFO | Train Epoch: 79 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.193, 531.822/s, 531.822/s/gpu LR: 1.259867e-09 Logit Scale: 100.000 Contrastive_loss: 0.12120 (0.18811) Loss: 0.12120 (0.18811)
2024-08-30,09:56:51 | INFO | Train Epoch: 79 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 499.468/s, 499.468/s/gpu LR: 1.252925e-09 Logit Scale: 100.000 Contrastive_loss: 0.20157 (0.18946) Loss: 0.20157 (0.18946)
2024-08-30,09:57:11 | INFO | Train Epoch: 79 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 527.154/s, 527.154/s/gpu LR: 1.246000e-09 Logit Scale: 100.000 Contrastive_loss: 0.14440 (0.18536) Loss: 0.14440 (0.18536)
2024-08-30,09:57:30 | INFO | Train Epoch: 79 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.192, 498.812/s, 498.812/s/gpu LR: 1.239091e-09 Logit Scale: 100.000 Contrastive_loss: 0.16127 (0.18335) Loss: 0.16127 (0.18335)
2024-08-30,09:57:49 | INFO | Train Epoch: 79 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.192, 531.818/s, 531.818/s/gpu LR: 1.232199e-09 Logit Scale: 100.000 Contrastive_loss: 0.26192 (0.18940) Loss: 0.26192 (0.18940)
2024-08-30,09:58:08 | INFO | Train Epoch: 79 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.194, 502.308/s, 502.308/s/gpu LR: 1.225323e-09 Logit Scale: 100.000 Contrastive_loss: 0.14551 (0.18626) Loss: 0.14551 (0.18626)
2024-08-30,09:58:28 | INFO | Train Epoch: 79 [140100/145000.0 (97%)] Data (t): 0.109 Batch (t): 0.193, 522.786/s, 522.786/s/gpu LR: 1.218464e-09 Logit Scale: 100.000 Contrastive_loss: 0.17170 (0.18529) Loss: 0.17170 (0.18529)
2024-08-30,09:58:37 | INFO | Train Epoch: 79 [145000/145000.0 (100%)] Data (t): 0.110 Batch (t): 0.194, 532.720/s, 532.720/s/gpu LR: 1.215109e-09 Logit Scale: 100.000 Contrastive_loss: 0.20386 (0.18645) Loss: 0.20386 (0.18645)
2024-08-30,09:58:47 | INFO | Train Epoch: 79 [150100/145000.0 (104%)] Data (t): 0.109 Batch (t): 0.193, 530.097/s, 530.097/s/gpu LR: 1.211621e-09 Logit Scale: 100.000 Contrastive_loss: 0.18563 (0.18640) Loss: 0.18563 (0.18640)
2024-08-30,09:59:06 | INFO | Train Epoch: 79 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 505.402/s, 505.402/s/gpu LR: 1.204795e-09 Logit Scale: 100.000 Contrastive_loss: 0.21314 (0.18789) Loss: 0.21314 (0.18789)
2024-08-30,09:59:26 | INFO | Train Epoch: 79 [170100/145000.0 (117%)] Data (t): 0.108 Batch (t): 0.193, 518.381/s, 518.381/s/gpu LR: 1.197985e-09 Logit Scale: 100.000 Contrastive_loss: 0.16266 (0.18656) Loss: 0.16266 (0.18656)
2024-08-30,09:59:45 | INFO | Train Epoch: 79 [180100/145000.0 (124%)] Data (t): 0.109 Batch (t): 0.193, 529.405/s, 529.405/s/gpu LR: 1.191193e-09 Logit Scale: 100.000 Contrastive_loss: 0.19057 (0.18676) Loss: 0.19057 (0.18676)
2024-08-30,10:00:04 | INFO | Train Epoch: 79 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.192, 526.883/s, 526.883/s/gpu LR: 1.184417e-09 Logit Scale: 100.000 Contrastive_loss: 0.23213 (0.18892) Loss: 0.23213 (0.18892)
2024-08-30,10:00:24 | INFO | Train Epoch: 79 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 528.217/s, 528.217/s/gpu LR: 1.177657e-09 Logit Scale: 100.000 Contrastive_loss: 0.15727 (0.18748) Loss: 0.15727 (0.18748)
2024-08-30,10:00:43 | INFO | Train Epoch: 79 [210100/145000.0 (145%)] Data (t): 0.108 Batch (t): 0.192, 539.594/s, 539.594/s/gpu LR: 1.170915e-09 Logit Scale: 100.000 Contrastive_loss: 0.23983 (0.18976) Loss: 0.23983 (0.18976)
2024-08-30,10:01:02 | INFO | Train Epoch: 79 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.192, 535.113/s, 535.113/s/gpu LR: 1.164189e-09 Logit Scale: 100.000 Contrastive_loss: 0.29160 (0.19400) Loss: 0.29160 (0.19400)
2024-08-30,10:01:21 | INFO | Train Epoch: 79 [230100/145000.0 (159%)] Data (t): 0.109 Batch (t): 0.193, 527.091/s, 527.091/s/gpu LR: 1.157480e-09 Logit Scale: 100.000 Contrastive_loss: 0.28766 (0.19775) Loss: 0.28766 (0.19775)
2024-08-30,10:01:41 | INFO | Train Epoch: 79 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 502.083/s, 502.083/s/gpu LR: 1.150788e-09 Logit Scale: 100.000 Contrastive_loss: 0.15142 (0.19597) Loss: 0.15142 (0.19597)
2024-08-30,10:02:00 | INFO | Train Epoch: 79 [250100/145000.0 (172%)] Data (t): 0.107 Batch (t): 0.192, 529.498/s, 529.498/s/gpu LR: 1.144113e-09 Logit Scale: 100.000 Contrastive_loss: 0.20768 (0.19640) Loss: 0.20768 (0.19640)
2024-08-30,10:02:19 | INFO | Train Epoch: 79 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 491.990/s, 491.990/s/gpu LR: 1.137455e-09 Logit Scale: 100.000 Contrastive_loss: 0.20731 (0.19679) Loss: 0.20731 (0.19679)
2024-08-30,10:02:38 | INFO | Train Epoch: 79 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 535.667/s, 535.667/s/gpu LR: 1.130813e-09 Logit Scale: 100.000 Contrastive_loss: 0.24628 (0.19850) Loss: 0.24628 (0.19850)
2024-08-30,10:02:58 | INFO | Train Epoch: 79 [280100/145000.0 (193%)] Data (t): 0.109 Batch (t): 0.193, 505.854/s, 505.854/s/gpu LR: 1.124189e-09 Logit Scale: 100.000 Contrastive_loss: 0.16163 (0.19727) Loss: 0.16163 (0.19727)
2024-08-30,10:03:17 | INFO | Train Epoch: 79 [290100/145000.0 (200%)] Data (t): 0.109 Batch (t): 0.193, 532.549/s, 532.549/s/gpu LR: 1.117582e-09 Logit Scale: 100.000 Contrastive_loss: 0.14029 (0.19543) Loss: 0.14029 (0.19543)
2024-08-30,10:03:36 | INFO | Train Epoch: 79 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.193, 496.468/s, 496.468/s/gpu LR: 1.110991e-09 Logit Scale: 100.000 Contrastive_loss: 0.24987 (0.19713) Loss: 0.24987 (0.19713)
2024-08-30,10:03:55 | INFO | Train Epoch: 79 [310100/145000.0 (214%)] Data (t): 0.108 Batch (t): 0.192, 537.008/s, 537.008/s/gpu LR: 1.104418e-09 Logit Scale: 100.000 Contrastive_loss: 0.23731 (0.19835) Loss: 0.23731 (0.19835)
2024-08-30,10:04:15 | INFO | Train Epoch: 79 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.192, 493.040/s, 493.040/s/gpu LR: 1.097862e-09 Logit Scale: 100.000 Contrastive_loss: 0.26740 (0.20038) Loss: 0.26740 (0.20038)
2024-08-30,10:04:34 | INFO | Train Epoch: 79 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 502.693/s, 502.693/s/gpu LR: 1.091323e-09 Logit Scale: 100.000 Contrastive_loss: 0.17898 (0.19977) Loss: 0.17898 (0.19977)
2024-08-30,10:04:53 | INFO | Train Epoch: 79 [340100/145000.0 (235%)] Data (t): 0.108 Batch (t): 0.193, 528.708/s, 528.708/s/gpu LR: 1.084801e-09 Logit Scale: 100.000 Contrastive_loss: 0.23096 (0.20063) Loss: 0.23096 (0.20063)
2024-08-30,10:05:12 | INFO | Train Epoch: 79 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.192, 541.830/s, 541.830/s/gpu LR: 1.078296e-09 Logit Scale: 100.000 Contrastive_loss: 0.28701 (0.20297) Loss: 0.28701 (0.20297)
2024-08-30,10:05:32 | INFO | Train Epoch: 79 [360100/145000.0 (248%)] Data (t): 0.109 Batch (t): 0.193, 496.735/s, 496.735/s/gpu LR: 1.071809e-09 Logit Scale: 100.000 Contrastive_loss: 0.14552 (0.20146) Loss: 0.14552 (0.20146)
2024-08-30,10:05:51 | INFO | Train Epoch: 79 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 532.952/s, 532.952/s/gpu LR: 1.065339e-09 Logit Scale: 100.000 Contrastive_loss: 0.24652 (0.20261) Loss: 0.24652 (0.20261)
2024-08-30,10:06:10 | INFO | Train Epoch: 79 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 534.157/s, 534.157/s/gpu LR: 1.058886e-09 Logit Scale: 100.000 Contrastive_loss: 0.22472 (0.20317) Loss: 0.22472 (0.20317)
2024-08-30,10:06:29 | INFO | Train Epoch: 79 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 501.961/s, 501.961/s/gpu LR: 1.052450e-09 Logit Scale: 100.000 Contrastive_loss: 0.12237 (0.20119) Loss: 0.12237 (0.20119)
2024-08-30,10:06:49 | INFO | Train Epoch: 79 [400100/145000.0 (276%)] Data (t): 0.109 Batch (t): 0.193, 496.769/s, 496.769/s/gpu LR: 1.046031e-09 Logit Scale: 100.000 Contrastive_loss: 0.21250 (0.20146) Loss: 0.21250 (0.20146)
2024-08-30,10:07:08 | INFO | Train Epoch: 79 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 494.649/s, 494.649/s/gpu LR: 1.039630e-09 Logit Scale: 100.000 Contrastive_loss: 0.082497 (0.19870) Loss: 0.082497 (0.19870)
2024-08-30,10:07:16 | INFO | Eval Epoch: 80 [200 / 1000]	Clip Loss: 0.484047	
2024-08-30,10:07:16 | INFO | Eval Epoch: 80 image_to_text_mean_rank: 2.4020	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6740	image_to_text_R@5: 0.9070	image_to_text_R@10: 0.9640	text_to_image_mean_rank: 3.9410	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6730	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4407	epoch: 80.0000	num_samples: 1000.0000
2024-08-30,10:07:18 | INFO | Start epoch 80
2024-08-30,10:07:18 | INFO | Train Epoch: 80 [   100/145000.0 (0%)] Data (t): 0.024 Batch (t): 0.112, 894.976/s, 894.976/s/gpu LR: 1.215040e-09 Logit Scale: 100.000 Contrastive_loss: 0.15997 (0.15997) Loss: 0.15997 (0.15997)
2024-08-30,10:07:37 | INFO | Train Epoch: 80 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.195, 535.031/s, 535.031/s/gpu LR: 1.208206e-09 Logit Scale: 100.000 Contrastive_loss: 0.32465 (0.24231) Loss: 0.32465 (0.24231)
2024-08-30,10:07:57 | INFO | Train Epoch: 80 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.193, 504.950/s, 504.950/s/gpu LR: 1.201388e-09 Logit Scale: 100.000 Contrastive_loss: 0.32954 (0.27138) Loss: 0.32954 (0.27138)
2024-08-30,10:08:16 | INFO | Train Epoch: 80 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 535.211/s, 535.211/s/gpu LR: 1.194587e-09 Logit Scale: 100.000 Contrastive_loss: 0.16864 (0.24570) Loss: 0.16864 (0.24570)
2024-08-30,10:08:35 | INFO | Train Epoch: 80 [ 40100/145000.0 (28%)] Data (t): 0.109 Batch (t): 0.193, 535.527/s, 535.527/s/gpu LR: 1.187803e-09 Logit Scale: 100.000 Contrastive_loss: 0.12747 (0.22205) Loss: 0.12747 (0.22205)
2024-08-30,10:08:55 | INFO | Train Epoch: 80 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.194, 526.976/s, 526.976/s/gpu LR: 1.181035e-09 Logit Scale: 100.000 Contrastive_loss: 0.14820 (0.20974) Loss: 0.14820 (0.20974)
2024-08-30,10:09:14 | INFO | Train Epoch: 80 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 534.609/s, 534.609/s/gpu LR: 1.174284e-09 Logit Scale: 100.000 Contrastive_loss: 0.11037 (0.19555) Loss: 0.11037 (0.19555)
2024-08-30,10:09:33 | INFO | Train Epoch: 80 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 526.722/s, 526.722/s/gpu LR: 1.167550e-09 Logit Scale: 100.000 Contrastive_loss: 0.20200 (0.19635) Loss: 0.20200 (0.19635)
2024-08-30,10:09:52 | INFO | Train Epoch: 80 [ 80100/145000.0 (55%)] Data (t): 0.108 Batch (t): 0.193, 528.402/s, 528.402/s/gpu LR: 1.160833e-09 Logit Scale: 100.000 Contrastive_loss: 0.12109 (0.18799) Loss: 0.12109 (0.18799)
2024-08-30,10:10:12 | INFO | Train Epoch: 80 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.192, 502.110/s, 502.110/s/gpu LR: 1.154132e-09 Logit Scale: 100.000 Contrastive_loss: 0.20135 (0.18933) Loss: 0.20135 (0.18933)
2024-08-30,10:10:31 | INFO | Train Epoch: 80 [100100/145000.0 (69%)] Data (t): 0.108 Batch (t): 0.193, 527.296/s, 527.296/s/gpu LR: 1.147448e-09 Logit Scale: 100.000 Contrastive_loss: 0.14424 (0.18523) Loss: 0.14424 (0.18523)
2024-08-30,10:10:50 | INFO | Train Epoch: 80 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.192, 522.769/s, 522.769/s/gpu LR: 1.140782e-09 Logit Scale: 100.000 Contrastive_loss: 0.16120 (0.18323) Loss: 0.16120 (0.18323)
2024-08-30,10:11:09 | INFO | Train Epoch: 80 [120100/145000.0 (83%)] Data (t): 0.107 Batch (t): 0.192, 500.058/s, 500.058/s/gpu LR: 1.134132e-09 Logit Scale: 100.000 Contrastive_loss: 0.26173 (0.18926) Loss: 0.26173 (0.18926)
2024-08-30,10:11:29 | INFO | Train Epoch: 80 [130100/145000.0 (90%)] Data (t): 0.108 Batch (t): 0.192, 503.445/s, 503.445/s/gpu LR: 1.127499e-09 Logit Scale: 100.000 Contrastive_loss: 0.14537 (0.18613) Loss: 0.14537 (0.18613)
2024-08-30,10:11:48 | INFO | Train Epoch: 80 [140100/145000.0 (97%)] Data (t): 0.107 Batch (t): 0.192, 498.449/s, 498.449/s/gpu LR: 1.120883e-09 Logit Scale: 100.000 Contrastive_loss: 0.17165 (0.18516) Loss: 0.17165 (0.18516)
2024-08-30,10:11:57 | INFO | Train Epoch: 80 [145000/145000.0 (100%)] Data (t): 0.110 Batch (t): 0.193, 502.716/s, 502.716/s/gpu LR: 1.117648e-09 Logit Scale: 100.000 Contrastive_loss: 0.20360 (0.18632) Loss: 0.20360 (0.18632)
2024-08-30,10:12:07 | INFO | Train Epoch: 80 [150100/145000.0 (104%)] Data (t): 0.110 Batch (t): 0.194, 521.783/s, 521.783/s/gpu LR: 1.114284e-09 Logit Scale: 100.000 Contrastive_loss: 0.18542 (0.18626) Loss: 0.18542 (0.18626)
2024-08-30,10:12:26 | INFO | Train Epoch: 80 [160100/145000.0 (110%)] Data (t): 0.108 Batch (t): 0.193, 535.188/s, 535.188/s/gpu LR: 1.107703e-09 Logit Scale: 100.000 Contrastive_loss: 0.21291 (0.18774) Loss: 0.21291 (0.18774)
2024-08-30,10:12:46 | INFO | Train Epoch: 80 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 505.655/s, 505.655/s/gpu LR: 1.101138e-09 Logit Scale: 100.000 Contrastive_loss: 0.16248 (0.18641) Loss: 0.16248 (0.18641)
2024-08-30,10:13:05 | INFO | Train Epoch: 80 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.193, 505.516/s, 505.516/s/gpu LR: 1.094590e-09 Logit Scale: 100.000 Contrastive_loss: 0.19047 (0.18662) Loss: 0.19047 (0.18662)
2024-08-30,10:13:24 | INFO | Train Epoch: 80 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 491.777/s, 491.777/s/gpu LR: 1.088060e-09 Logit Scale: 100.000 Contrastive_loss: 0.23192 (0.18877) Loss: 0.23192 (0.18877)
2024-08-30,10:13:44 | INFO | Train Epoch: 80 [200100/145000.0 (138%)] Data (t): 0.109 Batch (t): 0.193, 498.591/s, 498.591/s/gpu LR: 1.081547e-09 Logit Scale: 100.000 Contrastive_loss: 0.15716 (0.18734) Loss: 0.15716 (0.18734)
2024-08-30,10:14:03 | INFO | Train Epoch: 80 [210100/145000.0 (145%)] Data (t): 0.107 Batch (t): 0.192, 547.680/s, 547.680/s/gpu LR: 1.075050e-09 Logit Scale: 100.000 Contrastive_loss: 0.23969 (0.18961) Loss: 0.23969 (0.18961)
2024-08-30,10:14:22 | INFO | Train Epoch: 80 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 495.931/s, 495.931/s/gpu LR: 1.068572e-09 Logit Scale: 100.000 Contrastive_loss: 0.29142 (0.19386) Loss: 0.29142 (0.19386)
2024-08-30,10:14:41 | INFO | Train Epoch: 80 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.193, 524.139/s, 524.139/s/gpu LR: 1.062110e-09 Logit Scale: 100.000 Contrastive_loss: 0.28737 (0.19760) Loss: 0.28737 (0.19760)
2024-08-30,10:15:01 | INFO | Train Epoch: 80 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.192, 523.800/s, 523.800/s/gpu LR: 1.055666e-09 Logit Scale: 100.000 Contrastive_loss: 0.15127 (0.19581) Loss: 0.15127 (0.19581)
2024-08-30,10:15:20 | INFO | Train Epoch: 80 [250100/145000.0 (172%)] Data (t): 0.108 Batch (t): 0.192, 492.403/s, 492.403/s/gpu LR: 1.049238e-09 Logit Scale: 100.000 Contrastive_loss: 0.20750 (0.19625) Loss: 0.20750 (0.19625)
2024-08-30,10:15:39 | INFO | Train Epoch: 80 [260100/145000.0 (179%)] Data (t): 0.109 Batch (t): 0.193, 495.654/s, 495.654/s/gpu LR: 1.042829e-09 Logit Scale: 100.000 Contrastive_loss: 0.20717 (0.19664) Loss: 0.20717 (0.19664)
2024-08-30,10:15:58 | INFO | Train Epoch: 80 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.193, 528.926/s, 528.926/s/gpu LR: 1.036436e-09 Logit Scale: 100.000 Contrastive_loss: 0.24619 (0.19835) Loss: 0.24619 (0.19835)
2024-08-30,10:16:18 | INFO | Train Epoch: 80 [280100/145000.0 (193%)] Data (t): 0.108 Batch (t): 0.193, 528.624/s, 528.624/s/gpu LR: 1.030061e-09 Logit Scale: 100.000 Contrastive_loss: 0.16149 (0.19712) Loss: 0.16149 (0.19712)
2024-08-30,10:16:37 | INFO | Train Epoch: 80 [290100/145000.0 (200%)] Data (t): 0.107 Batch (t): 0.192, 507.075/s, 507.075/s/gpu LR: 1.023704e-09 Logit Scale: 100.000 Contrastive_loss: 0.14020 (0.19528) Loss: 0.14020 (0.19528)
2024-08-30,10:16:56 | INFO | Train Epoch: 80 [300100/145000.0 (207%)] Data (t): 0.109 Batch (t): 0.193, 504.962/s, 504.962/s/gpu LR: 1.017363e-09 Logit Scale: 100.000 Contrastive_loss: 0.24966 (0.19698) Loss: 0.24966 (0.19698)
2024-08-30,10:17:15 | INFO | Train Epoch: 80 [310100/145000.0 (214%)] Data (t): 0.109 Batch (t): 0.193, 524.556/s, 524.556/s/gpu LR: 1.011041e-09 Logit Scale: 100.000 Contrastive_loss: 0.23719 (0.19820) Loss: 0.23719 (0.19820)
2024-08-30,10:17:35 | INFO | Train Epoch: 80 [320100/145000.0 (221%)] Data (t): 0.109 Batch (t): 0.193, 533.812/s, 533.812/s/gpu LR: 1.004736e-09 Logit Scale: 100.000 Contrastive_loss: 0.26725 (0.20023) Loss: 0.26725 (0.20023)
2024-08-30,10:17:54 | INFO | Train Epoch: 80 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 520.510/s, 520.510/s/gpu LR: 9.984480e-10 Logit Scale: 100.000 Contrastive_loss: 0.17890 (0.19962) Loss: 0.17890 (0.19962)
2024-08-30,10:18:13 | INFO | Train Epoch: 80 [340100/145000.0 (235%)] Data (t): 0.109 Batch (t): 0.193, 496.699/s, 496.699/s/gpu LR: 9.921778e-10 Logit Scale: 100.000 Contrastive_loss: 0.23070 (0.20048) Loss: 0.23070 (0.20048)
2024-08-30,10:18:33 | INFO | Train Epoch: 80 [350100/145000.0 (241%)] Data (t): 0.108 Batch (t): 0.193, 502.903/s, 502.903/s/gpu LR: 9.859253e-10 Logit Scale: 100.000 Contrastive_loss: 0.28698 (0.20282) Loss: 0.28698 (0.20282)
2024-08-30,10:18:52 | INFO | Train Epoch: 80 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 496.229/s, 496.229/s/gpu LR: 9.796904e-10 Logit Scale: 100.000 Contrastive_loss: 0.14550 (0.20131) Loss: 0.14550 (0.20131)
2024-08-30,10:19:11 | INFO | Train Epoch: 80 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 500.360/s, 500.360/s/gpu LR: 9.734731e-10 Logit Scale: 100.000 Contrastive_loss: 0.24636 (0.20247) Loss: 0.24636 (0.20247)
2024-08-30,10:19:30 | INFO | Train Epoch: 80 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 534.157/s, 534.157/s/gpu LR: 9.672735e-10 Logit Scale: 100.000 Contrastive_loss: 0.22456 (0.20302) Loss: 0.22456 (0.20302)
2024-08-30,10:19:50 | INFO | Train Epoch: 80 [390100/145000.0 (269%)] Data (t): 0.107 Batch (t): 0.192, 497.970/s, 497.970/s/gpu LR: 9.610916e-10 Logit Scale: 100.000 Contrastive_loss: 0.12230 (0.20105) Loss: 0.12230 (0.20105)
2024-08-30,10:20:09 | INFO | Train Epoch: 80 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.193, 495.996/s, 495.996/s/gpu LR: 9.549273e-10 Logit Scale: 100.000 Contrastive_loss: 0.21241 (0.20132) Loss: 0.21241 (0.20132)
2024-08-30,10:20:28 | INFO | Train Epoch: 80 [410100/145000.0 (283%)] Data (t): 0.109 Batch (t): 0.193, 495.938/s, 495.938/s/gpu LR: 9.487809e-10 Logit Scale: 100.000 Contrastive_loss: 0.082523 (0.19856) Loss: 0.082523 (0.19856)
2024-08-30,10:20:36 | INFO | Eval Epoch: 81 [200 / 1000]	Clip Loss: 0.484233	
2024-08-30,10:20:36 | INFO | Eval Epoch: 81 image_to_text_mean_rank: 2.4020	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6740	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9640	text_to_image_mean_rank: 3.9420	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6730	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4406	epoch: 81.0000	num_samples: 1000.0000
2024-08-30,10:20:38 | INFO | Start epoch 81
2024-08-30,10:20:38 | INFO | Train Epoch: 81 [   100/145000.0 (0%)] Data (t): 0.018 Batch (t): 0.098, 1016.97/s, 1016.97/s/gpu LR: 1.117582e-09 Logit Scale: 100.000 Contrastive_loss: 0.15971 (0.15971) Loss: 0.15971 (0.15971)
2024-08-30,10:20:57 | INFO | Train Epoch: 81 [ 10100/145000.0 (7%)] Data (t): 0.110 Batch (t): 0.194, 502.256/s, 502.256/s/gpu LR: 1.110991e-09 Logit Scale: 100.000 Contrastive_loss: 0.32453 (0.24212) Loss: 0.32453 (0.24212)
2024-08-30,10:21:17 | INFO | Train Epoch: 81 [ 20100/145000.0 (14%)] Data (t): 0.109 Batch (t): 0.193, 536.205/s, 536.205/s/gpu LR: 1.104418e-09 Logit Scale: 100.000 Contrastive_loss: 0.32922 (0.27115) Loss: 0.32922 (0.27115)
2024-08-30,10:21:36 | INFO | Train Epoch: 81 [ 30100/145000.0 (21%)] Data (t): 0.110 Batch (t): 0.194, 505.708/s, 505.708/s/gpu LR: 1.097862e-09 Logit Scale: 100.000 Contrastive_loss: 0.16851 (0.24549) Loss: 0.16851 (0.24549)
2024-08-30,10:21:55 | INFO | Train Epoch: 81 [ 40100/145000.0 (28%)] Data (t): 0.107 Batch (t): 0.192, 535.166/s, 535.166/s/gpu LR: 1.091323e-09 Logit Scale: 100.000 Contrastive_loss: 0.12745 (0.22188) Loss: 0.12745 (0.22188)
2024-08-30,10:22:14 | INFO | Train Epoch: 81 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.192, 529.052/s, 529.052/s/gpu LR: 1.084801e-09 Logit Scale: 100.000 Contrastive_loss: 0.14808 (0.20958) Loss: 0.14808 (0.20958)
2024-08-30,10:22:34 | INFO | Train Epoch: 81 [ 60100/145000.0 (41%)] Data (t): 0.108 Batch (t): 0.192, 536.399/s, 536.399/s/gpu LR: 1.078296e-09 Logit Scale: 100.000 Contrastive_loss: 0.11032 (0.19540) Loss: 0.11032 (0.19540)
2024-08-30,10:22:53 | INFO | Train Epoch: 81 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 492.062/s, 492.062/s/gpu LR: 1.071809e-09 Logit Scale: 100.000 Contrastive_loss: 0.20172 (0.19619) Loss: 0.20172 (0.19619)
2024-08-30,10:23:12 | INFO | Train Epoch: 81 [ 80100/145000.0 (55%)] Data (t): 0.107 Batch (t): 0.193, 499.302/s, 499.302/s/gpu LR: 1.065339e-09 Logit Scale: 100.000 Contrastive_loss: 0.12094 (0.18783) Loss: 0.12094 (0.18783)
2024-08-30,10:23:31 | INFO | Train Epoch: 81 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 527.086/s, 527.086/s/gpu LR: 1.058886e-09 Logit Scale: 100.000 Contrastive_loss: 0.20120 (0.18917) Loss: 0.20120 (0.18917)
2024-08-30,10:23:51 | INFO | Train Epoch: 81 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.194, 521.133/s, 521.133/s/gpu LR: 1.052450e-09 Logit Scale: 100.000 Contrastive_loss: 0.14425 (0.18508) Loss: 0.14425 (0.18508)
2024-08-30,10:24:10 | INFO | Train Epoch: 81 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 507.700/s, 507.700/s/gpu LR: 1.046031e-09 Logit Scale: 100.000 Contrastive_loss: 0.16106 (0.18308) Loss: 0.16106 (0.18308)
2024-08-30,10:24:29 | INFO | Train Epoch: 81 [120100/145000.0 (83%)] Data (t): 0.108 Batch (t): 0.193, 493.648/s, 493.648/s/gpu LR: 1.039630e-09 Logit Scale: 100.000 Contrastive_loss: 0.26164 (0.18913) Loss: 0.26164 (0.18913)
2024-08-30,10:24:49 | INFO | Train Epoch: 81 [130100/145000.0 (90%)] Data (t): 0.109 Batch (t): 0.193, 506.709/s, 506.709/s/gpu LR: 1.033247e-09 Logit Scale: 100.000 Contrastive_loss: 0.14528 (0.18599) Loss: 0.14528 (0.18599)
2024-08-30,10:25:08 | INFO | Train Epoch: 81 [140100/145000.0 (97%)] Data (t): 0.108 Batch (t): 0.192, 499.366/s, 499.366/s/gpu LR: 1.026880e-09 Logit Scale: 100.000 Contrastive_loss: 0.17158 (0.18503) Loss: 0.17158 (0.18503)
2024-08-30,10:25:17 | INFO | Train Epoch: 81 [145000/145000.0 (100%)] Data (t): 0.107 Batch (t): 0.192, 522.692/s, 522.692/s/gpu LR: 1.023767e-09 Logit Scale: 100.000 Contrastive_loss: 0.20346 (0.18618) Loss: 0.20346 (0.18618)
2024-08-30,10:25:27 | INFO | Train Epoch: 81 [150100/145000.0 (104%)] Data (t): 0.108 Batch (t): 0.193, 520.153/s, 520.153/s/gpu LR: 1.020531e-09 Logit Scale: 100.000 Contrastive_loss: 0.18530 (0.18613) Loss: 0.18530 (0.18613)
2024-08-30,10:25:47 | INFO | Train Epoch: 81 [160100/145000.0 (110%)] Data (t): 0.109 Batch (t): 0.193, 528.957/s, 528.957/s/gpu LR: 1.014200e-09 Logit Scale: 100.000 Contrastive_loss: 0.21279 (0.18761) Loss: 0.21279 (0.18761)
2024-08-30,10:26:06 | INFO | Train Epoch: 81 [170100/145000.0 (117%)] Data (t): 0.109 Batch (t): 0.193, 508.037/s, 508.037/s/gpu LR: 1.007886e-09 Logit Scale: 100.000 Contrastive_loss: 0.16248 (0.18629) Loss: 0.16248 (0.18629)
2024-08-30,10:26:25 | INFO | Train Epoch: 81 [180100/145000.0 (124%)] Data (t): 0.108 Batch (t): 0.192, 526.940/s, 526.940/s/gpu LR: 1.001590e-09 Logit Scale: 100.000 Contrastive_loss: 0.19044 (0.18650) Loss: 0.19044 (0.18650)
2024-08-30,10:26:44 | INFO | Train Epoch: 81 [190100/145000.0 (131%)] Data (t): 0.108 Batch (t): 0.193, 499.699/s, 499.699/s/gpu LR: 9.953107e-10 Logit Scale: 100.000 Contrastive_loss: 0.23188 (0.18866) Loss: 0.23188 (0.18866)
2024-08-30,10:27:04 | INFO | Train Epoch: 81 [200100/145000.0 (138%)] Data (t): 0.108 Batch (t): 0.193, 503.652/s, 503.652/s/gpu LR: 9.890494e-10 Logit Scale: 100.000 Contrastive_loss: 0.15701 (0.18722) Loss: 0.15701 (0.18722)
2024-08-30,10:27:23 | INFO | Train Epoch: 81 [210100/145000.0 (145%)] Data (t): 0.109 Batch (t): 0.193, 493.505/s, 493.505/s/gpu LR: 9.828057e-10 Logit Scale: 100.000 Contrastive_loss: 0.23958 (0.18950) Loss: 0.23958 (0.18950)
2024-08-30,10:27:42 | INFO | Train Epoch: 81 [220100/145000.0 (152%)] Data (t): 0.108 Batch (t): 0.193, 516.830/s, 516.830/s/gpu LR: 9.765795e-10 Logit Scale: 100.000 Contrastive_loss: 0.29118 (0.19373) Loss: 0.29118 (0.19373)
2024-08-30,10:28:01 | INFO | Train Epoch: 81 [230100/145000.0 (159%)] Data (t): 0.108 Batch (t): 0.192, 534.088/s, 534.088/s/gpu LR: 9.703711e-10 Logit Scale: 100.000 Contrastive_loss: 0.28744 (0.19748) Loss: 0.28744 (0.19748)
2024-08-30,10:28:21 | INFO | Train Epoch: 81 [240100/145000.0 (166%)] Data (t): 0.108 Batch (t): 0.193, 531.177/s, 531.177/s/gpu LR: 9.641803e-10 Logit Scale: 100.000 Contrastive_loss: 0.15116 (0.19570) Loss: 0.15116 (0.19570)
2024-08-30,10:28:40 | INFO | Train Epoch: 81 [250100/145000.0 (172%)] Data (t): 0.109 Batch (t): 0.193, 501.763/s, 501.763/s/gpu LR: 9.580072e-10 Logit Scale: 100.000 Contrastive_loss: 0.20751 (0.19614) Loss: 0.20751 (0.19614)
2024-08-30,10:28:59 | INFO | Train Epoch: 81 [260100/145000.0 (179%)] Data (t): 0.108 Batch (t): 0.194, 498.189/s, 498.189/s/gpu LR: 9.518519e-10 Logit Scale: 100.000 Contrastive_loss: 0.20710 (0.19653) Loss: 0.20710 (0.19653)
2024-08-30,10:29:19 | INFO | Train Epoch: 81 [270100/145000.0 (186%)] Data (t): 0.108 Batch (t): 0.192, 525.460/s, 525.460/s/gpu LR: 9.457143e-10 Logit Scale: 100.000 Contrastive_loss: 0.24616 (0.19824) Loss: 0.24616 (0.19824)
2024-08-30,10:29:38 | INFO | Train Epoch: 81 [280100/145000.0 (193%)] Data (t): 0.107 Batch (t): 0.191, 521.350/s, 521.350/s/gpu LR: 9.395945e-10 Logit Scale: 100.000 Contrastive_loss: 0.16140 (0.19701) Loss: 0.16140 (0.19701)
2024-08-30,10:29:57 | INFO | Train Epoch: 81 [290100/145000.0 (200%)] Data (t): 0.108 Batch (t): 0.192, 508.323/s, 508.323/s/gpu LR: 9.334925e-10 Logit Scale: 100.000 Contrastive_loss: 0.14019 (0.19518) Loss: 0.14019 (0.19518)
2024-08-30,10:30:16 | INFO | Train Epoch: 81 [300100/145000.0 (207%)] Data (t): 0.108 Batch (t): 0.192, 495.352/s, 495.352/s/gpu LR: 9.274083e-10 Logit Scale: 100.000 Contrastive_loss: 0.24964 (0.19688) Loss: 0.24964 (0.19688)
2024-08-30,10:30:35 | INFO | Train Epoch: 81 [310100/145000.0 (214%)] Data (t): 0.107 Batch (t): 0.192, 543.186/s, 543.186/s/gpu LR: 9.213420e-10 Logit Scale: 100.000 Contrastive_loss: 0.23724 (0.19811) Loss: 0.23724 (0.19811)
2024-08-30,10:30:55 | INFO | Train Epoch: 81 [320100/145000.0 (221%)] Data (t): 0.108 Batch (t): 0.192, 526.439/s, 526.439/s/gpu LR: 9.152936e-10 Logit Scale: 100.000 Contrastive_loss: 0.26709 (0.20013) Loss: 0.26709 (0.20013)
2024-08-30,10:31:14 | INFO | Train Epoch: 81 [330100/145000.0 (228%)] Data (t): 0.108 Batch (t): 0.193, 494.875/s, 494.875/s/gpu LR: 9.092632e-10 Logit Scale: 100.000 Contrastive_loss: 0.17880 (0.19952) Loss: 0.17880 (0.19952)
2024-08-30,10:31:33 | INFO | Train Epoch: 81 [340100/145000.0 (235%)] Data (t): 0.107 Batch (t): 0.192, 540.750/s, 540.750/s/gpu LR: 9.032506e-10 Logit Scale: 100.000 Contrastive_loss: 0.23054 (0.20039) Loss: 0.23054 (0.20039)
2024-08-30,10:31:52 | INFO | Train Epoch: 81 [350100/145000.0 (241%)] Data (t): 0.106 Batch (t): 0.191, 536.693/s, 536.693/s/gpu LR: 8.972560e-10 Logit Scale: 100.000 Contrastive_loss: 0.28684 (0.20272) Loss: 0.28684 (0.20272)
2024-08-30,10:32:11 | INFO | Train Epoch: 81 [360100/145000.0 (248%)] Data (t): 0.108 Batch (t): 0.193, 535.724/s, 535.724/s/gpu LR: 8.912795e-10 Logit Scale: 100.000 Contrastive_loss: 0.14547 (0.20122) Loss: 0.14547 (0.20122)
2024-08-30,10:32:31 | INFO | Train Epoch: 81 [370100/145000.0 (255%)] Data (t): 0.108 Batch (t): 0.193, 501.083/s, 501.083/s/gpu LR: 8.853209e-10 Logit Scale: 100.000 Contrastive_loss: 0.24625 (0.20237) Loss: 0.24625 (0.20237)
2024-08-30,10:32:50 | INFO | Train Epoch: 81 [380100/145000.0 (262%)] Data (t): 0.108 Batch (t): 0.192, 511.100/s, 511.100/s/gpu LR: 8.793804e-10 Logit Scale: 100.000 Contrastive_loss: 0.22446 (0.20292) Loss: 0.22446 (0.20292)
2024-08-30,10:33:09 | INFO | Train Epoch: 81 [390100/145000.0 (269%)] Data (t): 0.108 Batch (t): 0.193, 521.017/s, 521.017/s/gpu LR: 8.734580e-10 Logit Scale: 100.000 Contrastive_loss: 0.12225 (0.20096) Loss: 0.12225 (0.20096)
2024-08-30,10:33:28 | INFO | Train Epoch: 81 [400100/145000.0 (276%)] Data (t): 0.108 Batch (t): 0.192, 498.990/s, 498.990/s/gpu LR: 8.675537e-10 Logit Scale: 100.000 Contrastive_loss: 0.21227 (0.20122) Loss: 0.21227 (0.20122)
2024-08-30,10:33:48 | INFO | Train Epoch: 81 [410100/145000.0 (283%)] Data (t): 0.107 Batch (t): 0.192, 533.626/s, 533.626/s/gpu LR: 8.616675e-10 Logit Scale: 100.000 Contrastive_loss: 0.082493 (0.19846) Loss: 0.082493 (0.19846)
2024-08-30,10:33:55 | INFO | Eval Epoch: 82 [200 / 1000]	Clip Loss: 0.484205	
2024-08-30,10:33:56 | INFO | Eval Epoch: 82 image_to_text_mean_rank: 2.4010	image_to_text_median_rank: 1.0000	image_to_text_R@1: 0.6740	image_to_text_R@5: 0.9080	image_to_text_R@10: 0.9640	text_to_image_mean_rank: 3.9430	text_to_image_median_rank: 1.0000	text_to_image_R@1: 0.6730	text_to_image_R@5: 0.9070	text_to_image_R@10: 0.9660	clip_val_loss: 0.4407	epoch: 82.0000	num_samples: 1000.0000
2024-08-30,10:33:57 | INFO | Start epoch 82
2024-08-30,10:33:57 | INFO | Train Epoch: 82 [   100/145000.0 (0%)] Data (t): 0.024 Batch (t): 0.114, 877.200/s, 877.200/s/gpu LR: 1.023704e-09 Logit Scale: 100.000 Contrastive_loss: 0.15960 (0.15960) Loss: 0.15960 (0.15960)
2024-08-30,10:34:17 | INFO | Train Epoch: 82 [ 10100/145000.0 (7%)] Data (t): 0.109 Batch (t): 0.194, 489.235/s, 489.235/s/gpu LR: 1.017363e-09 Logit Scale: 100.000 Contrastive_loss: 0.32438 (0.24199) Loss: 0.32438 (0.24199)
2024-08-30,10:34:36 | INFO | Train Epoch: 82 [ 20100/145000.0 (14%)] Data (t): 0.108 Batch (t): 0.192, 505.848/s, 505.848/s/gpu LR: 1.011041e-09 Logit Scale: 100.000 Contrastive_loss: 0.32914 (0.27104) Loss: 0.32914 (0.27104)
2024-08-30,10:34:55 | INFO | Train Epoch: 82 [ 30100/145000.0 (21%)] Data (t): 0.109 Batch (t): 0.193, 507.319/s, 507.319/s/gpu LR: 1.004736e-09 Logit Scale: 100.000 Contrastive_loss: 0.16834 (0.24536) Loss: 0.16834 (0.24536)
2024-08-30,10:35:14 | INFO | Train Epoch: 82 [ 40100/145000.0 (28%)] Data (t): 0.108 Batch (t): 0.192, 535.746/s, 535.746/s/gpu LR: 9.984480e-10 Logit Scale: 100.000 Contrastive_loss: 0.12746 (0.22178) Loss: 0.12746 (0.22178)
2024-08-30,10:35:34 | INFO | Train Epoch: 82 [ 50100/145000.0 (35%)] Data (t): 0.109 Batch (t): 0.193, 503.649/s, 503.649/s/gpu LR: 9.921778e-10 Logit Scale: 100.000 Contrastive_loss: 0.14789 (0.20947) Loss: 0.14789 (0.20947)
2024-08-30,10:35:53 | INFO | Train Epoch: 82 [ 60100/145000.0 (41%)] Data (t): 0.109 Batch (t): 0.193, 507.640/s, 507.640/s/gpu LR: 9.859253e-10 Logit Scale: 100.000 Contrastive_loss: 0.11029 (0.19530) Loss: 0.11029 (0.19530)
2024-08-30,10:36:12 | INFO | Train Epoch: 82 [ 70100/145000.0 (48%)] Data (t): 0.109 Batch (t): 0.193, 527.959/s, 527.959/s/gpu LR: 9.796904e-10 Logit Scale: 100.000 Contrastive_loss: 0.20158 (0.19608) Loss: 0.20158 (0.19608)
2024-08-30,10:36:32 | INFO | Train Epoch: 82 [ 80100/145000.0 (55%)] Data (t): 0.109 Batch (t): 0.194, 534.461/s, 534.461/s/gpu LR: 9.734731e-10 Logit Scale: 100.000 Contrastive_loss: 0.12089 (0.18773) Loss: 0.12089 (0.18773)
2024-08-30,10:36:51 | INFO | Train Epoch: 82 [ 90100/145000.0 (62%)] Data (t): 0.108 Batch (t): 0.193, 495.663/s, 495.663/s/gpu LR: 9.672735e-10 Logit Scale: 100.000 Contrastive_loss: 0.20120 (0.18908) Loss: 0.20120 (0.18908)
2024-08-30,10:37:10 | INFO | Train Epoch: 82 [100100/145000.0 (69%)] Data (t): 0.109 Batch (t): 0.193, 514.497/s, 514.497/s/gpu LR: 9.610916e-10 Logit Scale: 100.000 Contrastive_loss: 0.14414 (0.18499) Loss: 0.14414 (0.18499)
2024-08-30,10:37:30 | INFO | Train Epoch: 82 [110100/145000.0 (76%)] Data (t): 0.108 Batch (t): 0.193, 524.703/s, 524.703/s/gpu LR: 9.549273e-10 Logit Scale: 100.000 Contrastive_loss: 0.16102 (0.18299) Loss: 0.16102 (0.18299)
2024-08-30,10:37:49 | INFO | Train Epoch: 82 [120100/145000.0 (83%)] Data (t): 0.112 Batch (t): 0.199, 492.822/s, 492.822/s/gpu LR: 9.487809e-10 Logit Scale: 100.000 Contrastive_loss: 0.26155 (0.18904) Loss: 0.26155 (0.18904)
